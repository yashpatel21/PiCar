{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "218010bc",
      "metadata": {
        "id": "218010bc"
      },
      "source": [
        "### Code for training a Monocular Depth Estimation Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "6k2Nxo-Yt1eL",
        "outputId": "fa1c7650-0371-4c7a-bec8-29b24e1b83db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6k2Nxo-Yt1eL",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan  7 02:28:20 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              49W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "feefdfad-e1db-4542-a771-2783b67a85bf",
      "metadata": {
        "id": "feefdfad-e1db-4542-a771-2783b67a85bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'  # Add this line\n",
        "\n",
        "# Now import TensorFlow and other libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, DepthwiseConv2D, ReLU,\n",
        "    MaxPooling2D, Concatenate, Layer, Add\n",
        ")\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple GPU check\n",
        "print(f\"GPU Available: {tf.test.is_built_with_cuda()}\")\n",
        "print(f\"GPU Devices: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "# Enable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "A2pcytzMzSe1",
        "outputId": "bcafe9a2-82c5-4c7e-f988-f5bb0dac7fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "A2pcytzMzSe1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d69d1198",
      "metadata": {
        "id": "d69d1198"
      },
      "outputs": [],
      "source": [
        "# Paths and Configuration\n",
        "TRAIN_CSV = \"datasets/nyu_data/data/nyu2_train.csv\"\n",
        "TEST_CSV = \"datasets/nyu_data/data/nyu2_test.csv\"\n",
        "BASE_PATH = \"datasets/nyu_data/\"\n",
        "TARGET_SIZE = (320, 320)\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate schedule configuration\n",
        "initial_learning_rate = 0.001\n",
        "maximal_learning_rate = 0.006\n",
        "step_size = 2000\n",
        "cycle_steps = 8000\n",
        "\n",
        "def cyclic_learning_rate(step):\n",
        "    \"\"\"Cyclic learning rate calculation\"\"\"\n",
        "    cycle = np.floor(1 + step / (2 * step_size))\n",
        "    x = np.abs(step / step_size - 2 * cycle + 1)\n",
        "    lr = initial_learning_rate + (maximal_learning_rate - initial_learning_rate) * max(0., 1 - x)\n",
        "    return lr"
      ],
      "metadata": {
        "id": "984Ra03-dqDs"
      },
      "id": "984Ra03-dqDs",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup():\n",
        "    \"\"\"Aggressive cleanup of memory\"\"\"\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "rQwNKCccuHnk"
      },
      "id": "rQwNKCccuHnk",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c77e4276",
      "metadata": {
        "id": "c77e4276"
      },
      "outputs": [],
      "source": [
        "class MemoryEfficientDataset:\n",
        "    def __init__(self, csv_path, batch_size=32):\n",
        "        self.data = pd.read_csv(csv_path, header=None, names=[\"rgb_path\", \"depth_path\"])\n",
        "        self.batch_size = batch_size\n",
        "        self.current_index = 0\n",
        "        self.steps = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.batch_size\n",
        "\n",
        "    def get_batch(self):\n",
        "        if self.current_index >= len(self.data):\n",
        "            self.current_index = 0\n",
        "            return None\n",
        "\n",
        "        batch_data = self.data.iloc[self.current_index:self.current_index + self.batch_size]\n",
        "        self.current_index += self.batch_size\n",
        "\n",
        "        rgb_batch = []\n",
        "        depth_batch = []\n",
        "\n",
        "        for _, row in batch_data.iterrows():\n",
        "            rgb_path = BASE_PATH + row[\"rgb_path\"]\n",
        "            depth_path = BASE_PATH + row[\"depth_path\"]\n",
        "\n",
        "            rgb_img = cv2.imread(rgb_path)\n",
        "            rgb_img = cv2.resize(rgb_img, TARGET_SIZE)\n",
        "            rgb_img = rgb_img.astype(np.float32) / 255.0\n",
        "\n",
        "            depth_map = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n",
        "            depth_map = cv2.resize(depth_map, TARGET_SIZE)\n",
        "            depth_map = depth_map.astype(np.float32) / 255.0\n",
        "\n",
        "            rgb_batch.append(rgb_img)\n",
        "            depth_batch.append(depth_map)\n",
        "\n",
        "        self.steps += 1\n",
        "        return np.array(rgb_batch), np.array(depth_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d087ac28",
      "metadata": {
        "id": "d087ac28"
      },
      "outputs": [],
      "source": [
        "class UpsampleBlock(Layer):\n",
        "    \"\"\"Edge TPU compatible upsampling\"\"\"\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(UpsampleBlock, self).__init__(**kwargs)\n",
        "        self.conv = Conv2D(\n",
        "            filters * 4,\n",
        "            1,\n",
        "            padding='same',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "        )\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        x = tf.nn.depth_to_space(x, 2)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "def conv_block(x, filters, kernel_size=3, strides=1):\n",
        "    \"\"\"Convolutional block with ReLU activation and L2 regularization\"\"\"\n",
        "    x = Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        padding='same',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "    )(x)\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def residual_block(x, filters):\n",
        "    \"\"\"Edge TPU compatible residual block with L2 regularization\"\"\"\n",
        "    skip = x\n",
        "\n",
        "    if skip.shape[-1] != filters:\n",
        "        skip = Conv2D(\n",
        "            filters,\n",
        "            1,\n",
        "            padding='same',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "        )(skip)\n",
        "\n",
        "    x = conv_block(x, filters)\n",
        "    x = Conv2D(\n",
        "        filters,\n",
        "        3,\n",
        "        padding='same',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "    )(x)\n",
        "    x = Add()([x, skip])\n",
        "    x = ReLU()(x)\n",
        "    return x\n",
        "\n",
        "def create_depth_model(input_shape=(320, 320, 3)):\n",
        "    \"\"\"Create Edge TPU compatible depth estimation model with increased capacity\"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Initial Feature Extraction with more filters\n",
        "    x = conv_block(inputs, 48, strides=2)  # 160x160\n",
        "    x = conv_block(x, 48)\n",
        "    block1 = residual_block(x, 48)  # Save 160x160\n",
        "\n",
        "    # Encoder Stage 1\n",
        "    x = conv_block(block1, 96, strides=2)  # 80x80\n",
        "    x = residual_block(x, 96)\n",
        "    block2 = residual_block(x, 96)  # Save 80x80\n",
        "\n",
        "    # Encoder Stage 2 with additional residual blocks\n",
        "    x = conv_block(block2, 128, strides=2)  # 40x40\n",
        "    x = residual_block(x, 128)\n",
        "    x = residual_block(x, 128)\n",
        "    block3 = residual_block(x, 128)  # Save 40x40\n",
        "\n",
        "    # Bridge with increased capacity\n",
        "    x = conv_block(block3, 256)  # 40x40\n",
        "    x = residual_block(x, 256)\n",
        "    x = residual_block(x, 256)\n",
        "\n",
        "    # Decoder Stage 1 (40x40 -> 80x80)\n",
        "    x = UpsampleBlock(128)(x)  # Now 80x80\n",
        "    x = Concatenate()([x, block2])  # block2 is also 80x80\n",
        "    x = residual_block(x, 128)\n",
        "    x = residual_block(x, 128)\n",
        "\n",
        "    # Decoder Stage 2 (80x80 -> 160x160)\n",
        "    x = UpsampleBlock(96)(x)  # Now 160x160\n",
        "    x = Concatenate()([x, block1])  # block1 is also 160x160\n",
        "    x = residual_block(x, 96)\n",
        "    x = residual_block(x, 96)\n",
        "\n",
        "    # Final upsampling (160x160 -> 320x320)\n",
        "    x = UpsampleBlock(48)(x)  # Now 320x320\n",
        "    x = residual_block(x, 48)\n",
        "\n",
        "    # Final layers\n",
        "    x = Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    outputs = Conv2D(1, 3, padding='same', activation='relu6', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function(reduce_retracing=True)\n",
        "def train_step(model, optimizer, x, y, step):\n",
        "    \"\"\"Single training step with memory optimization and gradient clipping\"\"\"\n",
        "    step = tf.cast(step, tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = tf.reduce_mean(tf.square(y - predictions[:,:,:,0]))\n",
        "        reg_loss = tf.reduce_sum(model.losses)\n",
        "        total_loss = loss + reg_loss\n",
        "\n",
        "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "    gradients = [tf.clip_by_norm(g, 1.0) if g is not None else g for g in gradients]\n",
        "\n",
        "    # Calculate learning rate\n",
        "    cycle = tf.floor(1 + step / (2 * step_size))\n",
        "    x = tf.abs(step / step_size - 2 * cycle + 1)\n",
        "    lr = initial_learning_rate + (maximal_learning_rate - initial_learning_rate) * tf.maximum(0., 1 - x)\n",
        "\n",
        "    optimizer.learning_rate.assign(lr)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, lr\n",
        "\n",
        "@tf.function(reduce_retracing=True)\n",
        "def val_step(model, x, y):\n",
        "    \"\"\"Calculate validation loss\"\"\"\n",
        "    predictions = model(x, training=False)\n",
        "    loss = tf.reduce_mean(tf.square(y - predictions[:,:,:,0]))\n",
        "    reg_loss = tf.reduce_sum(model.losses)\n",
        "    return loss + reg_loss"
      ],
      "metadata": {
        "id": "zSyNKBaMzptX"
      },
      "id": "zSyNKBaMzptX",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanup()"
      ],
      "metadata": {
        "id": "-bx87yOXz7ix"
      },
      "id": "-bx87yOXz7ix",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "print(\"\\nCreating datasets...\")\n",
        "train_data = MemoryEfficientDataset(TRAIN_CSV, BATCH_SIZE)\n",
        "val_data = MemoryEfficientDataset(TEST_CSV, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Z3goRay5E1At",
        "outputId": "00f4a3bf-394f-47e0-a4b9-9dcc81bb56e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Z3goRay5E1At",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating datasets...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and compile model\n",
        "print(\"\\nCreating model...\")\n",
        "model = create_depth_model()\n",
        "model.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)"
      ],
      "metadata": {
        "id": "O2Or0XJUE1oY",
        "outputId": "9fc7bee0-9c87-418a-ec3d-72e5bbd5fb17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "O2Or0XJUE1oY",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │          \u001b[38;5;34m1,344\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu (\u001b[38;5;33mReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m20,784\u001b[0m │ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m20,784\u001b[0m │ re_lu_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m20,784\u001b[0m │ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ re_lu_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │         \u001b[38;5;34m41,568\u001b[0m │ re_lu_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │         \u001b[38;5;34m83,040\u001b[0m │ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_5 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │         \u001b[38;5;34m83,040\u001b[0m │ re_lu_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_6 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │         \u001b[38;5;34m83,040\u001b[0m │ re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_7 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │         \u001b[38;5;34m83,040\u001b[0m │ re_lu_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│                           │                        │                │ re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_8 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m110,720\u001b[0m │ re_lu_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_9 (\u001b[38;5;33mReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_10 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ re_lu_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_11 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_12 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ re_lu_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_13 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_14 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ re_lu_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_15 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m295,168\u001b[0m │ re_lu_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_16 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m590,080\u001b[0m │ re_lu_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_17 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m590,080\u001b[0m │ re_lu_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ re_lu_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_18 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m590,080\u001b[0m │ re_lu_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_19 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │        \u001b[38;5;34m590,080\u001b[0m │ re_lu_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ re_lu_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_20 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ upsample_block            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m131,584\u001b[0m │ re_lu_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mUpsampleBlock\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m224\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ upsample_block[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                           │                        │                │ re_lu_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m258,176\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_22 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m28,800\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_23 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_24 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m147,584\u001b[0m │ re_lu_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_9 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ conv2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ re_lu_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_25 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ upsample_block_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │         \u001b[38;5;34m49,536\u001b[0m │ re_lu_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mUpsampleBlock\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m144\u001b[0m)  │              \u001b[38;5;34m0\u001b[0m │ upsample_block_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ re_lu_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │        \u001b[38;5;34m124,512\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_27 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │         \u001b[38;5;34m83,040\u001b[0m │ re_lu_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │         \u001b[38;5;34m13,920\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_10 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ conv2d_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_28 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │         \u001b[38;5;34m83,040\u001b[0m │ re_lu_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_29 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │         \u001b[38;5;34m83,040\u001b[0m │ re_lu_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_11 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ re_lu_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_30 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ upsample_block_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m18,624\u001b[0m │ re_lu_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mUpsampleBlock\u001b[0m)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m20,784\u001b[0m │ upsample_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_32 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m20,784\u001b[0m │ re_lu_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_12 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ upsample_block_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_33 (\u001b[38;5;33mReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_36 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m13,856\u001b[0m │ re_lu_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_37 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m320\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │            \u001b[38;5;34m289\u001b[0m │ conv2d_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,784</span> │ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,784</span> │ re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,784</span> │ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">41,568</span> │ re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │ re_lu_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │ re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │ re_lu_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│                           │                        │                │ re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">110,720</span> │ re_lu_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ re_lu_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ re_lu_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ re_lu_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │ re_lu_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ re_lu_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ re_lu_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ re_lu_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ re_lu_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │ re_lu_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ re_lu_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ upsample_block            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ re_lu_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpsampleBlock</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ upsample_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                           │                        │                │ re_lu_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">258,176</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">28,800</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ re_lu_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ re_lu_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ upsample_block_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">49,536</span> │ re_lu_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpsampleBlock</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>)  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ upsample_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">124,512</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │ re_lu_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">13,920</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ conv2d_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │ re_lu_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,040</span> │ re_lu_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ re_lu_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ upsample_block_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,624</span> │ re_lu_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpsampleBlock</span>)           │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,784</span> │ upsample_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">20,784</span> │ re_lu_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ upsample_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ re_lu_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">13,856</span> │ re_lu_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │            <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │ conv2d_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,461,873\u001b[0m (20.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,461,873</span> (20.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,461,873\u001b[0m (20.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,461,873</span> (20.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 1: Checkpoint Setup\n",
        "try:\n",
        "    print(\"\\nSetting up checkpoints...\")\n",
        "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "    checkpoint_dir = './training_checkpoints'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    manager = tf.train.CheckpointManager(\n",
        "        checkpoint, checkpoint_dir, max_to_keep=3\n",
        "    )\n",
        "    print(\"Checkpoint setup complete!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in checkpoint setup: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "thqNlaKVE66H",
        "outputId": "eabfb90e-9fcc-4369-f87b-ed4b8a62816c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "thqNlaKVE66H",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Setting up checkpoints...\n",
            "Checkpoint setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training section\n",
        "try:\n",
        "    print(\"\\nStarting training...\")\n",
        "    epochs = 50\n",
        "    training_history = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Training\n",
        "        train_losses = []\n",
        "        learning_rates = []\n",
        "        steps = len(train_data)\n",
        "\n",
        "        for step in range(steps):\n",
        "            batch = train_data.get_batch()\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            x_batch, y_batch = batch\n",
        "            if x_batch.size == 0 or y_batch.size == 0:\n",
        "                continue\n",
        "\n",
        "            # Convert step to tensor\n",
        "            step_tensor = tf.constant(global_step, dtype=tf.float32)\n",
        "            loss, current_lr = train_step(model, optimizer, x_batch, y_batch, step_tensor)\n",
        "\n",
        "            if not tf.math.is_nan(loss) and not tf.math.is_inf(loss):\n",
        "                train_losses.append(float(loss))\n",
        "                learning_rates.append(float(current_lr))\n",
        "\n",
        "            if step % 10 == 0 and train_losses:\n",
        "                current_mean = np.mean(train_losses[-10:])\n",
        "                if not np.isnan(current_mean):\n",
        "                    print(f\"Step {step}/{steps}, Loss: {current_mean:.4f}, LR: {current_lr:.6f}\")\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                cleanup()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Validation\n",
        "        val_losses = []\n",
        "        val_steps = len(val_data)\n",
        "        for step in range(val_steps):\n",
        "            batch = val_data.get_batch()\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            x_val, y_val = batch\n",
        "            if x_val.size == 0 or y_val.size == 0:\n",
        "                continue\n",
        "\n",
        "            val_loss = val_step(model, x_val, y_val)\n",
        "            if not tf.math.is_nan(val_loss) and not tf.math.is_inf(val_loss):\n",
        "                val_losses.append(float(val_loss))\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = np.mean(train_losses) if train_losses else float('inf')\n",
        "        epoch_val_loss = np.mean(val_losses) if val_losses else float('inf')\n",
        "        epoch_lr = np.mean(learning_rates) if learning_rates else 0.0\n",
        "\n",
        "        # Early stopping check\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            model.save('best_model.h5')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "        if not np.isnan(epoch_train_loss) and not np.isnan(epoch_val_loss):\n",
        "            training_history.append({\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': epoch_train_loss,\n",
        "                'val_loss': epoch_val_loss,\n",
        "                'learning_rate': epoch_lr\n",
        "            })\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1} Results:\")\n",
        "            print(f\"Training Loss: {epoch_train_loss:.4f}\")\n",
        "            print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
        "            print(f\"Learning Rate: {epoch_lr:.6f}\")\n",
        "        else:\n",
        "            print(f\"\\nEpoch {epoch+1} produced invalid losses, skipping...\")\n",
        "\n",
        "            if epoch > 0:\n",
        "                print(\"Training unstable, stopping...\")\n",
        "                break\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 and not np.isnan(epoch_train_loss):\n",
        "            save_path = manager.save()\n",
        "            print(f\"\\nSaved checkpoint for epoch {epoch+1}: {save_path}\")\n",
        "            cleanup()\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during training: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "m4feTj4dE89z",
        "outputId": "cde6e93d-8c8e-4556-83a4-6a192a2aac60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "m4feTj4dE89z",
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/50\n",
            "Step 0/1584, Loss: 0.1621, LR: 0.001000\n",
            "Step 10/1584, Loss: 0.0688, LR: 0.001025\n",
            "Step 20/1584, Loss: 0.0105, LR: 0.001050\n",
            "Step 30/1584, Loss: 0.0117, LR: 0.001075\n",
            "Step 40/1584, Loss: 0.0160, LR: 0.001100\n",
            "Step 50/1584, Loss: 0.0167, LR: 0.001125\n",
            "Step 60/1584, Loss: 0.0294, LR: 0.001150\n",
            "Step 70/1584, Loss: 0.0130, LR: 0.001175\n",
            "Step 80/1584, Loss: 0.0178, LR: 0.001200\n",
            "Step 90/1584, Loss: 0.0071, LR: 0.001225\n",
            "Step 100/1584, Loss: 0.0059, LR: 0.001250\n",
            "Step 110/1584, Loss: 0.0260, LR: 0.001275\n",
            "Step 120/1584, Loss: 0.0188, LR: 0.001300\n",
            "Step 130/1584, Loss: 0.0084, LR: 0.001325\n",
            "Step 140/1584, Loss: 0.0168, LR: 0.001350\n",
            "Step 150/1584, Loss: 0.0168, LR: 0.001375\n",
            "Step 160/1584, Loss: 0.0107, LR: 0.001400\n",
            "Step 170/1584, Loss: 0.0189, LR: 0.001425\n",
            "Step 180/1584, Loss: 0.0175, LR: 0.001450\n",
            "Step 190/1584, Loss: 0.0183, LR: 0.001475\n",
            "Step 200/1584, Loss: 0.0162, LR: 0.001500\n",
            "Step 210/1584, Loss: 0.0203, LR: 0.001525\n",
            "Step 220/1584, Loss: 0.0114, LR: 0.001550\n",
            "Step 230/1584, Loss: 0.0621, LR: 0.001575\n",
            "Step 240/1584, Loss: 0.0487, LR: 0.001600\n",
            "Step 250/1584, Loss: 0.0544, LR: 0.001625\n",
            "Step 260/1584, Loss: 0.0533, LR: 0.001650\n",
            "Step 270/1584, Loss: 0.0651, LR: 0.001675\n",
            "Step 280/1584, Loss: 0.0549, LR: 0.001700\n",
            "Step 290/1584, Loss: 0.0170, LR: 0.001725\n",
            "Step 300/1584, Loss: 0.0091, LR: 0.001750\n",
            "Step 310/1584, Loss: 0.0169, LR: 0.001775\n",
            "Step 320/1584, Loss: 0.0262, LR: 0.001800\n",
            "Step 330/1584, Loss: 0.0280, LR: 0.001825\n",
            "Step 340/1584, Loss: 0.0277, LR: 0.001850\n",
            "Step 350/1584, Loss: 0.0204, LR: 0.001875\n",
            "Step 360/1584, Loss: 0.0134, LR: 0.001900\n",
            "Step 370/1584, Loss: 0.0190, LR: 0.001925\n",
            "Step 380/1584, Loss: 0.0201, LR: 0.001950\n",
            "Step 390/1584, Loss: 0.0537, LR: 0.001975\n",
            "Step 400/1584, Loss: 0.0372, LR: 0.002000\n",
            "Step 410/1584, Loss: 0.0224, LR: 0.002025\n",
            "Step 420/1584, Loss: 0.0079, LR: 0.002050\n",
            "Step 430/1584, Loss: 0.1010, LR: 0.002075\n",
            "Step 440/1584, Loss: 0.0171, LR: 0.002100\n",
            "Step 450/1584, Loss: 0.0291, LR: 0.002125\n",
            "Step 460/1584, Loss: 0.0172, LR: 0.002150\n",
            "Step 470/1584, Loss: 0.0149, LR: 0.002175\n",
            "Step 480/1584, Loss: 0.0225, LR: 0.002200\n",
            "Step 490/1584, Loss: 0.0093, LR: 0.002225\n",
            "Step 500/1584, Loss: 0.0082, LR: 0.002250\n",
            "Step 510/1584, Loss: 0.0155, LR: 0.002275\n",
            "Step 520/1584, Loss: 0.0131, LR: 0.002300\n",
            "Step 530/1584, Loss: 0.0104, LR: 0.002325\n",
            "Step 540/1584, Loss: 0.0190, LR: 0.002350\n",
            "Step 550/1584, Loss: 0.0206, LR: 0.002375\n",
            "Step 560/1584, Loss: 0.0199, LR: 0.002400\n",
            "Step 570/1584, Loss: 0.0158, LR: 0.002425\n",
            "Step 580/1584, Loss: 0.0189, LR: 0.002450\n",
            "Step 590/1584, Loss: 0.0206, LR: 0.002475\n",
            "Step 600/1584, Loss: 0.0535, LR: 0.002500\n",
            "Step 610/1584, Loss: 0.0671, LR: 0.002525\n",
            "Step 620/1584, Loss: 0.0424, LR: 0.002550\n",
            "Step 630/1584, Loss: 0.0218, LR: 0.002575\n",
            "Step 640/1584, Loss: 0.0482, LR: 0.002600\n",
            "Step 650/1584, Loss: 0.0115, LR: 0.002625\n",
            "Step 660/1584, Loss: 0.0420, LR: 0.002650\n",
            "Step 670/1584, Loss: 0.0370, LR: 0.002675\n",
            "Step 680/1584, Loss: 0.0284, LR: 0.002700\n",
            "Step 690/1584, Loss: 0.0237, LR: 0.002725\n",
            "Step 700/1584, Loss: 0.0150, LR: 0.002750\n",
            "Step 710/1584, Loss: 0.0357, LR: 0.002775\n",
            "Step 720/1584, Loss: 0.0354, LR: 0.002800\n",
            "Step 730/1584, Loss: 0.0185, LR: 0.002825\n",
            "Step 740/1584, Loss: 0.0614, LR: 0.002850\n",
            "Step 750/1584, Loss: 0.0547, LR: 0.002875\n",
            "Step 760/1584, Loss: 0.0363, LR: 0.002900\n",
            "Step 770/1584, Loss: 0.0203, LR: 0.002925\n",
            "Step 780/1584, Loss: 0.0114, LR: 0.002950\n",
            "Step 790/1584, Loss: 0.0346, LR: 0.002975\n",
            "Step 800/1584, Loss: 0.0527, LR: 0.003000\n",
            "Step 810/1584, Loss: 0.0337, LR: 0.003025\n",
            "Step 820/1584, Loss: 0.0233, LR: 0.003050\n",
            "Step 830/1584, Loss: 0.0122, LR: 0.003075\n",
            "Step 840/1584, Loss: 0.0304, LR: 0.003100\n",
            "Step 850/1584, Loss: 0.0304, LR: 0.003125\n",
            "Step 860/1584, Loss: 0.0090, LR: 0.003150\n",
            "Step 870/1584, Loss: 0.0288, LR: 0.003175\n",
            "Step 880/1584, Loss: 0.0319, LR: 0.003200\n",
            "Step 890/1584, Loss: 0.0409, LR: 0.003225\n",
            "Step 900/1584, Loss: 0.0229, LR: 0.003250\n",
            "Step 910/1584, Loss: 0.0108, LR: 0.003275\n",
            "Step 920/1584, Loss: 0.0576, LR: 0.003300\n",
            "Step 930/1584, Loss: 0.0722, LR: 0.003325\n",
            "Step 940/1584, Loss: 0.0304, LR: 0.003350\n",
            "Step 950/1584, Loss: 0.0150, LR: 0.003375\n",
            "Step 960/1584, Loss: 0.0236, LR: 0.003400\n",
            "Step 970/1584, Loss: 0.0098, LR: 0.003425\n",
            "Step 980/1584, Loss: 0.0187, LR: 0.003450\n",
            "Step 990/1584, Loss: 0.0182, LR: 0.003475\n",
            "Step 1000/1584, Loss: 0.0114, LR: 0.003500\n",
            "Step 1010/1584, Loss: 0.0085, LR: 0.003525\n",
            "Step 1020/1584, Loss: 0.0480, LR: 0.003550\n",
            "Step 1030/1584, Loss: 0.0394, LR: 0.003575\n",
            "Step 1040/1584, Loss: 0.0381, LR: 0.003600\n",
            "Step 1050/1584, Loss: 0.0229, LR: 0.003625\n",
            "Step 1060/1584, Loss: 0.0141, LR: 0.003650\n",
            "Step 1070/1584, Loss: 0.0103, LR: 0.003675\n",
            "Step 1080/1584, Loss: 0.0072, LR: 0.003700\n",
            "Step 1090/1584, Loss: 0.0322, LR: 0.003725\n",
            "Step 1100/1584, Loss: 0.0077, LR: 0.003750\n",
            "Step 1110/1584, Loss: 0.0134, LR: 0.003775\n",
            "Step 1120/1584, Loss: 0.0519, LR: 0.003800\n",
            "Step 1130/1584, Loss: 0.1106, LR: 0.003825\n",
            "Step 1140/1584, Loss: 0.0557, LR: 0.003850\n",
            "Step 1150/1584, Loss: 0.0261, LR: 0.003875\n",
            "Step 1160/1584, Loss: 0.0172, LR: 0.003900\n",
            "Step 1170/1584, Loss: 0.0370, LR: 0.003925\n",
            "Step 1180/1584, Loss: 0.0289, LR: 0.003950\n",
            "Step 1190/1584, Loss: 0.0109, LR: 0.003975\n",
            "Step 1200/1584, Loss: 0.0165, LR: 0.004000\n",
            "Step 1210/1584, Loss: 0.0177, LR: 0.004025\n",
            "Step 1220/1584, Loss: 0.0189, LR: 0.004050\n",
            "Step 1230/1584, Loss: 0.0095, LR: 0.004075\n",
            "Step 1240/1584, Loss: 0.0084, LR: 0.004100\n",
            "Step 1250/1584, Loss: 0.0181, LR: 0.004125\n",
            "Step 1260/1584, Loss: 0.0216, LR: 0.004150\n",
            "Step 1270/1584, Loss: 0.0161, LR: 0.004175\n",
            "Step 1280/1584, Loss: 0.0297, LR: 0.004200\n",
            "Step 1290/1584, Loss: 0.0153, LR: 0.004225\n",
            "Step 1300/1584, Loss: 0.0163, LR: 0.004250\n",
            "Step 1310/1584, Loss: 0.0225, LR: 0.004275\n",
            "Step 1320/1584, Loss: 0.0259, LR: 0.004300\n",
            "Step 1330/1584, Loss: 0.0163, LR: 0.004325\n",
            "Step 1340/1584, Loss: 0.0198, LR: 0.004350\n",
            "Step 1350/1584, Loss: 0.0126, LR: 0.004375\n",
            "Step 1360/1584, Loss: 0.0133, LR: 0.004400\n",
            "Step 1370/1584, Loss: 0.0416, LR: 0.004425\n",
            "Step 1380/1584, Loss: 0.0300, LR: 0.004450\n",
            "Step 1390/1584, Loss: 0.0188, LR: 0.004475\n",
            "Step 1400/1584, Loss: 0.0087, LR: 0.004500\n",
            "Step 1410/1584, Loss: 0.0192, LR: 0.004525\n",
            "Step 1420/1584, Loss: 0.0351, LR: 0.004550\n",
            "Step 1430/1584, Loss: 0.0171, LR: 0.004575\n",
            "Step 1440/1584, Loss: 0.0154, LR: 0.004600\n",
            "Step 1450/1584, Loss: 0.0096, LR: 0.004625\n",
            "Step 1460/1584, Loss: 0.0102, LR: 0.004650\n",
            "Step 1470/1584, Loss: 0.0155, LR: 0.004675\n",
            "Step 1480/1584, Loss: 0.0091, LR: 0.004700\n",
            "Step 1490/1584, Loss: 0.0077, LR: 0.004725\n",
            "Step 1500/1584, Loss: 0.0130, LR: 0.004750\n",
            "Step 1510/1584, Loss: 0.0082, LR: 0.004775\n",
            "Step 1520/1584, Loss: 0.0090, LR: 0.004800\n",
            "Step 1530/1584, Loss: 0.0211, LR: 0.004825\n",
            "Step 1540/1584, Loss: 0.0154, LR: 0.004850\n",
            "Step 1550/1584, Loss: 0.0226, LR: 0.004875\n",
            "Step 1560/1584, Loss: 0.0260, LR: 0.004900\n",
            "Step 1570/1584, Loss: 0.0175, LR: 0.004925\n",
            "Step 1580/1584, Loss: 0.0218, LR: 0.004950\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Results:\n",
            "Training Loss: 0.0253\n",
            "Validation Loss: 0.0840\n",
            "Learning Rate: 0.002979\n",
            "\n",
            "Epoch 2/50\n",
            "Step 10/1584, Loss: 0.0212, LR: 0.004983\n",
            "Step 20/1584, Loss: 0.0173, LR: 0.005008\n",
            "Step 30/1584, Loss: 0.0108, LR: 0.005033\n",
            "Step 40/1584, Loss: 0.0128, LR: 0.005058\n",
            "Step 50/1584, Loss: 0.0151, LR: 0.005083\n",
            "Step 60/1584, Loss: 0.0195, LR: 0.005108\n",
            "Step 70/1584, Loss: 0.0139, LR: 0.005133\n",
            "Step 80/1584, Loss: 0.0169, LR: 0.005158\n",
            "Step 90/1584, Loss: 0.0083, LR: 0.005183\n",
            "Step 100/1584, Loss: 0.0072, LR: 0.005208\n",
            "Step 110/1584, Loss: 0.0213, LR: 0.005233\n",
            "Step 120/1584, Loss: 0.0185, LR: 0.005258\n",
            "Step 130/1584, Loss: 0.0087, LR: 0.005283\n",
            "Step 140/1584, Loss: 0.0168, LR: 0.005308\n",
            "Step 150/1584, Loss: 0.0223, LR: 0.005333\n",
            "Step 160/1584, Loss: 0.0128, LR: 0.005358\n",
            "Step 170/1584, Loss: 0.0139, LR: 0.005383\n",
            "Step 180/1584, Loss: 0.0163, LR: 0.005408\n",
            "Step 190/1584, Loss: 0.0191, LR: 0.005433\n",
            "Step 200/1584, Loss: 0.0144, LR: 0.005458\n",
            "Step 210/1584, Loss: 0.0177, LR: 0.005483\n",
            "Step 220/1584, Loss: 0.0101, LR: 0.005508\n",
            "Step 230/1584, Loss: 0.0432, LR: 0.005533\n",
            "Step 240/1584, Loss: 0.0338, LR: 0.005558\n",
            "Step 250/1584, Loss: 0.0345, LR: 0.005583\n",
            "Step 260/1584, Loss: 0.0489, LR: 0.005608\n",
            "Step 270/1584, Loss: 0.0643, LR: 0.005633\n",
            "Step 280/1584, Loss: 0.0583, LR: 0.005658\n",
            "Step 290/1584, Loss: 0.0202, LR: 0.005683\n",
            "Step 300/1584, Loss: 0.0177, LR: 0.005708\n",
            "Step 310/1584, Loss: 0.0156, LR: 0.005733\n",
            "Step 320/1584, Loss: 0.0276, LR: 0.005758\n",
            "Step 330/1584, Loss: 0.0305, LR: 0.005783\n",
            "Step 340/1584, Loss: 0.0224, LR: 0.005808\n",
            "Step 350/1584, Loss: 0.0174, LR: 0.005833\n",
            "Step 360/1584, Loss: 0.0177, LR: 0.005858\n",
            "Step 370/1584, Loss: 0.0179, LR: 0.005883\n",
            "Step 380/1584, Loss: 0.0212, LR: 0.005908\n",
            "Step 390/1584, Loss: 0.0482, LR: 0.005933\n",
            "Step 400/1584, Loss: 0.0415, LR: 0.005958\n",
            "Step 410/1584, Loss: 0.0203, LR: 0.005983\n",
            "Step 420/1584, Loss: 0.0140, LR: 0.005993\n",
            "Step 430/1584, Loss: 0.0747, LR: 0.005968\n",
            "Step 440/1584, Loss: 0.0177, LR: 0.005943\n",
            "Step 450/1584, Loss: 0.0232, LR: 0.005918\n",
            "Step 460/1584, Loss: 0.0183, LR: 0.005893\n",
            "Step 470/1584, Loss: 0.0119, LR: 0.005867\n",
            "Step 480/1584, Loss: 0.0194, LR: 0.005842\n",
            "Step 490/1584, Loss: 0.0083, LR: 0.005817\n",
            "Step 500/1584, Loss: 0.0083, LR: 0.005792\n",
            "Step 510/1584, Loss: 0.0135, LR: 0.005767\n",
            "Step 520/1584, Loss: 0.0136, LR: 0.005742\n",
            "Step 530/1584, Loss: 0.0089, LR: 0.005717\n",
            "Step 540/1584, Loss: 0.0200, LR: 0.005692\n",
            "Step 550/1584, Loss: 0.0191, LR: 0.005667\n",
            "Step 560/1584, Loss: 0.0210, LR: 0.005642\n",
            "Step 570/1584, Loss: 0.0156, LR: 0.005617\n",
            "Step 580/1584, Loss: 0.0189, LR: 0.005592\n",
            "Step 590/1584, Loss: 0.0193, LR: 0.005567\n",
            "Step 600/1584, Loss: 0.0524, LR: 0.005542\n",
            "Step 610/1584, Loss: 0.0642, LR: 0.005517\n",
            "Step 620/1584, Loss: 0.0422, LR: 0.005493\n",
            "Step 630/1584, Loss: 0.0293, LR: 0.005468\n",
            "Step 640/1584, Loss: 0.0361, LR: 0.005443\n",
            "Step 650/1584, Loss: 0.0120, LR: 0.005418\n",
            "Step 660/1584, Loss: 0.0388, LR: 0.005393\n",
            "Step 670/1584, Loss: 0.0341, LR: 0.005368\n",
            "Step 680/1584, Loss: 0.0283, LR: 0.005342\n",
            "Step 690/1584, Loss: 0.0242, LR: 0.005317\n",
            "Step 700/1584, Loss: 0.0182, LR: 0.005292\n",
            "Step 710/1584, Loss: 0.0327, LR: 0.005267\n",
            "Step 720/1584, Loss: 0.0295, LR: 0.005242\n",
            "Step 730/1584, Loss: 0.0215, LR: 0.005217\n",
            "Step 740/1584, Loss: 0.0531, LR: 0.005192\n",
            "Step 750/1584, Loss: 0.0577, LR: 0.005167\n",
            "Step 760/1584, Loss: 0.0334, LR: 0.005142\n",
            "Step 770/1584, Loss: 0.0253, LR: 0.005117\n",
            "Step 780/1584, Loss: 0.0282, LR: 0.005092\n",
            "Step 790/1584, Loss: 0.0199, LR: 0.005067\n",
            "Step 800/1584, Loss: 0.0443, LR: 0.005042\n",
            "Step 810/1584, Loss: 0.0331, LR: 0.005017\n",
            "Step 820/1584, Loss: 0.0183, LR: 0.004992\n",
            "Step 830/1584, Loss: 0.0124, LR: 0.004967\n",
            "Step 840/1584, Loss: 0.0238, LR: 0.004943\n",
            "Step 850/1584, Loss: 0.0335, LR: 0.004918\n",
            "Step 860/1584, Loss: 0.0104, LR: 0.004892\n",
            "Step 870/1584, Loss: 0.0256, LR: 0.004867\n",
            "Step 880/1584, Loss: 0.0322, LR: 0.004842\n",
            "Step 890/1584, Loss: 0.0419, LR: 0.004817\n",
            "Step 900/1584, Loss: 0.0203, LR: 0.004792\n",
            "Step 910/1584, Loss: 0.0098, LR: 0.004767\n",
            "Step 920/1584, Loss: 0.0499, LR: 0.004742\n",
            "Step 930/1584, Loss: 0.0749, LR: 0.004717\n",
            "Step 940/1584, Loss: 0.0321, LR: 0.004692\n",
            "Step 950/1584, Loss: 0.0153, LR: 0.004667\n",
            "Step 960/1584, Loss: 0.0224, LR: 0.004642\n",
            "Step 970/1584, Loss: 0.0105, LR: 0.004617\n",
            "Step 980/1584, Loss: 0.0175, LR: 0.004592\n",
            "Step 990/1584, Loss: 0.0188, LR: 0.004567\n",
            "Step 1000/1584, Loss: 0.0123, LR: 0.004542\n",
            "Step 1010/1584, Loss: 0.0081, LR: 0.004517\n",
            "Step 1020/1584, Loss: 0.0421, LR: 0.004492\n",
            "Step 1030/1584, Loss: 0.0436, LR: 0.004467\n",
            "Step 1040/1584, Loss: 0.0321, LR: 0.004442\n",
            "Step 1050/1584, Loss: 0.0234, LR: 0.004417\n",
            "Step 1060/1584, Loss: 0.0196, LR: 0.004392\n",
            "Step 1070/1584, Loss: 0.0117, LR: 0.004367\n",
            "Step 1080/1584, Loss: 0.0066, LR: 0.004342\n",
            "Step 1090/1584, Loss: 0.0319, LR: 0.004317\n",
            "Step 1100/1584, Loss: 0.0086, LR: 0.004292\n",
            "Step 1110/1584, Loss: 0.0098, LR: 0.004267\n",
            "Step 1120/1584, Loss: 0.0444, LR: 0.004242\n",
            "Step 1130/1584, Loss: 0.1115, LR: 0.004217\n",
            "Step 1140/1584, Loss: 0.0627, LR: 0.004192\n",
            "Step 1150/1584, Loss: 0.0278, LR: 0.004167\n",
            "Step 1160/1584, Loss: 0.0161, LR: 0.004142\n",
            "Step 1170/1584, Loss: 0.0364, LR: 0.004118\n",
            "Step 1180/1584, Loss: 0.0321, LR: 0.004092\n",
            "Step 1190/1584, Loss: 0.0100, LR: 0.004067\n",
            "Step 1200/1584, Loss: 0.0183, LR: 0.004042\n",
            "Step 1210/1584, Loss: 0.0163, LR: 0.004017\n",
            "Step 1220/1584, Loss: 0.0195, LR: 0.003992\n",
            "Step 1230/1584, Loss: 0.0106, LR: 0.003967\n",
            "Step 1240/1584, Loss: 0.0084, LR: 0.003942\n",
            "Step 1250/1584, Loss: 0.0164, LR: 0.003917\n",
            "Step 1260/1584, Loss: 0.0225, LR: 0.003892\n",
            "Step 1270/1584, Loss: 0.0168, LR: 0.003867\n",
            "Step 1280/1584, Loss: 0.0265, LR: 0.003842\n",
            "Step 1290/1584, Loss: 0.0186, LR: 0.003817\n",
            "Step 1300/1584, Loss: 0.0153, LR: 0.003792\n",
            "Step 1310/1584, Loss: 0.0225, LR: 0.003767\n",
            "Step 1320/1584, Loss: 0.0228, LR: 0.003742\n",
            "Step 1330/1584, Loss: 0.0200, LR: 0.003717\n",
            "Step 1340/1584, Loss: 0.0207, LR: 0.003692\n",
            "Step 1350/1584, Loss: 0.0120, LR: 0.003667\n",
            "Step 1360/1584, Loss: 0.0132, LR: 0.003642\n",
            "Step 1370/1584, Loss: 0.0347, LR: 0.003617\n",
            "Step 1380/1584, Loss: 0.0358, LR: 0.003592\n",
            "Step 1390/1584, Loss: 0.0196, LR: 0.003568\n",
            "Step 1400/1584, Loss: 0.0071, LR: 0.003543\n",
            "Step 1410/1584, Loss: 0.0204, LR: 0.003518\n",
            "Step 1420/1584, Loss: 0.0299, LR: 0.003492\n",
            "Step 1430/1584, Loss: 0.0217, LR: 0.003467\n",
            "Step 1440/1584, Loss: 0.0158, LR: 0.003442\n",
            "Step 1450/1584, Loss: 0.0110, LR: 0.003417\n",
            "Step 1460/1584, Loss: 0.0091, LR: 0.003392\n",
            "Step 1470/1584, Loss: 0.0142, LR: 0.003367\n",
            "Step 1480/1584, Loss: 0.0103, LR: 0.003342\n",
            "Step 1490/1584, Loss: 0.0079, LR: 0.003317\n",
            "Step 1500/1584, Loss: 0.0116, LR: 0.003292\n",
            "Step 1510/1584, Loss: 0.0096, LR: 0.003267\n",
            "Step 1520/1584, Loss: 0.0073, LR: 0.003242\n",
            "Step 1530/1584, Loss: 0.0210, LR: 0.003217\n",
            "Step 1540/1584, Loss: 0.0160, LR: 0.003192\n",
            "Step 1550/1584, Loss: 0.0227, LR: 0.003167\n",
            "Step 1560/1584, Loss: 0.0277, LR: 0.003142\n",
            "Step 1570/1584, Loss: 0.0167, LR: 0.003117\n",
            "Step 1580/1584, Loss: 0.0220, LR: 0.003092\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Results:\n",
            "Training Loss: 0.0240\n",
            "Validation Loss: 0.0802\n",
            "Learning Rate: 0.004789\n",
            "\n",
            "Epoch 3/50\n",
            "Step 0/1584, Loss: 0.0286, LR: 0.003082\n",
            "Step 10/1584, Loss: 0.0232, LR: 0.003060\n",
            "Step 20/1584, Loss: 0.0172, LR: 0.003035\n",
            "Step 30/1584, Loss: 0.0108, LR: 0.003010\n",
            "Step 40/1584, Loss: 0.0108, LR: 0.002985\n",
            "Step 50/1584, Loss: 0.0175, LR: 0.002960\n",
            "Step 60/1584, Loss: 0.0160, LR: 0.002935\n",
            "Step 70/1584, Loss: 0.0168, LR: 0.002910\n",
            "Step 80/1584, Loss: 0.0165, LR: 0.002885\n",
            "Step 90/1584, Loss: 0.0096, LR: 0.002860\n",
            "Step 100/1584, Loss: 0.0064, LR: 0.002835\n",
            "Step 110/1584, Loss: 0.0195, LR: 0.002810\n",
            "Step 120/1584, Loss: 0.0196, LR: 0.002785\n",
            "Step 130/1584, Loss: 0.0103, LR: 0.002760\n",
            "Step 140/1584, Loss: 0.0122, LR: 0.002735\n",
            "Step 150/1584, Loss: 0.0256, LR: 0.002710\n",
            "Step 160/1584, Loss: 0.0102, LR: 0.002685\n",
            "Step 170/1584, Loss: 0.0134, LR: 0.002660\n",
            "Step 180/1584, Loss: 0.0162, LR: 0.002635\n",
            "Step 190/1584, Loss: 0.0191, LR: 0.002610\n",
            "Step 200/1584, Loss: 0.0144, LR: 0.002585\n",
            "Step 210/1584, Loss: 0.0183, LR: 0.002560\n",
            "Step 220/1584, Loss: 0.0093, LR: 0.002535\n",
            "Step 230/1584, Loss: 0.0365, LR: 0.002510\n",
            "Step 240/1584, Loss: 0.0396, LR: 0.002485\n",
            "Step 250/1584, Loss: 0.0324, LR: 0.002460\n",
            "Step 260/1584, Loss: 0.0449, LR: 0.002435\n",
            "Step 270/1584, Loss: 0.0659, LR: 0.002410\n",
            "Step 280/1584, Loss: 0.0645, LR: 0.002385\n",
            "Step 290/1584, Loss: 0.0178, LR: 0.002360\n",
            "Step 300/1584, Loss: 0.0182, LR: 0.002335\n",
            "Step 310/1584, Loss: 0.0182, LR: 0.002310\n",
            "Step 320/1584, Loss: 0.0263, LR: 0.002285\n",
            "Step 330/1584, Loss: 0.0262, LR: 0.002260\n",
            "Step 340/1584, Loss: 0.0239, LR: 0.002235\n",
            "Step 350/1584, Loss: 0.0168, LR: 0.002210\n",
            "Step 360/1584, Loss: 0.0184, LR: 0.002185\n",
            "Step 370/1584, Loss: 0.0171, LR: 0.002160\n",
            "Step 380/1584, Loss: 0.0231, LR: 0.002135\n",
            "Step 390/1584, Loss: 0.0402, LR: 0.002110\n",
            "Step 400/1584, Loss: 0.0463, LR: 0.002085\n",
            "Step 410/1584, Loss: 0.0149, LR: 0.002060\n",
            "Step 420/1584, Loss: 0.0121, LR: 0.002035\n",
            "Step 430/1584, Loss: 0.0614, LR: 0.002010\n",
            "Step 440/1584, Loss: 0.0241, LR: 0.001985\n",
            "Step 450/1584, Loss: 0.0227, LR: 0.001960\n",
            "Step 460/1584, Loss: 0.0140, LR: 0.001935\n",
            "Step 470/1584, Loss: 0.0124, LR: 0.001910\n",
            "Step 480/1584, Loss: 0.0162, LR: 0.001885\n",
            "Step 490/1584, Loss: 0.0110, LR: 0.001860\n",
            "Step 500/1584, Loss: 0.0096, LR: 0.001835\n",
            "Step 510/1584, Loss: 0.0116, LR: 0.001810\n",
            "Step 520/1584, Loss: 0.0133, LR: 0.001785\n",
            "Step 530/1584, Loss: 0.0106, LR: 0.001760\n",
            "Step 540/1584, Loss: 0.0189, LR: 0.001735\n",
            "Step 550/1584, Loss: 0.0173, LR: 0.001710\n",
            "Step 560/1584, Loss: 0.0225, LR: 0.001685\n",
            "Step 570/1584, Loss: 0.0169, LR: 0.001660\n",
            "Step 580/1584, Loss: 0.0201, LR: 0.001635\n",
            "Step 590/1584, Loss: 0.0197, LR: 0.001610\n",
            "Step 600/1584, Loss: 0.0539, LR: 0.001585\n",
            "Step 610/1584, Loss: 0.0794, LR: 0.001560\n",
            "Step 620/1584, Loss: 0.0334, LR: 0.001535\n",
            "Step 630/1584, Loss: 0.0223, LR: 0.001510\n",
            "Step 640/1584, Loss: 0.0364, LR: 0.001485\n",
            "Step 650/1584, Loss: 0.0139, LR: 0.001460\n",
            "Step 660/1584, Loss: 0.0331, LR: 0.001435\n",
            "Step 670/1584, Loss: 0.0345, LR: 0.001410\n",
            "Step 680/1584, Loss: 0.0283, LR: 0.001385\n",
            "Step 690/1584, Loss: 0.0260, LR: 0.001360\n",
            "Step 700/1584, Loss: 0.0214, LR: 0.001335\n",
            "Step 710/1584, Loss: 0.0266, LR: 0.001310\n",
            "Step 720/1584, Loss: 0.0249, LR: 0.001285\n",
            "Step 730/1584, Loss: 0.0264, LR: 0.001260\n",
            "Step 740/1584, Loss: 0.0487, LR: 0.001235\n",
            "Step 750/1584, Loss: 0.0619, LR: 0.001210\n",
            "Step 760/1584, Loss: 0.0393, LR: 0.001185\n",
            "Step 770/1584, Loss: 0.0147, LR: 0.001160\n",
            "Step 780/1584, Loss: 0.0261, LR: 0.001135\n",
            "Step 790/1584, Loss: 0.0178, LR: 0.001110\n",
            "Step 800/1584, Loss: 0.0357, LR: 0.001085\n",
            "Step 810/1584, Loss: 0.0341, LR: 0.001060\n",
            "Step 820/1584, Loss: 0.0166, LR: 0.001035\n",
            "Step 830/1584, Loss: 0.0125, LR: 0.001010\n",
            "Step 840/1584, Loss: 0.0211, LR: 0.001015\n",
            "Step 850/1584, Loss: 0.0357, LR: 0.001040\n",
            "Step 860/1584, Loss: 0.0150, LR: 0.001065\n",
            "Step 870/1584, Loss: 0.0234, LR: 0.001090\n",
            "Step 880/1584, Loss: 0.0315, LR: 0.001115\n",
            "Step 890/1584, Loss: 0.0376, LR: 0.001140\n",
            "Step 900/1584, Loss: 0.0202, LR: 0.001165\n",
            "Step 910/1584, Loss: 0.0106, LR: 0.001190\n",
            "Step 920/1584, Loss: 0.0428, LR: 0.001215\n",
            "Step 930/1584, Loss: 0.0790, LR: 0.001240\n",
            "Step 940/1584, Loss: 0.0310, LR: 0.001265\n",
            "Step 950/1584, Loss: 0.0160, LR: 0.001290\n",
            "Step 960/1584, Loss: 0.0224, LR: 0.001315\n",
            "Step 970/1584, Loss: 0.0107, LR: 0.001340\n",
            "Step 980/1584, Loss: 0.0162, LR: 0.001365\n",
            "Step 990/1584, Loss: 0.0190, LR: 0.001390\n",
            "Step 1000/1584, Loss: 0.0132, LR: 0.001415\n",
            "Step 1010/1584, Loss: 0.0096, LR: 0.001440\n",
            "Step 1020/1584, Loss: 0.0337, LR: 0.001465\n",
            "Step 1030/1584, Loss: 0.0455, LR: 0.001490\n",
            "Step 1040/1584, Loss: 0.0268, LR: 0.001515\n",
            "Step 1050/1584, Loss: 0.0230, LR: 0.001540\n",
            "Step 1060/1584, Loss: 0.0193, LR: 0.001565\n",
            "Step 1070/1584, Loss: 0.0142, LR: 0.001590\n",
            "Step 1080/1584, Loss: 0.0071, LR: 0.001615\n",
            "Step 1090/1584, Loss: 0.0284, LR: 0.001640\n",
            "Step 1100/1584, Loss: 0.0103, LR: 0.001665\n",
            "Step 1110/1584, Loss: 0.0088, LR: 0.001690\n",
            "Step 1120/1584, Loss: 0.0388, LR: 0.001715\n",
            "Step 1130/1584, Loss: 0.1080, LR: 0.001740\n",
            "Step 1140/1584, Loss: 0.0726, LR: 0.001765\n",
            "Step 1150/1584, Loss: 0.0239, LR: 0.001790\n",
            "Step 1160/1584, Loss: 0.0120, LR: 0.001815\n",
            "Step 1170/1584, Loss: 0.0331, LR: 0.001840\n",
            "Step 1180/1584, Loss: 0.0349, LR: 0.001865\n",
            "Step 1190/1584, Loss: 0.0130, LR: 0.001890\n",
            "Step 1200/1584, Loss: 0.0154, LR: 0.001915\n",
            "Step 1210/1584, Loss: 0.0144, LR: 0.001940\n",
            "Step 1220/1584, Loss: 0.0198, LR: 0.001965\n",
            "Step 1230/1584, Loss: 0.0112, LR: 0.001990\n",
            "Step 1240/1584, Loss: 0.0073, LR: 0.002015\n",
            "Step 1250/1584, Loss: 0.0139, LR: 0.002040\n",
            "Step 1260/1584, Loss: 0.0232, LR: 0.002065\n",
            "Step 1270/1584, Loss: 0.0188, LR: 0.002090\n",
            "Step 1280/1584, Loss: 0.0217, LR: 0.002115\n",
            "Step 1290/1584, Loss: 0.0206, LR: 0.002140\n",
            "Step 1300/1584, Loss: 0.0148, LR: 0.002165\n",
            "Step 1310/1584, Loss: 0.0225, LR: 0.002190\n",
            "Step 1320/1584, Loss: 0.0203, LR: 0.002215\n",
            "Step 1330/1584, Loss: 0.0230, LR: 0.002240\n",
            "Step 1340/1584, Loss: 0.0219, LR: 0.002265\n",
            "Step 1350/1584, Loss: 0.0127, LR: 0.002290\n",
            "Step 1360/1584, Loss: 0.0127, LR: 0.002315\n",
            "Step 1370/1584, Loss: 0.0285, LR: 0.002340\n",
            "Step 1380/1584, Loss: 0.0392, LR: 0.002365\n",
            "Step 1390/1584, Loss: 0.0192, LR: 0.002390\n",
            "Step 1400/1584, Loss: 0.0076, LR: 0.002415\n",
            "Step 1410/1584, Loss: 0.0199, LR: 0.002440\n",
            "Step 1420/1584, Loss: 0.0250, LR: 0.002465\n",
            "Step 1430/1584, Loss: 0.0269, LR: 0.002490\n",
            "Step 1440/1584, Loss: 0.0152, LR: 0.002515\n",
            "Step 1450/1584, Loss: 0.0122, LR: 0.002540\n",
            "Step 1460/1584, Loss: 0.0087, LR: 0.002565\n",
            "Step 1470/1584, Loss: 0.0116, LR: 0.002590\n",
            "Step 1480/1584, Loss: 0.0125, LR: 0.002615\n",
            "Step 1490/1584, Loss: 0.0080, LR: 0.002640\n",
            "Step 1500/1584, Loss: 0.0113, LR: 0.002665\n",
            "Step 1510/1584, Loss: 0.0102, LR: 0.002690\n",
            "Step 1520/1584, Loss: 0.0053, LR: 0.002715\n",
            "Step 1530/1584, Loss: 0.0211, LR: 0.002740\n",
            "Step 1540/1584, Loss: 0.0170, LR: 0.002765\n",
            "Step 1550/1584, Loss: 0.0209, LR: 0.002790\n",
            "Step 1560/1584, Loss: 0.0295, LR: 0.002815\n",
            "Step 1570/1584, Loss: 0.0166, LR: 0.002840\n",
            "Step 1580/1584, Loss: 0.0220, LR: 0.002865\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3 Results:\n",
            "Training Loss: 0.0235\n",
            "Validation Loss: 0.0793\n",
            "Learning Rate: 0.001992\n",
            "\n",
            "Epoch 4/50\n",
            "Step 0/1584, Loss: 0.0186, LR: 0.002875\n",
            "Step 10/1584, Loss: 0.0243, LR: 0.002898\n",
            "Step 20/1584, Loss: 0.0169, LR: 0.002923\n",
            "Step 30/1584, Loss: 0.0103, LR: 0.002948\n",
            "Step 40/1584, Loss: 0.0085, LR: 0.002972\n",
            "Step 50/1584, Loss: 0.0200, LR: 0.002998\n",
            "Step 60/1584, Loss: 0.0138, LR: 0.003022\n",
            "Step 70/1584, Loss: 0.0190, LR: 0.003048\n",
            "Step 80/1584, Loss: 0.0159, LR: 0.003072\n",
            "Step 90/1584, Loss: 0.0111, LR: 0.003098\n",
            "Step 100/1584, Loss: 0.0061, LR: 0.003123\n",
            "Step 110/1584, Loss: 0.0185, LR: 0.003148\n",
            "Step 120/1584, Loss: 0.0196, LR: 0.003173\n",
            "Step 130/1584, Loss: 0.0111, LR: 0.003198\n",
            "Step 140/1584, Loss: 0.0117, LR: 0.003223\n",
            "Step 150/1584, Loss: 0.0267, LR: 0.003248\n",
            "Step 160/1584, Loss: 0.0099, LR: 0.003273\n",
            "Step 170/1584, Loss: 0.0136, LR: 0.003298\n",
            "Step 180/1584, Loss: 0.0153, LR: 0.003323\n",
            "Step 190/1584, Loss: 0.0199, LR: 0.003348\n",
            "Step 200/1584, Loss: 0.0149, LR: 0.003373\n",
            "Step 210/1584, Loss: 0.0179, LR: 0.003398\n",
            "Step 220/1584, Loss: 0.0099, LR: 0.003423\n",
            "Step 230/1584, Loss: 0.0312, LR: 0.003448\n",
            "Step 240/1584, Loss: 0.0444, LR: 0.003473\n",
            "Step 250/1584, Loss: 0.0302, LR: 0.003497\n",
            "Step 260/1584, Loss: 0.0420, LR: 0.003523\n",
            "Step 270/1584, Loss: 0.0651, LR: 0.003548\n",
            "Step 280/1584, Loss: 0.0654, LR: 0.003573\n",
            "Step 290/1584, Loss: 0.0224, LR: 0.003598\n",
            "Step 300/1584, Loss: 0.0185, LR: 0.003623\n",
            "Step 310/1584, Loss: 0.0164, LR: 0.003648\n",
            "Step 320/1584, Loss: 0.0256, LR: 0.003673\n",
            "Step 330/1584, Loss: 0.0273, LR: 0.003698\n",
            "Step 340/1584, Loss: 0.0259, LR: 0.003723\n",
            "Step 350/1584, Loss: 0.0164, LR: 0.003748\n",
            "Step 360/1584, Loss: 0.0191, LR: 0.003773\n",
            "Step 370/1584, Loss: 0.0157, LR: 0.003798\n",
            "Step 380/1584, Loss: 0.0235, LR: 0.003823\n",
            "Step 390/1584, Loss: 0.0377, LR: 0.003848\n",
            "Step 400/1584, Loss: 0.0512, LR: 0.003873\n",
            "Step 410/1584, Loss: 0.0157, LR: 0.003898\n",
            "Step 420/1584, Loss: 0.0147, LR: 0.003923\n",
            "Step 430/1584, Loss: 0.0565, LR: 0.003948\n",
            "Step 440/1584, Loss: 0.0287, LR: 0.003973\n",
            "Step 450/1584, Loss: 0.0246, LR: 0.003998\n",
            "Step 460/1584, Loss: 0.0173, LR: 0.004023\n",
            "Step 470/1584, Loss: 0.0129, LR: 0.004048\n",
            "Step 480/1584, Loss: 0.0124, LR: 0.004073\n",
            "Step 490/1584, Loss: 0.0146, LR: 0.004098\n",
            "Step 500/1584, Loss: 0.0093, LR: 0.004123\n",
            "Step 510/1584, Loss: 0.0109, LR: 0.004148\n",
            "Step 520/1584, Loss: 0.0134, LR: 0.004173\n",
            "Step 530/1584, Loss: 0.0113, LR: 0.004198\n",
            "Step 540/1584, Loss: 0.0192, LR: 0.004223\n",
            "Step 550/1584, Loss: 0.0159, LR: 0.004248\n",
            "Step 560/1584, Loss: 0.0235, LR: 0.004273\n",
            "Step 570/1584, Loss: 0.0155, LR: 0.004298\n",
            "Step 580/1584, Loss: 0.0184, LR: 0.004323\n",
            "Step 590/1584, Loss: 0.0177, LR: 0.004348\n",
            "Step 600/1584, Loss: 0.0432, LR: 0.004373\n",
            "Step 610/1584, Loss: 0.0724, LR: 0.004398\n",
            "Step 620/1584, Loss: 0.0424, LR: 0.004423\n",
            "Step 630/1584, Loss: 0.0299, LR: 0.004448\n",
            "Step 640/1584, Loss: 0.0382, LR: 0.004473\n",
            "Step 650/1584, Loss: 0.0131, LR: 0.004498\n",
            "Step 660/1584, Loss: 0.0312, LR: 0.004523\n",
            "Step 670/1584, Loss: 0.0385, LR: 0.004548\n",
            "Step 680/1584, Loss: 0.0272, LR: 0.004573\n",
            "Step 690/1584, Loss: 0.0255, LR: 0.004598\n",
            "Step 700/1584, Loss: 0.0208, LR: 0.004623\n",
            "Step 710/1584, Loss: 0.0232, LR: 0.004648\n",
            "Step 720/1584, Loss: 0.0303, LR: 0.004673\n",
            "Step 730/1584, Loss: 0.0283, LR: 0.004698\n",
            "Step 740/1584, Loss: 0.0447, LR: 0.004723\n",
            "Step 750/1584, Loss: 0.0587, LR: 0.004748\n",
            "Step 760/1584, Loss: 0.0409, LR: 0.004773\n",
            "Step 770/1584, Loss: 0.0191, LR: 0.004798\n",
            "Step 780/1584, Loss: 0.0349, LR: 0.004823\n",
            "Step 790/1584, Loss: 0.0170, LR: 0.004848\n",
            "Step 800/1584, Loss: 0.0410, LR: 0.004873\n",
            "Step 810/1584, Loss: 0.0392, LR: 0.004898\n",
            "Step 820/1584, Loss: 0.0195, LR: 0.004923\n",
            "Step 830/1584, Loss: 0.0116, LR: 0.004948\n",
            "Step 840/1584, Loss: 0.0167, LR: 0.004973\n",
            "Step 850/1584, Loss: 0.0406, LR: 0.004998\n",
            "Step 860/1584, Loss: 0.0114, LR: 0.005023\n",
            "Step 870/1584, Loss: 0.0195, LR: 0.005048\n",
            "Step 880/1584, Loss: 0.0341, LR: 0.005073\n",
            "Step 890/1584, Loss: 0.0395, LR: 0.005098\n",
            "Step 900/1584, Loss: 0.0246, LR: 0.005123\n",
            "Step 910/1584, Loss: 0.0124, LR: 0.005148\n",
            "Step 920/1584, Loss: 0.0365, LR: 0.005173\n",
            "Step 930/1584, Loss: 0.0782, LR: 0.005198\n",
            "Step 940/1584, Loss: 0.0408, LR: 0.005223\n",
            "Step 950/1584, Loss: 0.0165, LR: 0.005248\n",
            "Step 960/1584, Loss: 0.0228, LR: 0.005273\n",
            "Step 970/1584, Loss: 0.0116, LR: 0.005298\n",
            "Step 980/1584, Loss: 0.0146, LR: 0.005323\n",
            "Step 990/1584, Loss: 0.0200, LR: 0.005348\n",
            "Step 1000/1584, Loss: 0.0134, LR: 0.005373\n",
            "Step 1010/1584, Loss: 0.0082, LR: 0.005398\n",
            "Step 1020/1584, Loss: 0.0345, LR: 0.005423\n",
            "Step 1030/1584, Loss: 0.0456, LR: 0.005448\n",
            "Step 1040/1584, Loss: 0.0376, LR: 0.005473\n",
            "Step 1050/1584, Loss: 0.0243, LR: 0.005498\n",
            "Step 1060/1584, Loss: 0.0206, LR: 0.005523\n",
            "Step 1070/1584, Loss: 0.0126, LR: 0.005548\n",
            "Step 1080/1584, Loss: 0.0073, LR: 0.005573\n",
            "Step 1090/1584, Loss: 0.0323, LR: 0.005598\n",
            "Step 1100/1584, Loss: 0.0087, LR: 0.005623\n",
            "Step 1110/1584, Loss: 0.0082, LR: 0.005648\n",
            "Step 1120/1584, Loss: 0.0366, LR: 0.005673\n",
            "Step 1130/1584, Loss: 0.1005, LR: 0.005698\n",
            "Step 1140/1584, Loss: 0.0776, LR: 0.005723\n",
            "Step 1150/1584, Loss: 0.0267, LR: 0.005748\n",
            "Step 1160/1584, Loss: 0.0222, LR: 0.005773\n",
            "Step 1170/1584, Loss: 0.0299, LR: 0.005798\n",
            "Step 1180/1584, Loss: 0.0394, LR: 0.005823\n",
            "Step 1190/1584, Loss: 0.0091, LR: 0.005848\n",
            "Step 1200/1584, Loss: 0.0195, LR: 0.005873\n",
            "Step 1210/1584, Loss: 0.0146, LR: 0.005898\n",
            "Step 1220/1584, Loss: 0.0199, LR: 0.005923\n",
            "Step 1230/1584, Loss: 0.0124, LR: 0.005948\n",
            "Step 1240/1584, Loss: 0.0093, LR: 0.005973\n",
            "Step 1250/1584, Loss: 0.0111, LR: 0.005998\n",
            "Step 1260/1584, Loss: 0.0261, LR: 0.005977\n",
            "Step 1270/1584, Loss: 0.0184, LR: 0.005952\n",
            "Step 1280/1584, Loss: 0.0230, LR: 0.005927\n",
            "Step 1290/1584, Loss: 0.0238, LR: 0.005902\n",
            "Step 1300/1584, Loss: 0.0149, LR: 0.005877\n",
            "Step 1310/1584, Loss: 0.0222, LR: 0.005852\n",
            "Step 1320/1584, Loss: 0.0177, LR: 0.005827\n",
            "Step 1330/1584, Loss: 0.0268, LR: 0.005802\n",
            "Step 1340/1584, Loss: 0.0192, LR: 0.005777\n",
            "Step 1350/1584, Loss: 0.0125, LR: 0.005752\n",
            "Step 1360/1584, Loss: 0.0148, LR: 0.005727\n",
            "Step 1370/1584, Loss: 0.0272, LR: 0.005702\n",
            "Step 1380/1584, Loss: 0.0422, LR: 0.005677\n",
            "Step 1390/1584, Loss: 0.0220, LR: 0.005652\n",
            "Step 1400/1584, Loss: 0.0101, LR: 0.005627\n",
            "Step 1410/1584, Loss: 0.0190, LR: 0.005602\n",
            "Step 1420/1584, Loss: 0.0204, LR: 0.005577\n",
            "Step 1430/1584, Loss: 0.0329, LR: 0.005552\n",
            "Step 1440/1584, Loss: 0.0145, LR: 0.005527\n",
            "Step 1450/1584, Loss: 0.0105, LR: 0.005502\n",
            "Step 1460/1584, Loss: 0.0087, LR: 0.005477\n",
            "Step 1470/1584, Loss: 0.0121, LR: 0.005452\n",
            "Step 1480/1584, Loss: 0.0148, LR: 0.005427\n",
            "Step 1490/1584, Loss: 0.0079, LR: 0.005402\n",
            "Step 1500/1584, Loss: 0.0120, LR: 0.005377\n",
            "Step 1510/1584, Loss: 0.0106, LR: 0.005352\n",
            "Step 1520/1584, Loss: 0.0046, LR: 0.005327\n",
            "Step 1530/1584, Loss: 0.0202, LR: 0.005302\n",
            "Step 1540/1584, Loss: 0.0170, LR: 0.005277\n",
            "Step 1550/1584, Loss: 0.0189, LR: 0.005252\n",
            "Step 1560/1584, Loss: 0.0282, LR: 0.005227\n",
            "Step 1570/1584, Loss: 0.0172, LR: 0.005202\n",
            "Step 1580/1584, Loss: 0.0215, LR: 0.005177\n",
            "\n",
            "Epoch 4 Results:\n",
            "Training Loss: 0.0240\n",
            "Validation Loss: 0.0835\n",
            "Learning Rate: 0.004678\n",
            "\n",
            "Epoch 5/50\n",
            "Step 0/1584, Loss: 0.0203, LR: 0.005167\n",
            "Step 10/1584, Loss: 0.0249, LR: 0.005145\n",
            "Step 20/1584, Loss: 0.0169, LR: 0.005120\n",
            "Step 30/1584, Loss: 0.0096, LR: 0.005095\n",
            "Step 40/1584, Loss: 0.0082, LR: 0.005070\n",
            "Step 50/1584, Loss: 0.0210, LR: 0.005045\n",
            "Step 60/1584, Loss: 0.0123, LR: 0.005020\n",
            "Step 70/1584, Loss: 0.0198, LR: 0.004995\n",
            "Step 80/1584, Loss: 0.0160, LR: 0.004970\n",
            "Step 90/1584, Loss: 0.0124, LR: 0.004945\n",
            "Step 100/1584, Loss: 0.0060, LR: 0.004920\n",
            "Step 110/1584, Loss: 0.0167, LR: 0.004895\n",
            "Step 120/1584, Loss: 0.0204, LR: 0.004870\n",
            "Step 130/1584, Loss: 0.0121, LR: 0.004845\n",
            "Step 140/1584, Loss: 0.0118, LR: 0.004820\n",
            "Step 150/1584, Loss: 0.0272, LR: 0.004795\n",
            "Step 160/1584, Loss: 0.0103, LR: 0.004770\n",
            "Step 170/1584, Loss: 0.0144, LR: 0.004745\n",
            "Step 180/1584, Loss: 0.0141, LR: 0.004720\n",
            "Step 190/1584, Loss: 0.0209, LR: 0.004695\n",
            "Step 200/1584, Loss: 0.0150, LR: 0.004670\n",
            "Step 210/1584, Loss: 0.0172, LR: 0.004645\n",
            "Step 220/1584, Loss: 0.0112, LR: 0.004620\n",
            "Step 230/1584, Loss: 0.0250, LR: 0.004595\n",
            "Step 240/1584, Loss: 0.0505, LR: 0.004570\n",
            "Step 250/1584, Loss: 0.0265, LR: 0.004545\n",
            "Step 260/1584, Loss: 0.0420, LR: 0.004520\n",
            "Step 270/1584, Loss: 0.0635, LR: 0.004495\n",
            "Step 280/1584, Loss: 0.0656, LR: 0.004470\n",
            "Step 290/1584, Loss: 0.0275, LR: 0.004445\n",
            "Step 300/1584, Loss: 0.0185, LR: 0.004420\n",
            "Step 310/1584, Loss: 0.0157, LR: 0.004395\n",
            "Step 320/1584, Loss: 0.0247, LR: 0.004370\n",
            "Step 330/1584, Loss: 0.0275, LR: 0.004345\n",
            "Step 340/1584, Loss: 0.0276, LR: 0.004320\n",
            "Step 350/1584, Loss: 0.0159, LR: 0.004295\n",
            "Step 360/1584, Loss: 0.0203, LR: 0.004270\n",
            "Step 370/1584, Loss: 0.0149, LR: 0.004245\n",
            "Step 380/1584, Loss: 0.0234, LR: 0.004220\n",
            "Step 390/1584, Loss: 0.0322, LR: 0.004195\n",
            "Step 400/1584, Loss: 0.0576, LR: 0.004170\n",
            "Step 410/1584, Loss: 0.0149, LR: 0.004145\n",
            "Step 420/1584, Loss: 0.0162, LR: 0.004120\n",
            "Step 430/1584, Loss: 0.0480, LR: 0.004095\n",
            "Step 440/1584, Loss: 0.0355, LR: 0.004070\n",
            "Step 450/1584, Loss: 0.0259, LR: 0.004045\n",
            "Step 460/1584, Loss: 0.0175, LR: 0.004020\n",
            "Step 470/1584, Loss: 0.0133, LR: 0.003995\n",
            "Step 480/1584, Loss: 0.0095, LR: 0.003970\n",
            "Step 490/1584, Loss: 0.0173, LR: 0.003945\n",
            "Step 500/1584, Loss: 0.0098, LR: 0.003920\n",
            "Step 510/1584, Loss: 0.0099, LR: 0.003895\n",
            "Step 520/1584, Loss: 0.0139, LR: 0.003870\n",
            "Step 530/1584, Loss: 0.0117, LR: 0.003845\n",
            "Step 540/1584, Loss: 0.0188, LR: 0.003820\n",
            "Step 550/1584, Loss: 0.0141, LR: 0.003795\n",
            "Step 560/1584, Loss: 0.0248, LR: 0.003770\n",
            "Step 570/1584, Loss: 0.0156, LR: 0.003745\n",
            "Step 580/1584, Loss: 0.0172, LR: 0.003720\n",
            "Step 590/1584, Loss: 0.0184, LR: 0.003695\n",
            "Step 600/1584, Loss: 0.0382, LR: 0.003670\n",
            "Step 610/1584, Loss: 0.0756, LR: 0.003645\n",
            "Step 620/1584, Loss: 0.0430, LR: 0.003620\n",
            "Step 630/1584, Loss: 0.0304, LR: 0.003595\n",
            "Step 640/1584, Loss: 0.0382, LR: 0.003570\n",
            "Step 650/1584, Loss: 0.0148, LR: 0.003545\n",
            "Step 660/1584, Loss: 0.0275, LR: 0.003520\n",
            "Step 670/1584, Loss: 0.0401, LR: 0.003495\n",
            "Step 680/1584, Loss: 0.0267, LR: 0.003470\n",
            "Step 690/1584, Loss: 0.0267, LR: 0.003445\n",
            "Step 700/1584, Loss: 0.0220, LR: 0.003420\n",
            "Step 710/1584, Loss: 0.0159, LR: 0.003395\n",
            "Step 720/1584, Loss: 0.0344, LR: 0.003370\n",
            "Step 730/1584, Loss: 0.0299, LR: 0.003345\n",
            "Step 740/1584, Loss: 0.0407, LR: 0.003320\n",
            "Step 750/1584, Loss: 0.0600, LR: 0.003295\n",
            "Step 760/1584, Loss: 0.0461, LR: 0.003270\n",
            "Step 770/1584, Loss: 0.0162, LR: 0.003245\n",
            "Step 780/1584, Loss: 0.0339, LR: 0.003220\n",
            "Step 790/1584, Loss: 0.0165, LR: 0.003195\n",
            "Step 800/1584, Loss: 0.0344, LR: 0.003170\n",
            "Step 810/1584, Loss: 0.0396, LR: 0.003145\n",
            "Step 820/1584, Loss: 0.0200, LR: 0.003120\n",
            "Step 830/1584, Loss: 0.0112, LR: 0.003095\n",
            "Step 840/1584, Loss: 0.0132, LR: 0.003070\n",
            "Step 850/1584, Loss: 0.0435, LR: 0.003045\n",
            "Step 860/1584, Loss: 0.0127, LR: 0.003020\n",
            "Step 870/1584, Loss: 0.0173, LR: 0.002995\n",
            "Step 880/1584, Loss: 0.0336, LR: 0.002970\n",
            "Step 890/1584, Loss: 0.0378, LR: 0.002945\n",
            "Step 900/1584, Loss: 0.0260, LR: 0.002920\n",
            "Step 910/1584, Loss: 0.0117, LR: 0.002895\n",
            "Step 920/1584, Loss: 0.0292, LR: 0.002870\n",
            "Step 930/1584, Loss: 0.0797, LR: 0.002845\n",
            "Step 940/1584, Loss: 0.0431, LR: 0.002820\n",
            "Step 950/1584, Loss: 0.0173, LR: 0.002795\n",
            "Step 960/1584, Loss: 0.0229, LR: 0.002770\n",
            "Step 970/1584, Loss: 0.0117, LR: 0.002745\n",
            "Step 980/1584, Loss: 0.0129, LR: 0.002720\n",
            "Step 990/1584, Loss: 0.0207, LR: 0.002695\n",
            "Step 1000/1584, Loss: 0.0144, LR: 0.002670\n",
            "Step 1010/1584, Loss: 0.0095, LR: 0.002645\n",
            "Step 1020/1584, Loss: 0.0271, LR: 0.002620\n",
            "Step 1030/1584, Loss: 0.0478, LR: 0.002595\n",
            "Step 1040/1584, Loss: 0.0316, LR: 0.002570\n",
            "Step 1050/1584, Loss: 0.0236, LR: 0.002545\n",
            "Step 1060/1584, Loss: 0.0210, LR: 0.002520\n",
            "Step 1070/1584, Loss: 0.0162, LR: 0.002495\n",
            "Step 1080/1584, Loss: 0.0066, LR: 0.002470\n",
            "Step 1090/1584, Loss: 0.0287, LR: 0.002445\n",
            "Step 1100/1584, Loss: 0.0093, LR: 0.002420\n",
            "Step 1110/1584, Loss: 0.0083, LR: 0.002395\n",
            "Step 1120/1584, Loss: 0.0329, LR: 0.002370\n",
            "Step 1130/1584, Loss: 0.0958, LR: 0.002345\n",
            "Step 1140/1584, Loss: 0.0909, LR: 0.002320\n",
            "Step 1150/1584, Loss: 0.0228, LR: 0.002295\n",
            "Step 1160/1584, Loss: 0.0166, LR: 0.002270\n",
            "Step 1170/1584, Loss: 0.0270, LR: 0.002245\n",
            "Step 1180/1584, Loss: 0.0395, LR: 0.002220\n",
            "Step 1190/1584, Loss: 0.0148, LR: 0.002195\n",
            "Step 1200/1584, Loss: 0.0144, LR: 0.002170\n",
            "Step 1210/1584, Loss: 0.0137, LR: 0.002145\n",
            "Step 1220/1584, Loss: 0.0205, LR: 0.002120\n",
            "Step 1230/1584, Loss: 0.0136, LR: 0.002095\n",
            "Step 1240/1584, Loss: 0.0071, LR: 0.002070\n",
            "Step 1250/1584, Loss: 0.0098, LR: 0.002045\n",
            "Step 1260/1584, Loss: 0.0239, LR: 0.002020\n",
            "Step 1270/1584, Loss: 0.0201, LR: 0.001995\n",
            "Step 1280/1584, Loss: 0.0178, LR: 0.001970\n",
            "Step 1290/1584, Loss: 0.0249, LR: 0.001945\n",
            "Step 1300/1584, Loss: 0.0133, LR: 0.001920\n",
            "Step 1310/1584, Loss: 0.0223, LR: 0.001895\n",
            "Step 1320/1584, Loss: 0.0188, LR: 0.001870\n",
            "Step 1330/1584, Loss: 0.0259, LR: 0.001845\n",
            "Step 1340/1584, Loss: 0.0208, LR: 0.001820\n",
            "Step 1350/1584, Loss: 0.0149, LR: 0.001795\n",
            "Step 1360/1584, Loss: 0.0125, LR: 0.001770\n",
            "Step 1370/1584, Loss: 0.0198, LR: 0.001745\n",
            "Step 1380/1584, Loss: 0.0461, LR: 0.001720\n",
            "Step 1390/1584, Loss: 0.0180, LR: 0.001695\n",
            "Step 1400/1584, Loss: 0.0104, LR: 0.001670\n",
            "Step 1410/1584, Loss: 0.0181, LR: 0.001645\n",
            "Step 1420/1584, Loss: 0.0152, LR: 0.001620\n",
            "Step 1430/1584, Loss: 0.0380, LR: 0.001595\n",
            "Step 1440/1584, Loss: 0.0134, LR: 0.001570\n",
            "Step 1450/1584, Loss: 0.0144, LR: 0.001545\n",
            "Step 1460/1584, Loss: 0.0089, LR: 0.001520\n",
            "Step 1470/1584, Loss: 0.0090, LR: 0.001495\n",
            "Step 1480/1584, Loss: 0.0145, LR: 0.001470\n",
            "Step 1490/1584, Loss: 0.0080, LR: 0.001445\n",
            "Step 1500/1584, Loss: 0.0106, LR: 0.001420\n",
            "Step 1510/1584, Loss: 0.0114, LR: 0.001395\n",
            "Step 1520/1584, Loss: 0.0049, LR: 0.001370\n",
            "Step 1530/1584, Loss: 0.0186, LR: 0.001345\n",
            "Step 1540/1584, Loss: 0.0180, LR: 0.001320\n",
            "Step 1550/1584, Loss: 0.0200, LR: 0.001295\n",
            "Step 1560/1584, Loss: 0.0316, LR: 0.001270\n",
            "Step 1570/1584, Loss: 0.0186, LR: 0.001245\n",
            "Step 1580/1584, Loss: 0.0228, LR: 0.001220\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5 Results:\n",
            "Training Loss: 0.0238\n",
            "Validation Loss: 0.0648\n",
            "Learning Rate: 0.003190\n",
            "\n",
            "Saved checkpoint for epoch 5: ./training_checkpoints/ckpt-1\n",
            "\n",
            "Epoch 6/50\n",
            "Step 0/1584, Loss: 0.0227, LR: 0.001210\n",
            "Step 10/1584, Loss: 0.0275, LR: 0.001187\n",
            "Step 20/1584, Loss: 0.0120, LR: 0.001162\n",
            "Step 30/1584, Loss: 0.0105, LR: 0.001137\n",
            "Step 40/1584, Loss: 0.0100, LR: 0.001112\n",
            "Step 50/1584, Loss: 0.0181, LR: 0.001087\n",
            "Step 60/1584, Loss: 0.0159, LR: 0.001062\n",
            "Step 70/1584, Loss: 0.0207, LR: 0.001037\n",
            "Step 80/1584, Loss: 0.0137, LR: 0.001012\n",
            "Step 90/1584, Loss: 0.0129, LR: 0.001013\n",
            "Step 100/1584, Loss: 0.0060, LR: 0.001038\n",
            "Step 110/1584, Loss: 0.0156, LR: 0.001063\n",
            "Step 120/1584, Loss: 0.0225, LR: 0.001088\n",
            "Step 130/1584, Loss: 0.0145, LR: 0.001113\n",
            "Step 140/1584, Loss: 0.0099, LR: 0.001138\n",
            "Step 150/1584, Loss: 0.0235, LR: 0.001163\n",
            "Step 160/1584, Loss: 0.0095, LR: 0.001188\n",
            "Step 170/1584, Loss: 0.0135, LR: 0.001213\n",
            "Step 180/1584, Loss: 0.0133, LR: 0.001238\n",
            "Step 190/1584, Loss: 0.0221, LR: 0.001263\n",
            "Step 200/1584, Loss: 0.0146, LR: 0.001288\n",
            "Step 210/1584, Loss: 0.0173, LR: 0.001313\n",
            "Step 220/1584, Loss: 0.0113, LR: 0.001338\n",
            "Step 230/1584, Loss: 0.0186, LR: 0.001363\n",
            "Step 240/1584, Loss: 0.0579, LR: 0.001388\n",
            "Step 250/1584, Loss: 0.0235, LR: 0.001412\n",
            "Step 260/1584, Loss: 0.0375, LR: 0.001438\n",
            "Step 270/1584, Loss: 0.0673, LR: 0.001463\n",
            "Step 280/1584, Loss: 0.0696, LR: 0.001488\n",
            "Step 290/1584, Loss: 0.0295, LR: 0.001512\n",
            "Step 300/1584, Loss: 0.0159, LR: 0.001538\n",
            "Step 310/1584, Loss: 0.0166, LR: 0.001563\n",
            "Step 320/1584, Loss: 0.0247, LR: 0.001588\n",
            "Step 330/1584, Loss: 0.0248, LR: 0.001613\n",
            "Step 340/1584, Loss: 0.0281, LR: 0.001638\n",
            "Step 350/1584, Loss: 0.0155, LR: 0.001663\n",
            "Step 360/1584, Loss: 0.0213, LR: 0.001688\n",
            "Step 370/1584, Loss: 0.0146, LR: 0.001713\n",
            "Step 380/1584, Loss: 0.0232, LR: 0.001738\n",
            "Step 390/1584, Loss: 0.0255, LR: 0.001763\n",
            "Step 400/1584, Loss: 0.0585, LR: 0.001788\n",
            "Step 410/1584, Loss: 0.0174, LR: 0.001813\n",
            "Step 420/1584, Loss: 0.0140, LR: 0.001838\n",
            "Step 430/1584, Loss: 0.0372, LR: 0.001863\n",
            "Step 440/1584, Loss: 0.0429, LR: 0.001888\n",
            "Step 450/1584, Loss: 0.0275, LR: 0.001913\n",
            "Step 460/1584, Loss: 0.0133, LR: 0.001937\n",
            "Step 470/1584, Loss: 0.0137, LR: 0.001963\n",
            "Step 480/1584, Loss: 0.0098, LR: 0.001988\n",
            "Step 490/1584, Loss: 0.0168, LR: 0.002013\n",
            "Step 500/1584, Loss: 0.0109, LR: 0.002038\n",
            "Step 510/1584, Loss: 0.0087, LR: 0.002063\n",
            "Step 520/1584, Loss: 0.0144, LR: 0.002088\n",
            "Step 530/1584, Loss: 0.0121, LR: 0.002113\n",
            "Step 540/1584, Loss: 0.0182, LR: 0.002138\n",
            "Step 550/1584, Loss: 0.0126, LR: 0.002163\n",
            "Step 560/1584, Loss: 0.0253, LR: 0.002188\n",
            "Step 570/1584, Loss: 0.0167, LR: 0.002213\n",
            "Step 580/1584, Loss: 0.0161, LR: 0.002238\n",
            "Step 590/1584, Loss: 0.0208, LR: 0.002263\n",
            "Step 600/1584, Loss: 0.0324, LR: 0.002288\n",
            "Step 610/1584, Loss: 0.0829, LR: 0.002313\n",
            "Step 620/1584, Loss: 0.0429, LR: 0.002338\n",
            "Step 630/1584, Loss: 0.0290, LR: 0.002363\n",
            "Step 640/1584, Loss: 0.0343, LR: 0.002388\n",
            "Step 650/1584, Loss: 0.0194, LR: 0.002413\n",
            "Step 660/1584, Loss: 0.0245, LR: 0.002438\n",
            "Step 670/1584, Loss: 0.0411, LR: 0.002463\n",
            "Step 680/1584, Loss: 0.0269, LR: 0.002488\n",
            "Step 690/1584, Loss: 0.0276, LR: 0.002513\n",
            "Step 700/1584, Loss: 0.0233, LR: 0.002538\n",
            "Step 710/1584, Loss: 0.0149, LR: 0.002563\n",
            "Step 720/1584, Loss: 0.0333, LR: 0.002588\n",
            "Step 730/1584, Loss: 0.0308, LR: 0.002613\n",
            "Step 740/1584, Loss: 0.0363, LR: 0.002638\n",
            "Step 750/1584, Loss: 0.0610, LR: 0.002662\n",
            "Step 760/1584, Loss: 0.0508, LR: 0.002688\n",
            "Step 770/1584, Loss: 0.0150, LR: 0.002713\n",
            "Step 780/1584, Loss: 0.0308, LR: 0.002738\n",
            "Step 790/1584, Loss: 0.0185, LR: 0.002763\n",
            "Step 800/1584, Loss: 0.0311, LR: 0.002788\n",
            "Step 810/1584, Loss: 0.0398, LR: 0.002813\n",
            "Step 820/1584, Loss: 0.0208, LR: 0.002838\n",
            "Step 830/1584, Loss: 0.0122, LR: 0.002863\n",
            "Step 840/1584, Loss: 0.0128, LR: 0.002888\n",
            "Step 850/1584, Loss: 0.0427, LR: 0.002913\n",
            "Step 860/1584, Loss: 0.0143, LR: 0.002938\n",
            "Step 870/1584, Loss: 0.0155, LR: 0.002963\n",
            "Step 880/1584, Loss: 0.0332, LR: 0.002988\n",
            "Step 890/1584, Loss: 0.0366, LR: 0.003013\n",
            "Step 900/1584, Loss: 0.0292, LR: 0.003038\n",
            "Step 910/1584, Loss: 0.0123, LR: 0.003063\n",
            "Step 920/1584, Loss: 0.0221, LR: 0.003088\n",
            "Step 930/1584, Loss: 0.0797, LR: 0.003113\n",
            "Step 940/1584, Loss: 0.0492, LR: 0.003138\n",
            "Step 950/1584, Loss: 0.0180, LR: 0.003163\n",
            "Step 960/1584, Loss: 0.0224, LR: 0.003187\n",
            "Step 970/1584, Loss: 0.0130, LR: 0.003213\n",
            "Step 980/1584, Loss: 0.0112, LR: 0.003238\n",
            "Step 990/1584, Loss: 0.0216, LR: 0.003263\n",
            "Step 1000/1584, Loss: 0.0151, LR: 0.003287\n",
            "Step 1010/1584, Loss: 0.0093, LR: 0.003313\n",
            "Step 1020/1584, Loss: 0.0236, LR: 0.003338\n",
            "Step 1030/1584, Loss: 0.0491, LR: 0.003363\n",
            "Step 1040/1584, Loss: 0.0335, LR: 0.003388\n",
            "Step 1050/1584, Loss: 0.0248, LR: 0.003413\n",
            "Step 1060/1584, Loss: 0.0227, LR: 0.003438\n",
            "Step 1070/1584, Loss: 0.0160, LR: 0.003463\n",
            "Step 1080/1584, Loss: 0.0066, LR: 0.003488\n",
            "Step 1090/1584, Loss: 0.0297, LR: 0.003513\n",
            "Step 1100/1584, Loss: 0.0087, LR: 0.003538\n",
            "Step 1110/1584, Loss: 0.0072, LR: 0.003563\n",
            "Step 1120/1584, Loss: 0.0310, LR: 0.003588\n",
            "Step 1130/1584, Loss: 0.0898, LR: 0.003613\n",
            "Step 1140/1584, Loss: 0.0972, LR: 0.003638\n",
            "Step 1150/1584, Loss: 0.0218, LR: 0.003663\n",
            "Step 1160/1584, Loss: 0.0226, LR: 0.003688\n",
            "Step 1170/1584, Loss: 0.0267, LR: 0.003712\n",
            "Step 1180/1584, Loss: 0.0412, LR: 0.003738\n",
            "Step 1190/1584, Loss: 0.0133, LR: 0.003763\n",
            "Step 1200/1584, Loss: 0.0150, LR: 0.003788\n",
            "Step 1210/1584, Loss: 0.0144, LR: 0.003812\n",
            "Step 1220/1584, Loss: 0.0209, LR: 0.003838\n",
            "Step 1230/1584, Loss: 0.0150, LR: 0.003863\n",
            "Step 1240/1584, Loss: 0.0077, LR: 0.003888\n",
            "Step 1250/1584, Loss: 0.0096, LR: 0.003913\n",
            "Step 1260/1584, Loss: 0.0233, LR: 0.003938\n",
            "Step 1270/1584, Loss: 0.0202, LR: 0.003963\n",
            "Step 1280/1584, Loss: 0.0178, LR: 0.003988\n",
            "Step 1290/1584, Loss: 0.0263, LR: 0.004013\n",
            "Step 1300/1584, Loss: 0.0136, LR: 0.004038\n",
            "Step 1310/1584, Loss: 0.0224, LR: 0.004063\n",
            "Step 1320/1584, Loss: 0.0200, LR: 0.004088\n",
            "Step 1330/1584, Loss: 0.0253, LR: 0.004113\n",
            "Step 1340/1584, Loss: 0.0193, LR: 0.004138\n",
            "Step 1350/1584, Loss: 0.0138, LR: 0.004163\n",
            "Step 1360/1584, Loss: 0.0139, LR: 0.004188\n",
            "Step 1370/1584, Loss: 0.0162, LR: 0.004213\n",
            "Step 1380/1584, Loss: 0.0543, LR: 0.004238\n",
            "Step 1390/1584, Loss: 0.0178, LR: 0.004263\n",
            "Step 1400/1584, Loss: 0.0126, LR: 0.004288\n",
            "Step 1410/1584, Loss: 0.0175, LR: 0.004313\n",
            "Step 1420/1584, Loss: 0.0103, LR: 0.004337\n",
            "Step 1430/1584, Loss: 0.0435, LR: 0.004363\n",
            "Step 1440/1584, Loss: 0.0133, LR: 0.004388\n",
            "Step 1450/1584, Loss: 0.0128, LR: 0.004413\n",
            "Step 1460/1584, Loss: 0.0074, LR: 0.004438\n",
            "Step 1470/1584, Loss: 0.0120, LR: 0.004463\n",
            "Step 1480/1584, Loss: 0.0150, LR: 0.004488\n",
            "Step 1490/1584, Loss: 0.0085, LR: 0.004513\n",
            "Step 1500/1584, Loss: 0.0102, LR: 0.004538\n",
            "Step 1510/1584, Loss: 0.0125, LR: 0.004563\n",
            "Step 1520/1584, Loss: 0.0049, LR: 0.004588\n",
            "Step 1530/1584, Loss: 0.0170, LR: 0.004613\n",
            "Step 1540/1584, Loss: 0.0180, LR: 0.004638\n",
            "Step 1550/1584, Loss: 0.0173, LR: 0.004663\n",
            "Step 1560/1584, Loss: 0.0278, LR: 0.004688\n",
            "Step 1570/1584, Loss: 0.0187, LR: 0.004713\n",
            "Step 1580/1584, Loss: 0.0209, LR: 0.004738\n",
            "\n",
            "Epoch 6 Results:\n",
            "Training Loss: 0.0237\n",
            "Validation Loss: 0.0845\n",
            "Learning Rate: 0.002779\n",
            "\n",
            "Epoch 7/50\n",
            "Step 0/1584, Loss: 0.0216, LR: 0.004748\n",
            "Step 10/1584, Loss: 0.0261, LR: 0.004770\n",
            "Step 20/1584, Loss: 0.0140, LR: 0.004795\n",
            "Step 30/1584, Loss: 0.0129, LR: 0.004820\n",
            "Step 40/1584, Loss: 0.0105, LR: 0.004845\n",
            "Step 50/1584, Loss: 0.0188, LR: 0.004870\n",
            "Step 60/1584, Loss: 0.0119, LR: 0.004895\n",
            "Step 70/1584, Loss: 0.0211, LR: 0.004920\n",
            "Step 80/1584, Loss: 0.0142, LR: 0.004945\n",
            "Step 90/1584, Loss: 0.0145, LR: 0.004970\n",
            "Step 100/1584, Loss: 0.0056, LR: 0.004995\n",
            "Step 110/1584, Loss: 0.0142, LR: 0.005020\n",
            "Step 120/1584, Loss: 0.0218, LR: 0.005045\n",
            "Step 130/1584, Loss: 0.0141, LR: 0.005070\n",
            "Step 140/1584, Loss: 0.0098, LR: 0.005095\n",
            "Step 150/1584, Loss: 0.0261, LR: 0.005120\n",
            "Step 160/1584, Loss: 0.0111, LR: 0.005145\n",
            "Step 170/1584, Loss: 0.0156, LR: 0.005170\n",
            "Step 180/1584, Loss: 0.0123, LR: 0.005195\n",
            "Step 190/1584, Loss: 0.0206, LR: 0.005220\n",
            "Step 200/1584, Loss: 0.0170, LR: 0.005245\n",
            "Step 210/1584, Loss: 0.0162, LR: 0.005270\n",
            "Step 220/1584, Loss: 0.0126, LR: 0.005295\n",
            "Step 230/1584, Loss: 0.0129, LR: 0.005320\n",
            "Step 240/1584, Loss: 0.0622, LR: 0.005345\n",
            "Step 250/1584, Loss: 0.0224, LR: 0.005370\n",
            "Step 260/1584, Loss: 0.0384, LR: 0.005395\n",
            "Step 270/1584, Loss: 0.0616, LR: 0.005420\n",
            "Step 280/1584, Loss: 0.0653, LR: 0.005445\n",
            "Step 290/1584, Loss: 0.0367, LR: 0.005470\n",
            "Step 300/1584, Loss: 0.0189, LR: 0.005495\n",
            "Step 310/1584, Loss: 0.0152, LR: 0.005520\n",
            "Step 320/1584, Loss: 0.0226, LR: 0.005545\n",
            "Step 330/1584, Loss: 0.0272, LR: 0.005570\n",
            "Step 340/1584, Loss: 0.0303, LR: 0.005595\n",
            "Step 350/1584, Loss: 0.0159, LR: 0.005620\n",
            "Step 360/1584, Loss: 0.0223, LR: 0.005645\n",
            "Step 370/1584, Loss: 0.0140, LR: 0.005670\n",
            "Step 380/1584, Loss: 0.0221, LR: 0.005695\n",
            "Step 390/1584, Loss: 0.0178, LR: 0.005720\n",
            "Step 400/1584, Loss: 0.0661, LR: 0.005745\n",
            "Step 410/1584, Loss: 0.0221, LR: 0.005770\n",
            "Step 420/1584, Loss: 0.0210, LR: 0.005795\n",
            "Step 430/1584, Loss: 0.0312, LR: 0.005820\n",
            "Step 440/1584, Loss: 0.0559, LR: 0.005845\n",
            "Step 450/1584, Loss: 0.0235, LR: 0.005870\n",
            "Step 460/1584, Loss: 0.0216, LR: 0.005895\n",
            "Step 470/1584, Loss: 0.0143, LR: 0.005920\n",
            "Step 480/1584, Loss: 0.0099, LR: 0.005945\n",
            "Step 490/1584, Loss: 0.0177, LR: 0.005970\n",
            "Step 500/1584, Loss: 0.0101, LR: 0.005995\n",
            "Step 510/1584, Loss: 0.0072, LR: 0.005980\n",
            "Step 520/1584, Loss: 0.0164, LR: 0.005955\n",
            "Step 530/1584, Loss: 0.0121, LR: 0.005930\n",
            "Step 540/1584, Loss: 0.0186, LR: 0.005905\n",
            "Step 550/1584, Loss: 0.0114, LR: 0.005880\n",
            "Step 560/1584, Loss: 0.0254, LR: 0.005855\n",
            "Step 570/1584, Loss: 0.0162, LR: 0.005830\n",
            "Step 580/1584, Loss: 0.0161, LR: 0.005805\n",
            "Step 590/1584, Loss: 0.0189, LR: 0.005780\n",
            "Step 600/1584, Loss: 0.0267, LR: 0.005755\n",
            "Step 610/1584, Loss: 0.0716, LR: 0.005730\n",
            "Step 620/1584, Loss: 0.0522, LR: 0.005705\n",
            "Step 630/1584, Loss: 0.0358, LR: 0.005680\n",
            "Step 640/1584, Loss: 0.0314, LR: 0.005655\n",
            "Step 650/1584, Loss: 0.0237, LR: 0.005630\n",
            "Step 660/1584, Loss: 0.0208, LR: 0.005605\n",
            "Step 670/1584, Loss: 0.0469, LR: 0.005580\n",
            "Step 680/1584, Loss: 0.0258, LR: 0.005555\n",
            "Step 690/1584, Loss: 0.0282, LR: 0.005530\n",
            "Step 700/1584, Loss: 0.0237, LR: 0.005505\n",
            "Step 710/1584, Loss: 0.0149, LR: 0.005480\n",
            "Step 720/1584, Loss: 0.0345, LR: 0.005455\n",
            "Step 730/1584, Loss: 0.0318, LR: 0.005430\n",
            "Step 740/1584, Loss: 0.0317, LR: 0.005405\n",
            "Step 750/1584, Loss: 0.0588, LR: 0.005380\n",
            "Step 760/1584, Loss: 0.0519, LR: 0.005355\n",
            "Step 770/1584, Loss: 0.0202, LR: 0.005330\n",
            "Step 780/1584, Loss: 0.0317, LR: 0.005305\n",
            "Step 790/1584, Loss: 0.0186, LR: 0.005280\n",
            "Step 800/1584, Loss: 0.0348, LR: 0.005255\n",
            "Step 810/1584, Loss: 0.0433, LR: 0.005230\n",
            "Step 820/1584, Loss: 0.0236, LR: 0.005205\n",
            "Step 830/1584, Loss: 0.0138, LR: 0.005180\n",
            "Step 840/1584, Loss: 0.0126, LR: 0.005155\n",
            "Step 850/1584, Loss: 0.0411, LR: 0.005130\n",
            "Step 860/1584, Loss: 0.0165, LR: 0.005105\n",
            "Step 870/1584, Loss: 0.0130, LR: 0.005080\n",
            "Step 880/1584, Loss: 0.0336, LR: 0.005055\n",
            "Step 890/1584, Loss: 0.0359, LR: 0.005030\n",
            "Step 900/1584, Loss: 0.0335, LR: 0.005005\n",
            "Step 910/1584, Loss: 0.0142, LR: 0.004980\n",
            "Step 920/1584, Loss: 0.0165, LR: 0.004955\n",
            "Step 930/1584, Loss: 0.0776, LR: 0.004930\n",
            "Step 940/1584, Loss: 0.0565, LR: 0.004905\n",
            "Step 950/1584, Loss: 0.0193, LR: 0.004880\n",
            "Step 960/1584, Loss: 0.0204, LR: 0.004855\n",
            "Step 970/1584, Loss: 0.0156, LR: 0.004830\n",
            "Step 980/1584, Loss: 0.0103, LR: 0.004805\n",
            "Step 990/1584, Loss: 0.0215, LR: 0.004780\n",
            "Step 1000/1584, Loss: 0.0159, LR: 0.004755\n",
            "Step 1010/1584, Loss: 0.0093, LR: 0.004730\n",
            "Step 1020/1584, Loss: 0.0197, LR: 0.004705\n",
            "Step 1030/1584, Loss: 0.0508, LR: 0.004680\n",
            "Step 1040/1584, Loss: 0.0354, LR: 0.004655\n",
            "Step 1050/1584, Loss: 0.0282, LR: 0.004630\n",
            "Step 1060/1584, Loss: 0.0241, LR: 0.004605\n",
            "Step 1070/1584, Loss: 0.0149, LR: 0.004580\n",
            "Step 1080/1584, Loss: 0.0072, LR: 0.004555\n",
            "Step 1090/1584, Loss: 0.0306, LR: 0.004530\n",
            "Step 1100/1584, Loss: 0.0087, LR: 0.004505\n",
            "Step 1110/1584, Loss: 0.0067, LR: 0.004480\n",
            "Step 1120/1584, Loss: 0.0287, LR: 0.004455\n",
            "Step 1130/1584, Loss: 0.0819, LR: 0.004430\n",
            "Step 1140/1584, Loss: 0.1017, LR: 0.004405\n",
            "Step 1150/1584, Loss: 0.0253, LR: 0.004380\n",
            "Step 1160/1584, Loss: 0.0259, LR: 0.004355\n",
            "Step 1170/1584, Loss: 0.0246, LR: 0.004330\n",
            "Step 1180/1584, Loss: 0.0434, LR: 0.004305\n",
            "Step 1190/1584, Loss: 0.0130, LR: 0.004280\n",
            "Step 1200/1584, Loss: 0.0145, LR: 0.004255\n",
            "Step 1210/1584, Loss: 0.0144, LR: 0.004230\n",
            "Step 1220/1584, Loss: 0.0215, LR: 0.004205\n",
            "Step 1230/1584, Loss: 0.0159, LR: 0.004180\n",
            "Step 1240/1584, Loss: 0.0077, LR: 0.004155\n",
            "Step 1250/1584, Loss: 0.0095, LR: 0.004130\n",
            "Step 1260/1584, Loss: 0.0223, LR: 0.004105\n",
            "Step 1270/1584, Loss: 0.0206, LR: 0.004080\n",
            "Step 1280/1584, Loss: 0.0165, LR: 0.004055\n",
            "Step 1290/1584, Loss: 0.0273, LR: 0.004030\n",
            "Step 1300/1584, Loss: 0.0139, LR: 0.004005\n",
            "Step 1310/1584, Loss: 0.0217, LR: 0.003980\n",
            "Step 1320/1584, Loss: 0.0215, LR: 0.003955\n",
            "Step 1330/1584, Loss: 0.0246, LR: 0.003930\n",
            "Step 1340/1584, Loss: 0.0186, LR: 0.003905\n",
            "Step 1350/1584, Loss: 0.0153, LR: 0.003880\n",
            "Step 1360/1584, Loss: 0.0131, LR: 0.003855\n",
            "Step 1370/1584, Loss: 0.0138, LR: 0.003830\n",
            "Step 1380/1584, Loss: 0.0577, LR: 0.003805\n",
            "Step 1390/1584, Loss: 0.0159, LR: 0.003780\n",
            "Step 1400/1584, Loss: 0.0138, LR: 0.003755\n",
            "Step 1410/1584, Loss: 0.0160, LR: 0.003730\n",
            "Step 1420/1584, Loss: 0.0106, LR: 0.003705\n",
            "Step 1430/1584, Loss: 0.0442, LR: 0.003680\n",
            "Step 1440/1584, Loss: 0.0129, LR: 0.003655\n",
            "Step 1450/1584, Loss: 0.0140, LR: 0.003630\n",
            "Step 1460/1584, Loss: 0.0077, LR: 0.003605\n",
            "Step 1470/1584, Loss: 0.0114, LR: 0.003580\n",
            "Step 1480/1584, Loss: 0.0146, LR: 0.003555\n",
            "Step 1490/1584, Loss: 0.0087, LR: 0.003530\n",
            "Step 1500/1584, Loss: 0.0092, LR: 0.003505\n",
            "Step 1510/1584, Loss: 0.0131, LR: 0.003480\n",
            "Step 1520/1584, Loss: 0.0050, LR: 0.003455\n",
            "Step 1530/1584, Loss: 0.0155, LR: 0.003430\n",
            "Step 1540/1584, Loss: 0.0188, LR: 0.003405\n",
            "Step 1550/1584, Loss: 0.0173, LR: 0.003380\n",
            "Step 1560/1584, Loss: 0.0279, LR: 0.003355\n",
            "Step 1570/1584, Loss: 0.0197, LR: 0.003330\n",
            "Step 1580/1584, Loss: 0.0207, LR: 0.003305\n",
            "\n",
            "Epoch 7 Results:\n",
            "Training Loss: 0.0240\n",
            "Validation Loss: 0.0789\n",
            "Learning Rate: 0.004878\n",
            "\n",
            "Epoch 8/50\n",
            "Step 0/1584, Loss: 0.0214, LR: 0.003295\n",
            "Step 10/1584, Loss: 0.0253, LR: 0.003272\n",
            "Step 20/1584, Loss: 0.0136, LR: 0.003247\n",
            "Step 30/1584, Loss: 0.0160, LR: 0.003222\n",
            "Step 40/1584, Loss: 0.0112, LR: 0.003197\n",
            "Step 50/1584, Loss: 0.0169, LR: 0.003172\n",
            "Step 60/1584, Loss: 0.0124, LR: 0.003147\n",
            "Step 70/1584, Loss: 0.0221, LR: 0.003122\n",
            "Step 80/1584, Loss: 0.0130, LR: 0.003097\n",
            "Step 90/1584, Loss: 0.0150, LR: 0.003072\n",
            "Step 100/1584, Loss: 0.0055, LR: 0.003047\n",
            "Step 110/1584, Loss: 0.0121, LR: 0.003022\n",
            "Step 120/1584, Loss: 0.0228, LR: 0.002997\n",
            "Step 130/1584, Loss: 0.0155, LR: 0.002972\n",
            "Step 140/1584, Loss: 0.0086, LR: 0.002947\n",
            "Step 150/1584, Loss: 0.0245, LR: 0.002922\n",
            "Step 160/1584, Loss: 0.0116, LR: 0.002897\n",
            "Step 170/1584, Loss: 0.0136, LR: 0.002872\n",
            "Step 180/1584, Loss: 0.0125, LR: 0.002847\n",
            "Step 190/1584, Loss: 0.0178, LR: 0.002822\n",
            "Step 200/1584, Loss: 0.0195, LR: 0.002797\n",
            "Step 210/1584, Loss: 0.0157, LR: 0.002772\n",
            "Step 220/1584, Loss: 0.0133, LR: 0.002747\n",
            "Step 230/1584, Loss: 0.0093, LR: 0.002722\n",
            "Step 240/1584, Loss: 0.0656, LR: 0.002697\n",
            "Step 250/1584, Loss: 0.0180, LR: 0.002672\n",
            "Step 260/1584, Loss: 0.0391, LR: 0.002647\n",
            "Step 270/1584, Loss: 0.0585, LR: 0.002622\n",
            "Step 280/1584, Loss: 0.0672, LR: 0.002597\n",
            "Step 290/1584, Loss: 0.0397, LR: 0.002572\n",
            "Step 300/1584, Loss: 0.0182, LR: 0.002547\n",
            "Step 310/1584, Loss: 0.0171, LR: 0.002522\n",
            "Step 320/1584, Loss: 0.0227, LR: 0.002497\n",
            "Step 330/1584, Loss: 0.0244, LR: 0.002472\n",
            "Step 340/1584, Loss: 0.0304, LR: 0.002447\n",
            "Step 350/1584, Loss: 0.0166, LR: 0.002422\n",
            "Step 360/1584, Loss: 0.0221, LR: 0.002397\n",
            "Step 370/1584, Loss: 0.0133, LR: 0.002372\n",
            "Step 380/1584, Loss: 0.0211, LR: 0.002347\n",
            "Step 390/1584, Loss: 0.0168, LR: 0.002322\n",
            "Step 400/1584, Loss: 0.0624, LR: 0.002297\n",
            "Step 410/1584, Loss: 0.0238, LR: 0.002272\n",
            "Step 420/1584, Loss: 0.0165, LR: 0.002247\n",
            "Step 430/1584, Loss: 0.0221, LR: 0.002222\n",
            "Step 440/1584, Loss: 0.0592, LR: 0.002197\n",
            "Step 450/1584, Loss: 0.0240, LR: 0.002172\n",
            "Step 460/1584, Loss: 0.0162, LR: 0.002147\n",
            "Step 470/1584, Loss: 0.0133, LR: 0.002122\n",
            "Step 480/1584, Loss: 0.0107, LR: 0.002097\n",
            "Step 490/1584, Loss: 0.0170, LR: 0.002072\n",
            "Step 500/1584, Loss: 0.0112, LR: 0.002047\n",
            "Step 510/1584, Loss: 0.0069, LR: 0.002022\n",
            "Step 520/1584, Loss: 0.0155, LR: 0.001997\n",
            "Step 530/1584, Loss: 0.0126, LR: 0.001972\n",
            "Step 540/1584, Loss: 0.0161, LR: 0.001947\n",
            "Step 550/1584, Loss: 0.0129, LR: 0.001922\n",
            "Step 560/1584, Loss: 0.0244, LR: 0.001897\n",
            "Step 570/1584, Loss: 0.0181, LR: 0.001872\n",
            "Step 580/1584, Loss: 0.0169, LR: 0.001847\n",
            "Step 590/1584, Loss: 0.0199, LR: 0.001822\n",
            "Step 600/1584, Loss: 0.0270, LR: 0.001797\n",
            "Step 610/1584, Loss: 0.0791, LR: 0.001772\n",
            "Step 620/1584, Loss: 0.0482, LR: 0.001747\n",
            "Step 630/1584, Loss: 0.0328, LR: 0.001722\n",
            "Step 640/1584, Loss: 0.0260, LR: 0.001697\n",
            "Step 650/1584, Loss: 0.0292, LR: 0.001672\n",
            "Step 660/1584, Loss: 0.0191, LR: 0.001647\n",
            "Step 670/1584, Loss: 0.0430, LR: 0.001622\n",
            "Step 680/1584, Loss: 0.0270, LR: 0.001597\n",
            "Step 690/1584, Loss: 0.0283, LR: 0.001572\n",
            "Step 700/1584, Loss: 0.0267, LR: 0.001547\n",
            "Step 710/1584, Loss: 0.0149, LR: 0.001522\n",
            "Step 720/1584, Loss: 0.0306, LR: 0.001497\n",
            "Step 730/1584, Loss: 0.0325, LR: 0.001472\n",
            "Step 740/1584, Loss: 0.0270, LR: 0.001447\n",
            "Step 750/1584, Loss: 0.0597, LR: 0.001422\n",
            "Step 760/1584, Loss: 0.0607, LR: 0.001397\n",
            "Step 770/1584, Loss: 0.0155, LR: 0.001372\n",
            "Step 780/1584, Loss: 0.0241, LR: 0.001347\n",
            "Step 790/1584, Loss: 0.0209, LR: 0.001322\n",
            "Step 800/1584, Loss: 0.0238, LR: 0.001297\n",
            "Step 810/1584, Loss: 0.0398, LR: 0.001272\n",
            "Step 820/1584, Loss: 0.0233, LR: 0.001247\n",
            "Step 830/1584, Loss: 0.0142, LR: 0.001222\n",
            "Step 840/1584, Loss: 0.0135, LR: 0.001197\n",
            "Step 850/1584, Loss: 0.0362, LR: 0.001172\n",
            "Step 860/1584, Loss: 0.0225, LR: 0.001147\n",
            "Step 870/1584, Loss: 0.0139, LR: 0.001122\n",
            "Step 880/1584, Loss: 0.0311, LR: 0.001097\n",
            "Step 890/1584, Loss: 0.0329, LR: 0.001072\n",
            "Step 900/1584, Loss: 0.0345, LR: 0.001047\n",
            "Step 910/1584, Loss: 0.0129, LR: 0.001022\n",
            "Step 920/1584, Loss: 0.0146, LR: 0.001003\n",
            "Step 930/1584, Loss: 0.0716, LR: 0.001028\n",
            "Step 940/1584, Loss: 0.0587, LR: 0.001053\n",
            "Step 950/1584, Loss: 0.0202, LR: 0.001078\n",
            "Step 960/1584, Loss: 0.0182, LR: 0.001103\n",
            "Step 970/1584, Loss: 0.0179, LR: 0.001128\n",
            "Step 980/1584, Loss: 0.0093, LR: 0.001153\n",
            "Step 990/1584, Loss: 0.0214, LR: 0.001178\n",
            "Step 1000/1584, Loss: 0.0165, LR: 0.001203\n",
            "Step 1010/1584, Loss: 0.0105, LR: 0.001228\n",
            "Step 1020/1584, Loss: 0.0163, LR: 0.001253\n",
            "Step 1030/1584, Loss: 0.0453, LR: 0.001278\n",
            "Step 1040/1584, Loss: 0.0366, LR: 0.001303\n",
            "Step 1050/1584, Loss: 0.0231, LR: 0.001328\n",
            "Step 1060/1584, Loss: 0.0227, LR: 0.001353\n",
            "Step 1070/1584, Loss: 0.0165, LR: 0.001378\n",
            "Step 1080/1584, Loss: 0.0086, LR: 0.001403\n",
            "Step 1090/1584, Loss: 0.0270, LR: 0.001428\n",
            "Step 1100/1584, Loss: 0.0104, LR: 0.001453\n",
            "Step 1110/1584, Loss: 0.0084, LR: 0.001478\n",
            "Step 1120/1584, Loss: 0.0242, LR: 0.001503\n",
            "Step 1130/1584, Loss: 0.0735, LR: 0.001528\n",
            "Step 1140/1584, Loss: 0.1070, LR: 0.001553\n",
            "Step 1150/1584, Loss: 0.0330, LR: 0.001578\n",
            "Step 1160/1584, Loss: 0.0178, LR: 0.001603\n",
            "Step 1170/1584, Loss: 0.0199, LR: 0.001628\n",
            "Step 1180/1584, Loss: 0.0408, LR: 0.001653\n",
            "Step 1190/1584, Loss: 0.0191, LR: 0.001678\n",
            "Step 1200/1584, Loss: 0.0133, LR: 0.001703\n",
            "Step 1210/1584, Loss: 0.0130, LR: 0.001728\n",
            "Step 1220/1584, Loss: 0.0201, LR: 0.001753\n",
            "Step 1230/1584, Loss: 0.0172, LR: 0.001778\n",
            "Step 1240/1584, Loss: 0.0065, LR: 0.001803\n",
            "Step 1250/1584, Loss: 0.0089, LR: 0.001828\n",
            "Step 1260/1584, Loss: 0.0209, LR: 0.001853\n",
            "Step 1270/1584, Loss: 0.0209, LR: 0.001878\n",
            "Step 1280/1584, Loss: 0.0163, LR: 0.001903\n",
            "Step 1290/1584, Loss: 0.0272, LR: 0.001928\n",
            "Step 1300/1584, Loss: 0.0131, LR: 0.001953\n",
            "Step 1310/1584, Loss: 0.0202, LR: 0.001978\n",
            "Step 1320/1584, Loss: 0.0226, LR: 0.002003\n",
            "Step 1330/1584, Loss: 0.0245, LR: 0.002028\n",
            "Step 1340/1584, Loss: 0.0178, LR: 0.002053\n",
            "Step 1350/1584, Loss: 0.0182, LR: 0.002078\n",
            "Step 1360/1584, Loss: 0.0126, LR: 0.002103\n",
            "Step 1370/1584, Loss: 0.0131, LR: 0.002128\n",
            "Step 1380/1584, Loss: 0.0537, LR: 0.002153\n",
            "Step 1390/1584, Loss: 0.0158, LR: 0.002178\n",
            "Step 1400/1584, Loss: 0.0148, LR: 0.002203\n",
            "Step 1410/1584, Loss: 0.0140, LR: 0.002228\n",
            "Step 1420/1584, Loss: 0.0127, LR: 0.002253\n",
            "Step 1430/1584, Loss: 0.0438, LR: 0.002278\n",
            "Step 1440/1584, Loss: 0.0117, LR: 0.002303\n",
            "Step 1450/1584, Loss: 0.0152, LR: 0.002328\n",
            "Step 1460/1584, Loss: 0.0089, LR: 0.002353\n",
            "Step 1470/1584, Loss: 0.0101, LR: 0.002378\n",
            "Step 1480/1584, Loss: 0.0142, LR: 0.002403\n",
            "Step 1490/1584, Loss: 0.0087, LR: 0.002428\n",
            "Step 1500/1584, Loss: 0.0088, LR: 0.002453\n",
            "Step 1510/1584, Loss: 0.0126, LR: 0.002478\n",
            "Step 1520/1584, Loss: 0.0059, LR: 0.002503\n",
            "Step 1530/1584, Loss: 0.0138, LR: 0.002528\n",
            "Step 1540/1584, Loss: 0.0201, LR: 0.002553\n",
            "Step 1550/1584, Loss: 0.0170, LR: 0.002578\n",
            "Step 1560/1584, Loss: 0.0274, LR: 0.002603\n",
            "Step 1570/1584, Loss: 0.0216, LR: 0.002628\n",
            "Step 1580/1584, Loss: 0.0199, LR: 0.002653\n",
            "\n",
            "Epoch 8 Results:\n",
            "Training Loss: 0.0235\n",
            "Validation Loss: 0.0757\n",
            "Learning Rate: 0.002015\n",
            "\n",
            "Epoch 9/50\n",
            "Step 0/1584, Loss: 0.0224, LR: 0.002663\n",
            "Step 10/1584, Loss: 0.0252, LR: 0.002685\n",
            "Step 20/1584, Loss: 0.0140, LR: 0.002710\n",
            "Step 30/1584, Loss: 0.0169, LR: 0.002735\n",
            "Step 40/1584, Loss: 0.0115, LR: 0.002760\n",
            "Step 50/1584, Loss: 0.0165, LR: 0.002785\n",
            "Step 60/1584, Loss: 0.0119, LR: 0.002810\n",
            "Step 70/1584, Loss: 0.0232, LR: 0.002835\n",
            "Step 80/1584, Loss: 0.0123, LR: 0.002860\n",
            "Step 90/1584, Loss: 0.0155, LR: 0.002885\n",
            "Step 100/1584, Loss: 0.0057, LR: 0.002910\n",
            "Step 110/1584, Loss: 0.0107, LR: 0.002935\n",
            "Step 120/1584, Loss: 0.0232, LR: 0.002960\n",
            "Step 130/1584, Loss: 0.0161, LR: 0.002985\n",
            "Step 140/1584, Loss: 0.0083, LR: 0.003010\n",
            "Step 150/1584, Loss: 0.0236, LR: 0.003035\n",
            "Step 160/1584, Loss: 0.0131, LR: 0.003060\n",
            "Step 170/1584, Loss: 0.0131, LR: 0.003085\n",
            "Step 180/1584, Loss: 0.0128, LR: 0.003110\n",
            "Step 190/1584, Loss: 0.0165, LR: 0.003135\n",
            "Step 200/1584, Loss: 0.0196, LR: 0.003160\n",
            "Step 210/1584, Loss: 0.0162, LR: 0.003185\n",
            "Step 220/1584, Loss: 0.0141, LR: 0.003210\n",
            "Step 230/1584, Loss: 0.0099, LR: 0.003235\n",
            "Step 240/1584, Loss: 0.0593, LR: 0.003260\n",
            "Step 250/1584, Loss: 0.0209, LR: 0.003285\n",
            "Step 260/1584, Loss: 0.0400, LR: 0.003310\n",
            "Step 270/1584, Loss: 0.0555, LR: 0.003335\n",
            "Step 280/1584, Loss: 0.0666, LR: 0.003360\n",
            "Step 290/1584, Loss: 0.0441, LR: 0.003385\n",
            "Step 300/1584, Loss: 0.0190, LR: 0.003410\n",
            "Step 310/1584, Loss: 0.0172, LR: 0.003435\n",
            "Step 320/1584, Loss: 0.0208, LR: 0.003460\n",
            "Step 330/1584, Loss: 0.0224, LR: 0.003485\n",
            "Step 340/1584, Loss: 0.0346, LR: 0.003510\n",
            "Step 350/1584, Loss: 0.0170, LR: 0.003535\n",
            "Step 360/1584, Loss: 0.0222, LR: 0.003560\n",
            "Step 370/1584, Loss: 0.0130, LR: 0.003585\n",
            "Step 380/1584, Loss: 0.0197, LR: 0.003610\n",
            "Step 390/1584, Loss: 0.0179, LR: 0.003635\n",
            "Step 400/1584, Loss: 0.0614, LR: 0.003660\n",
            "Step 410/1584, Loss: 0.0268, LR: 0.003685\n",
            "Step 420/1584, Loss: 0.0186, LR: 0.003710\n",
            "Step 430/1584, Loss: 0.0145, LR: 0.003735\n",
            "Step 440/1584, Loss: 0.0695, LR: 0.003760\n",
            "Step 450/1584, Loss: 0.0211, LR: 0.003785\n",
            "Step 460/1584, Loss: 0.0199, LR: 0.003810\n",
            "Step 470/1584, Loss: 0.0146, LR: 0.003835\n",
            "Step 480/1584, Loss: 0.0109, LR: 0.003860\n",
            "Step 490/1584, Loss: 0.0177, LR: 0.003885\n",
            "Step 500/1584, Loss: 0.0102, LR: 0.003910\n",
            "Step 510/1584, Loss: 0.0072, LR: 0.003935\n",
            "Step 520/1584, Loss: 0.0156, LR: 0.003960\n",
            "Step 530/1584, Loss: 0.0124, LR: 0.003985\n",
            "Step 540/1584, Loss: 0.0146, LR: 0.004010\n",
            "Step 550/1584, Loss: 0.0146, LR: 0.004035\n",
            "Step 560/1584, Loss: 0.0229, LR: 0.004060\n",
            "Step 570/1584, Loss: 0.0187, LR: 0.004085\n",
            "Step 580/1584, Loss: 0.0161, LR: 0.004110\n",
            "Step 590/1584, Loss: 0.0187, LR: 0.004135\n",
            "Step 600/1584, Loss: 0.0239, LR: 0.004160\n",
            "Step 610/1584, Loss: 0.0661, LR: 0.004185\n",
            "Step 620/1584, Loss: 0.0552, LR: 0.004210\n",
            "Step 630/1584, Loss: 0.0372, LR: 0.004235\n",
            "Step 640/1584, Loss: 0.0273, LR: 0.004260\n",
            "Step 650/1584, Loss: 0.0314, LR: 0.004285\n",
            "Step 660/1584, Loss: 0.0141, LR: 0.004310\n",
            "Step 670/1584, Loss: 0.0477, LR: 0.004335\n",
            "Step 680/1584, Loss: 0.0274, LR: 0.004360\n",
            "Step 690/1584, Loss: 0.0284, LR: 0.004385\n",
            "Step 700/1584, Loss: 0.0265, LR: 0.004410\n",
            "Step 710/1584, Loss: 0.0134, LR: 0.004435\n",
            "Step 720/1584, Loss: 0.0336, LR: 0.004460\n",
            "Step 730/1584, Loss: 0.0337, LR: 0.004485\n",
            "Step 740/1584, Loss: 0.0228, LR: 0.004510\n",
            "Step 750/1584, Loss: 0.0586, LR: 0.004535\n",
            "Step 760/1584, Loss: 0.0561, LR: 0.004560\n",
            "Step 770/1584, Loss: 0.0233, LR: 0.004585\n",
            "Step 780/1584, Loss: 0.0312, LR: 0.004610\n",
            "Step 790/1584, Loss: 0.0209, LR: 0.004635\n",
            "Step 800/1584, Loss: 0.0255, LR: 0.004660\n",
            "Step 810/1584, Loss: 0.0450, LR: 0.004685\n",
            "Step 820/1584, Loss: 0.0265, LR: 0.004710\n",
            "Step 830/1584, Loss: 0.0159, LR: 0.004735\n",
            "Step 840/1584, Loss: 0.0124, LR: 0.004760\n",
            "Step 850/1584, Loss: 0.0341, LR: 0.004785\n",
            "Step 860/1584, Loss: 0.0238, LR: 0.004810\n",
            "Step 870/1584, Loss: 0.0098, LR: 0.004835\n",
            "Step 880/1584, Loss: 0.0322, LR: 0.004860\n",
            "Step 890/1584, Loss: 0.0336, LR: 0.004885\n",
            "Step 900/1584, Loss: 0.0399, LR: 0.004910\n",
            "Step 910/1584, Loss: 0.0152, LR: 0.004935\n",
            "Step 920/1584, Loss: 0.0134, LR: 0.004960\n",
            "Step 930/1584, Loss: 0.0656, LR: 0.004985\n",
            "Step 940/1584, Loss: 0.0661, LR: 0.005010\n",
            "Step 950/1584, Loss: 0.0237, LR: 0.005035\n",
            "Step 960/1584, Loss: 0.0158, LR: 0.005060\n",
            "Step 970/1584, Loss: 0.0204, LR: 0.005085\n",
            "Step 980/1584, Loss: 0.0090, LR: 0.005110\n",
            "Step 990/1584, Loss: 0.0215, LR: 0.005135\n",
            "Step 1000/1584, Loss: 0.0170, LR: 0.005160\n",
            "Step 1010/1584, Loss: 0.0103, LR: 0.005185\n",
            "Step 1020/1584, Loss: 0.0114, LR: 0.005210\n",
            "Step 1030/1584, Loss: 0.0517, LR: 0.005235\n",
            "Step 1040/1584, Loss: 0.0370, LR: 0.005260\n",
            "Step 1050/1584, Loss: 0.0328, LR: 0.005285\n",
            "Step 1060/1584, Loss: 0.0247, LR: 0.005310\n",
            "Step 1070/1584, Loss: 0.0157, LR: 0.005335\n",
            "Step 1080/1584, Loss: 0.0083, LR: 0.005360\n",
            "Step 1090/1584, Loss: 0.0228, LR: 0.005385\n",
            "Step 1100/1584, Loss: 0.0176, LR: 0.005410\n",
            "Step 1110/1584, Loss: 0.0075, LR: 0.005435\n",
            "Step 1120/1584, Loss: 0.0205, LR: 0.005460\n",
            "Step 1130/1584, Loss: 0.0687, LR: 0.005485\n",
            "Step 1140/1584, Loss: 0.1015, LR: 0.005510\n",
            "Step 1150/1584, Loss: 0.0415, LR: 0.005535\n",
            "Step 1160/1584, Loss: 0.0302, LR: 0.005560\n",
            "Step 1170/1584, Loss: 0.0205, LR: 0.005585\n",
            "Step 1180/1584, Loss: 0.0413, LR: 0.005610\n",
            "Step 1190/1584, Loss: 0.0188, LR: 0.005635\n",
            "Step 1200/1584, Loss: 0.0127, LR: 0.005660\n",
            "Step 1210/1584, Loss: 0.0167, LR: 0.005685\n",
            "Step 1220/1584, Loss: 0.0194, LR: 0.005710\n",
            "Step 1230/1584, Loss: 0.0178, LR: 0.005735\n",
            "Step 1240/1584, Loss: 0.0088, LR: 0.005760\n",
            "Step 1250/1584, Loss: 0.0093, LR: 0.005785\n",
            "Step 1260/1584, Loss: 0.0211, LR: 0.005810\n",
            "Step 1270/1584, Loss: 0.0221, LR: 0.005835\n",
            "Step 1280/1584, Loss: 0.0123, LR: 0.005860\n",
            "Step 1290/1584, Loss: 0.0328, LR: 0.005885\n",
            "Step 1300/1584, Loss: 0.0157, LR: 0.005910\n",
            "Step 1310/1584, Loss: 0.0187, LR: 0.005935\n",
            "Step 1320/1584, Loss: 0.0231, LR: 0.005960\n",
            "Step 1330/1584, Loss: 0.0252, LR: 0.005985\n",
            "Step 1340/1584, Loss: 0.0164, LR: 0.005990\n",
            "Step 1350/1584, Loss: 0.0153, LR: 0.005965\n",
            "Step 1360/1584, Loss: 0.0152, LR: 0.005940\n",
            "Step 1370/1584, Loss: 0.0144, LR: 0.005915\n",
            "Step 1380/1584, Loss: 0.0500, LR: 0.005890\n",
            "Step 1390/1584, Loss: 0.0225, LR: 0.005865\n",
            "Step 1400/1584, Loss: 0.0185, LR: 0.005840\n",
            "Step 1410/1584, Loss: 0.0121, LR: 0.005815\n",
            "Step 1420/1584, Loss: 0.0151, LR: 0.005790\n",
            "Step 1430/1584, Loss: 0.0439, LR: 0.005765\n",
            "Step 1440/1584, Loss: 0.0116, LR: 0.005740\n",
            "Step 1450/1584, Loss: 0.0136, LR: 0.005715\n",
            "Step 1460/1584, Loss: 0.0080, LR: 0.005690\n",
            "Step 1470/1584, Loss: 0.0126, LR: 0.005665\n",
            "Step 1480/1584, Loss: 0.0153, LR: 0.005640\n",
            "Step 1490/1584, Loss: 0.0087, LR: 0.005615\n",
            "Step 1500/1584, Loss: 0.0092, LR: 0.005590\n",
            "Step 1510/1584, Loss: 0.0127, LR: 0.005565\n",
            "Step 1520/1584, Loss: 0.0066, LR: 0.005540\n",
            "Step 1530/1584, Loss: 0.0123, LR: 0.005515\n",
            "Step 1540/1584, Loss: 0.0201, LR: 0.005490\n",
            "Step 1550/1584, Loss: 0.0156, LR: 0.005465\n",
            "Step 1560/1584, Loss: 0.0245, LR: 0.005440\n",
            "Step 1570/1584, Loss: 0.0229, LR: 0.005415\n",
            "Step 1580/1584, Loss: 0.0186, LR: 0.005390\n",
            "\n",
            "Epoch 9 Results:\n",
            "Training Loss: 0.0240\n",
            "Validation Loss: 0.0838\n",
            "Learning Rate: 0.004543\n",
            "\n",
            "Epoch 10/50\n",
            "Step 0/1584, Loss: 0.0221, LR: 0.005380\n",
            "Step 10/1584, Loss: 0.0242, LR: 0.005357\n",
            "Step 20/1584, Loss: 0.0164, LR: 0.005332\n",
            "Step 30/1584, Loss: 0.0167, LR: 0.005307\n",
            "Step 40/1584, Loss: 0.0112, LR: 0.005282\n",
            "Step 50/1584, Loss: 0.0181, LR: 0.005257\n",
            "Step 60/1584, Loss: 0.0096, LR: 0.005232\n",
            "Step 70/1584, Loss: 0.0236, LR: 0.005207\n",
            "Step 80/1584, Loss: 0.0120, LR: 0.005182\n",
            "Step 90/1584, Loss: 0.0166, LR: 0.005157\n",
            "Step 100/1584, Loss: 0.0060, LR: 0.005132\n",
            "Step 110/1584, Loss: 0.0093, LR: 0.005107\n",
            "Step 120/1584, Loss: 0.0235, LR: 0.005082\n",
            "Step 130/1584, Loss: 0.0163, LR: 0.005057\n",
            "Step 140/1584, Loss: 0.0084, LR: 0.005032\n",
            "Step 150/1584, Loss: 0.0224, LR: 0.005007\n",
            "Step 160/1584, Loss: 0.0159, LR: 0.004982\n",
            "Step 170/1584, Loss: 0.0141, LR: 0.004957\n",
            "Step 180/1584, Loss: 0.0132, LR: 0.004932\n",
            "Step 190/1584, Loss: 0.0164, LR: 0.004907\n",
            "Step 200/1584, Loss: 0.0194, LR: 0.004882\n",
            "Step 210/1584, Loss: 0.0157, LR: 0.004857\n",
            "Step 220/1584, Loss: 0.0153, LR: 0.004832\n",
            "Step 230/1584, Loss: 0.0103, LR: 0.004807\n",
            "Step 240/1584, Loss: 0.0538, LR: 0.004782\n",
            "Step 250/1584, Loss: 0.0245, LR: 0.004757\n",
            "Step 260/1584, Loss: 0.0390, LR: 0.004732\n",
            "Step 270/1584, Loss: 0.0531, LR: 0.004707\n",
            "Step 280/1584, Loss: 0.0649, LR: 0.004682\n",
            "Step 290/1584, Loss: 0.0497, LR: 0.004657\n",
            "Step 300/1584, Loss: 0.0198, LR: 0.004632\n",
            "Step 310/1584, Loss: 0.0170, LR: 0.004607\n",
            "Step 320/1584, Loss: 0.0191, LR: 0.004582\n",
            "Step 330/1584, Loss: 0.0231, LR: 0.004557\n",
            "Step 340/1584, Loss: 0.0346, LR: 0.004532\n",
            "Step 350/1584, Loss: 0.0185, LR: 0.004507\n",
            "Step 360/1584, Loss: 0.0227, LR: 0.004482\n",
            "Step 370/1584, Loss: 0.0124, LR: 0.004457\n",
            "Step 380/1584, Loss: 0.0194, LR: 0.004432\n",
            "Step 390/1584, Loss: 0.0189, LR: 0.004407\n",
            "Step 400/1584, Loss: 0.0573, LR: 0.004382\n",
            "Step 410/1584, Loss: 0.0317, LR: 0.004357\n",
            "Step 420/1584, Loss: 0.0192, LR: 0.004332\n",
            "Step 430/1584, Loss: 0.0125, LR: 0.004307\n",
            "Step 440/1584, Loss: 0.0731, LR: 0.004282\n",
            "Step 450/1584, Loss: 0.0194, LR: 0.004257\n",
            "Step 460/1584, Loss: 0.0211, LR: 0.004232\n",
            "Step 470/1584, Loss: 0.0154, LR: 0.004207\n",
            "Step 480/1584, Loss: 0.0113, LR: 0.004182\n",
            "Step 490/1584, Loss: 0.0181, LR: 0.004157\n",
            "Step 500/1584, Loss: 0.0096, LR: 0.004132\n",
            "Step 510/1584, Loss: 0.0078, LR: 0.004107\n",
            "Step 520/1584, Loss: 0.0151, LR: 0.004082\n",
            "Step 530/1584, Loss: 0.0126, LR: 0.004057\n",
            "Step 540/1584, Loss: 0.0124, LR: 0.004032\n",
            "Step 550/1584, Loss: 0.0164, LR: 0.004007\n",
            "Step 560/1584, Loss: 0.0219, LR: 0.003982\n",
            "Step 570/1584, Loss: 0.0193, LR: 0.003957\n",
            "Step 580/1584, Loss: 0.0161, LR: 0.003932\n",
            "Step 590/1584, Loss: 0.0187, LR: 0.003907\n",
            "Step 600/1584, Loss: 0.0228, LR: 0.003882\n",
            "Step 610/1584, Loss: 0.0632, LR: 0.003857\n",
            "Step 620/1584, Loss: 0.0574, LR: 0.003832\n",
            "Step 630/1584, Loss: 0.0376, LR: 0.003807\n",
            "Step 640/1584, Loss: 0.0278, LR: 0.003782\n",
            "Step 650/1584, Loss: 0.0324, LR: 0.003757\n",
            "Step 660/1584, Loss: 0.0141, LR: 0.003732\n",
            "Step 670/1584, Loss: 0.0440, LR: 0.003707\n",
            "Step 680/1584, Loss: 0.0290, LR: 0.003682\n",
            "Step 690/1584, Loss: 0.0282, LR: 0.003657\n",
            "Step 700/1584, Loss: 0.0267, LR: 0.003632\n",
            "Step 710/1584, Loss: 0.0146, LR: 0.003607\n",
            "Step 720/1584, Loss: 0.0325, LR: 0.003582\n",
            "Step 730/1584, Loss: 0.0347, LR: 0.003557\n",
            "Step 740/1584, Loss: 0.0177, LR: 0.003532\n",
            "Step 750/1584, Loss: 0.0590, LR: 0.003507\n",
            "Step 760/1584, Loss: 0.0583, LR: 0.003482\n",
            "Step 770/1584, Loss: 0.0259, LR: 0.003457\n",
            "Step 780/1584, Loss: 0.0283, LR: 0.003432\n",
            "Step 790/1584, Loss: 0.0233, LR: 0.003407\n",
            "Step 800/1584, Loss: 0.0200, LR: 0.003382\n",
            "Step 810/1584, Loss: 0.0432, LR: 0.003357\n",
            "Step 820/1584, Loss: 0.0280, LR: 0.003332\n",
            "Step 830/1584, Loss: 0.0159, LR: 0.003307\n",
            "Step 840/1584, Loss: 0.0126, LR: 0.003282\n",
            "Step 850/1584, Loss: 0.0305, LR: 0.003257\n",
            "Step 860/1584, Loss: 0.0270, LR: 0.003232\n",
            "Step 870/1584, Loss: 0.0100, LR: 0.003207\n",
            "Step 880/1584, Loss: 0.0304, LR: 0.003182\n",
            "Step 890/1584, Loss: 0.0318, LR: 0.003157\n",
            "Step 900/1584, Loss: 0.0414, LR: 0.003132\n",
            "Step 910/1584, Loss: 0.0151, LR: 0.003107\n",
            "Step 920/1584, Loss: 0.0118, LR: 0.003082\n",
            "Step 930/1584, Loss: 0.0605, LR: 0.003057\n",
            "Step 940/1584, Loss: 0.0679, LR: 0.003032\n",
            "Step 950/1584, Loss: 0.0259, LR: 0.003007\n",
            "Step 960/1584, Loss: 0.0142, LR: 0.002982\n",
            "Step 970/1584, Loss: 0.0218, LR: 0.002957\n",
            "Step 980/1584, Loss: 0.0096, LR: 0.002932\n",
            "Step 990/1584, Loss: 0.0203, LR: 0.002907\n",
            "Step 1000/1584, Loss: 0.0175, LR: 0.002882\n",
            "Step 1010/1584, Loss: 0.0115, LR: 0.002857\n",
            "Step 1020/1584, Loss: 0.0110, LR: 0.002832\n",
            "Step 1030/1584, Loss: 0.0463, LR: 0.002807\n",
            "Step 1040/1584, Loss: 0.0402, LR: 0.002782\n",
            "Step 1050/1584, Loss: 0.0258, LR: 0.002757\n",
            "Step 1060/1584, Loss: 0.0236, LR: 0.002732\n",
            "Step 1070/1584, Loss: 0.0184, LR: 0.002707\n",
            "Step 1080/1584, Loss: 0.0103, LR: 0.002682\n",
            "Step 1090/1584, Loss: 0.0139, LR: 0.002657\n",
            "Step 1100/1584, Loss: 0.0236, LR: 0.002632\n",
            "Step 1110/1584, Loss: 0.0079, LR: 0.002607\n",
            "Step 1120/1584, Loss: 0.0171, LR: 0.002582\n",
            "Step 1130/1584, Loss: 0.0598, LR: 0.002557\n",
            "Step 1140/1584, Loss: 0.1097, LR: 0.002532\n",
            "Step 1150/1584, Loss: 0.0491, LR: 0.002507\n",
            "Step 1160/1584, Loss: 0.0221, LR: 0.002482\n",
            "Step 1170/1584, Loss: 0.0175, LR: 0.002457\n",
            "Step 1180/1584, Loss: 0.0383, LR: 0.002432\n",
            "Step 1190/1584, Loss: 0.0260, LR: 0.002407\n",
            "Step 1200/1584, Loss: 0.0117, LR: 0.002382\n",
            "Step 1210/1584, Loss: 0.0150, LR: 0.002357\n",
            "Step 1220/1584, Loss: 0.0182, LR: 0.002332\n",
            "Step 1230/1584, Loss: 0.0187, LR: 0.002307\n",
            "Step 1240/1584, Loss: 0.0081, LR: 0.002282\n",
            "Step 1250/1584, Loss: 0.0083, LR: 0.002257\n",
            "Step 1260/1584, Loss: 0.0188, LR: 0.002232\n",
            "Step 1270/1584, Loss: 0.0213, LR: 0.002207\n",
            "Step 1280/1584, Loss: 0.0166, LR: 0.002182\n",
            "Step 1290/1584, Loss: 0.0282, LR: 0.002157\n",
            "Step 1300/1584, Loss: 0.0140, LR: 0.002132\n",
            "Step 1310/1584, Loss: 0.0177, LR: 0.002107\n",
            "Step 1320/1584, Loss: 0.0225, LR: 0.002082\n",
            "Step 1330/1584, Loss: 0.0260, LR: 0.002057\n",
            "Step 1340/1584, Loss: 0.0154, LR: 0.002032\n",
            "Step 1350/1584, Loss: 0.0211, LR: 0.002007\n",
            "Step 1360/1584, Loss: 0.0128, LR: 0.001982\n",
            "Step 1370/1584, Loss: 0.0129, LR: 0.001957\n",
            "Step 1380/1584, Loss: 0.0429, LR: 0.001932\n",
            "Step 1390/1584, Loss: 0.0250, LR: 0.001907\n",
            "Step 1400/1584, Loss: 0.0169, LR: 0.001882\n",
            "Step 1410/1584, Loss: 0.0099, LR: 0.001857\n",
            "Step 1420/1584, Loss: 0.0173, LR: 0.001832\n",
            "Step 1430/1584, Loss: 0.0410, LR: 0.001807\n",
            "Step 1440/1584, Loss: 0.0120, LR: 0.001782\n",
            "Step 1450/1584, Loss: 0.0158, LR: 0.001757\n",
            "Step 1460/1584, Loss: 0.0106, LR: 0.001732\n",
            "Step 1470/1584, Loss: 0.0097, LR: 0.001707\n",
            "Step 1480/1584, Loss: 0.0138, LR: 0.001682\n",
            "Step 1490/1584, Loss: 0.0089, LR: 0.001657\n",
            "Step 1500/1584, Loss: 0.0080, LR: 0.001632\n",
            "Step 1510/1584, Loss: 0.0126, LR: 0.001607\n",
            "Step 1520/1584, Loss: 0.0073, LR: 0.001582\n",
            "Step 1530/1584, Loss: 0.0107, LR: 0.001557\n",
            "Step 1540/1584, Loss: 0.0209, LR: 0.001532\n",
            "Step 1550/1584, Loss: 0.0165, LR: 0.001507\n",
            "Step 1560/1584, Loss: 0.0255, LR: 0.001482\n",
            "Step 1570/1584, Loss: 0.0269, LR: 0.001457\n",
            "Step 1580/1584, Loss: 0.0191, LR: 0.001432\n",
            "\n",
            "Epoch 10 Results:\n",
            "Training Loss: 0.0238\n",
            "Validation Loss: 0.0651\n",
            "Learning Rate: 0.003402\n",
            "\n",
            "Saved checkpoint for epoch 10: ./training_checkpoints/ckpt-2\n",
            "\n",
            "Epoch 11/50\n",
            "Step 0/1584, Loss: 0.0231, LR: 0.001422\n",
            "Step 10/1584, Loss: 0.0245, LR: 0.001400\n",
            "Step 20/1584, Loss: 0.0186, LR: 0.001375\n",
            "Step 30/1584, Loss: 0.0145, LR: 0.001350\n",
            "Step 40/1584, Loss: 0.0118, LR: 0.001325\n",
            "Step 50/1584, Loss: 0.0138, LR: 0.001300\n",
            "Step 60/1584, Loss: 0.0134, LR: 0.001275\n",
            "Step 70/1584, Loss: 0.0258, LR: 0.001250\n",
            "Step 80/1584, Loss: 0.0105, LR: 0.001225\n",
            "Step 90/1584, Loss: 0.0162, LR: 0.001200\n",
            "Step 100/1584, Loss: 0.0067, LR: 0.001175\n",
            "Step 110/1584, Loss: 0.0084, LR: 0.001150\n",
            "Step 120/1584, Loss: 0.0227, LR: 0.001125\n",
            "Step 130/1584, Loss: 0.0186, LR: 0.001100\n",
            "Step 140/1584, Loss: 0.0103, LR: 0.001075\n",
            "Step 150/1584, Loss: 0.0158, LR: 0.001050\n",
            "Step 160/1584, Loss: 0.0182, LR: 0.001025\n",
            "Step 170/1584, Loss: 0.0110, LR: 0.001000\n",
            "Step 180/1584, Loss: 0.0132, LR: 0.001025\n",
            "Step 190/1584, Loss: 0.0164, LR: 0.001050\n",
            "Step 200/1584, Loss: 0.0196, LR: 0.001075\n",
            "Step 210/1584, Loss: 0.0145, LR: 0.001100\n",
            "Step 220/1584, Loss: 0.0174, LR: 0.001125\n",
            "Step 230/1584, Loss: 0.0092, LR: 0.001150\n",
            "Step 240/1584, Loss: 0.0496, LR: 0.001175\n",
            "Step 250/1584, Loss: 0.0284, LR: 0.001200\n",
            "Step 260/1584, Loss: 0.0394, LR: 0.001225\n",
            "Step 270/1584, Loss: 0.0499, LR: 0.001250\n",
            "Step 280/1584, Loss: 0.0696, LR: 0.001275\n",
            "Step 290/1584, Loss: 0.0561, LR: 0.001300\n",
            "Step 300/1584, Loss: 0.0159, LR: 0.001325\n",
            "Step 310/1584, Loss: 0.0154, LR: 0.001350\n",
            "Step 320/1584, Loss: 0.0200, LR: 0.001375\n",
            "Step 330/1584, Loss: 0.0238, LR: 0.001400\n",
            "Step 340/1584, Loss: 0.0287, LR: 0.001425\n",
            "Step 350/1584, Loss: 0.0212, LR: 0.001450\n",
            "Step 360/1584, Loss: 0.0193, LR: 0.001475\n",
            "Step 370/1584, Loss: 0.0166, LR: 0.001500\n",
            "Step 380/1584, Loss: 0.0182, LR: 0.001525\n",
            "Step 390/1584, Loss: 0.0220, LR: 0.001550\n",
            "Step 400/1584, Loss: 0.0487, LR: 0.001575\n",
            "Step 410/1584, Loss: 0.0354, LR: 0.001600\n",
            "Step 420/1584, Loss: 0.0155, LR: 0.001625\n",
            "Step 430/1584, Loss: 0.0120, LR: 0.001650\n",
            "Step 440/1584, Loss: 0.0696, LR: 0.001675\n",
            "Step 450/1584, Loss: 0.0184, LR: 0.001700\n",
            "Step 460/1584, Loss: 0.0201, LR: 0.001725\n",
            "Step 470/1584, Loss: 0.0134, LR: 0.001750\n",
            "Step 480/1584, Loss: 0.0118, LR: 0.001775\n",
            "Step 490/1584, Loss: 0.0176, LR: 0.001800\n",
            "Step 500/1584, Loss: 0.0101, LR: 0.001825\n",
            "Step 510/1584, Loss: 0.0087, LR: 0.001850\n",
            "Step 520/1584, Loss: 0.0142, LR: 0.001875\n",
            "Step 530/1584, Loss: 0.0130, LR: 0.001900\n",
            "Step 540/1584, Loss: 0.0102, LR: 0.001925\n",
            "Step 550/1584, Loss: 0.0178, LR: 0.001950\n",
            "Step 560/1584, Loss: 0.0203, LR: 0.001975\n",
            "Step 570/1584, Loss: 0.0208, LR: 0.002000\n",
            "Step 580/1584, Loss: 0.0165, LR: 0.002025\n",
            "Step 590/1584, Loss: 0.0197, LR: 0.002050\n",
            "Step 600/1584, Loss: 0.0223, LR: 0.002075\n",
            "Step 610/1584, Loss: 0.0637, LR: 0.002100\n",
            "Step 620/1584, Loss: 0.0638, LR: 0.002125\n",
            "Step 630/1584, Loss: 0.0344, LR: 0.002150\n",
            "Step 640/1584, Loss: 0.0251, LR: 0.002175\n",
            "Step 650/1584, Loss: 0.0338, LR: 0.002200\n",
            "Step 660/1584, Loss: 0.0150, LR: 0.002225\n",
            "Step 670/1584, Loss: 0.0382, LR: 0.002250\n",
            "Step 680/1584, Loss: 0.0317, LR: 0.002275\n",
            "Step 690/1584, Loss: 0.0284, LR: 0.002300\n",
            "Step 700/1584, Loss: 0.0268, LR: 0.002325\n",
            "Step 710/1584, Loss: 0.0169, LR: 0.002350\n",
            "Step 720/1584, Loss: 0.0305, LR: 0.002375\n",
            "Step 730/1584, Loss: 0.0341, LR: 0.002400\n",
            "Step 740/1584, Loss: 0.0152, LR: 0.002425\n",
            "Step 750/1584, Loss: 0.0580, LR: 0.002450\n",
            "Step 760/1584, Loss: 0.0609, LR: 0.002475\n",
            "Step 770/1584, Loss: 0.0292, LR: 0.002500\n",
            "Step 780/1584, Loss: 0.0231, LR: 0.002525\n",
            "Step 790/1584, Loss: 0.0252, LR: 0.002550\n",
            "Step 800/1584, Loss: 0.0185, LR: 0.002575\n",
            "Step 810/1584, Loss: 0.0406, LR: 0.002600\n",
            "Step 820/1584, Loss: 0.0297, LR: 0.002625\n",
            "Step 830/1584, Loss: 0.0166, LR: 0.002650\n",
            "Step 840/1584, Loss: 0.0126, LR: 0.002675\n",
            "Step 850/1584, Loss: 0.0278, LR: 0.002700\n",
            "Step 860/1584, Loss: 0.0294, LR: 0.002725\n",
            "Step 870/1584, Loss: 0.0111, LR: 0.002750\n",
            "Step 880/1584, Loss: 0.0279, LR: 0.002775\n",
            "Step 890/1584, Loss: 0.0310, LR: 0.002800\n",
            "Step 900/1584, Loss: 0.0422, LR: 0.002825\n",
            "Step 910/1584, Loss: 0.0168, LR: 0.002850\n",
            "Step 920/1584, Loss: 0.0102, LR: 0.002875\n",
            "Step 930/1584, Loss: 0.0555, LR: 0.002900\n",
            "Step 940/1584, Loss: 0.0712, LR: 0.002925\n",
            "Step 950/1584, Loss: 0.0281, LR: 0.002950\n",
            "Step 960/1584, Loss: 0.0146, LR: 0.002975\n",
            "Step 970/1584, Loss: 0.0223, LR: 0.003000\n",
            "Step 980/1584, Loss: 0.0098, LR: 0.003025\n",
            "Step 990/1584, Loss: 0.0187, LR: 0.003050\n",
            "Step 1000/1584, Loss: 0.0183, LR: 0.003075\n",
            "Step 1010/1584, Loss: 0.0120, LR: 0.003100\n",
            "Step 1020/1584, Loss: 0.0097, LR: 0.003125\n",
            "Step 1030/1584, Loss: 0.0439, LR: 0.003150\n",
            "Step 1040/1584, Loss: 0.0421, LR: 0.003175\n",
            "Step 1050/1584, Loss: 0.0276, LR: 0.003200\n",
            "Step 1060/1584, Loss: 0.0237, LR: 0.003225\n",
            "Step 1070/1584, Loss: 0.0185, LR: 0.003250\n",
            "Step 1080/1584, Loss: 0.0114, LR: 0.003275\n",
            "Step 1090/1584, Loss: 0.0071, LR: 0.003300\n",
            "Step 1100/1584, Loss: 0.0308, LR: 0.003325\n",
            "Step 1110/1584, Loss: 0.0080, LR: 0.003350\n",
            "Step 1120/1584, Loss: 0.0135, LR: 0.003375\n",
            "Step 1130/1584, Loss: 0.0518, LR: 0.003400\n",
            "Step 1140/1584, Loss: 0.1108, LR: 0.003425\n",
            "Step 1150/1584, Loss: 0.0558, LR: 0.003450\n",
            "Step 1160/1584, Loss: 0.0257, LR: 0.003475\n",
            "Step 1170/1584, Loss: 0.0172, LR: 0.003500\n",
            "Step 1180/1584, Loss: 0.0373, LR: 0.003525\n",
            "Step 1190/1584, Loss: 0.0290, LR: 0.003550\n",
            "Step 1200/1584, Loss: 0.0109, LR: 0.003575\n",
            "Step 1210/1584, Loss: 0.0165, LR: 0.003600\n",
            "Step 1220/1584, Loss: 0.0176, LR: 0.003625\n",
            "Step 1230/1584, Loss: 0.0189, LR: 0.003650\n",
            "Step 1240/1584, Loss: 0.0095, LR: 0.003675\n",
            "Step 1250/1584, Loss: 0.0085, LR: 0.003700\n",
            "Step 1260/1584, Loss: 0.0181, LR: 0.003725\n",
            "Step 1270/1584, Loss: 0.0216, LR: 0.003750\n",
            "Step 1280/1584, Loss: 0.0161, LR: 0.003775\n",
            "Step 1290/1584, Loss: 0.0298, LR: 0.003800\n",
            "Step 1300/1584, Loss: 0.0154, LR: 0.003825\n",
            "Step 1310/1584, Loss: 0.0163, LR: 0.003850\n",
            "Step 1320/1584, Loss: 0.0225, LR: 0.003875\n",
            "Step 1330/1584, Loss: 0.0259, LR: 0.003900\n",
            "Step 1340/1584, Loss: 0.0163, LR: 0.003925\n",
            "Step 1350/1584, Loss: 0.0198, LR: 0.003950\n",
            "Step 1360/1584, Loss: 0.0126, LR: 0.003975\n",
            "Step 1370/1584, Loss: 0.0133, LR: 0.004000\n",
            "Step 1380/1584, Loss: 0.0416, LR: 0.004025\n",
            "Step 1390/1584, Loss: 0.0300, LR: 0.004050\n",
            "Step 1400/1584, Loss: 0.0188, LR: 0.004075\n",
            "Step 1410/1584, Loss: 0.0087, LR: 0.004100\n",
            "Step 1420/1584, Loss: 0.0192, LR: 0.004125\n",
            "Step 1430/1584, Loss: 0.0351, LR: 0.004150\n",
            "Step 1440/1584, Loss: 0.0171, LR: 0.004175\n",
            "Step 1450/1584, Loss: 0.0154, LR: 0.004200\n",
            "Step 1460/1584, Loss: 0.0096, LR: 0.004225\n",
            "Step 1470/1584, Loss: 0.0102, LR: 0.004250\n",
            "Step 1480/1584, Loss: 0.0155, LR: 0.004275\n",
            "Step 1490/1584, Loss: 0.0091, LR: 0.004300\n",
            "Step 1500/1584, Loss: 0.0077, LR: 0.004325\n",
            "Step 1510/1584, Loss: 0.0130, LR: 0.004350\n",
            "Step 1520/1584, Loss: 0.0082, LR: 0.004375\n",
            "Step 1530/1584, Loss: 0.0090, LR: 0.004400\n",
            "Step 1540/1584, Loss: 0.0211, LR: 0.004425\n",
            "Step 1550/1584, Loss: 0.0154, LR: 0.004450\n",
            "Step 1560/1584, Loss: 0.0226, LR: 0.004475\n",
            "Step 1570/1584, Loss: 0.0260, LR: 0.004500\n",
            "Step 1580/1584, Loss: 0.0175, LR: 0.004525\n",
            "\n",
            "Epoch 11 Results:\n",
            "Training Loss: 0.0237\n",
            "Validation Loss: 0.0826\n",
            "Learning Rate: 0.002600\n",
            "\n",
            "Epoch 12/50\n",
            "Step 0/1584, Loss: 0.0223, LR: 0.004535\n",
            "Step 20/1584, Loss: 0.0212, LR: 0.004583\n",
            "Step 30/1584, Loss: 0.0173, LR: 0.004608\n",
            "Step 40/1584, Loss: 0.0109, LR: 0.004633\n",
            "Step 50/1584, Loss: 0.0128, LR: 0.004658\n",
            "Step 60/1584, Loss: 0.0151, LR: 0.004683\n",
            "Step 70/1584, Loss: 0.0195, LR: 0.004708\n",
            "Step 80/1584, Loss: 0.0139, LR: 0.004732\n",
            "Step 90/1584, Loss: 0.0169, LR: 0.004758\n",
            "Step 100/1584, Loss: 0.0083, LR: 0.004783\n",
            "Step 110/1584, Loss: 0.0072, LR: 0.004808\n",
            "Step 120/1584, Loss: 0.0213, LR: 0.004833\n",
            "Step 130/1584, Loss: 0.0185, LR: 0.004858\n",
            "Step 140/1584, Loss: 0.0088, LR: 0.004883\n",
            "Step 150/1584, Loss: 0.0168, LR: 0.004908\n",
            "Step 160/1584, Loss: 0.0223, LR: 0.004932\n",
            "Step 170/1584, Loss: 0.0127, LR: 0.004958\n",
            "Step 180/1584, Loss: 0.0139, LR: 0.004983\n",
            "Step 190/1584, Loss: 0.0163, LR: 0.005008\n",
            "Step 200/1584, Loss: 0.0191, LR: 0.005033\n",
            "Step 210/1584, Loss: 0.0144, LR: 0.005058\n",
            "Step 220/1584, Loss: 0.0177, LR: 0.005083\n",
            "Step 230/1584, Loss: 0.0101, LR: 0.005108\n",
            "Step 240/1584, Loss: 0.0431, LR: 0.005133\n",
            "Step 250/1584, Loss: 0.0338, LR: 0.005158\n",
            "Step 260/1584, Loss: 0.0345, LR: 0.005183\n",
            "Step 270/1584, Loss: 0.0488, LR: 0.005208\n",
            "Step 280/1584, Loss: 0.0643, LR: 0.005233\n",
            "Step 290/1584, Loss: 0.0583, LR: 0.005258\n",
            "Step 300/1584, Loss: 0.0201, LR: 0.005283\n",
            "Step 310/1584, Loss: 0.0178, LR: 0.005308\n",
            "Step 320/1584, Loss: 0.0158, LR: 0.005333\n",
            "Step 330/1584, Loss: 0.0274, LR: 0.005358\n",
            "Step 340/1584, Loss: 0.0305, LR: 0.005383\n",
            "Step 350/1584, Loss: 0.0224, LR: 0.005408\n",
            "Step 360/1584, Loss: 0.0174, LR: 0.005433\n",
            "Step 370/1584, Loss: 0.0177, LR: 0.005458\n",
            "Step 380/1584, Loss: 0.0179, LR: 0.005483\n",
            "Step 390/1584, Loss: 0.0212, LR: 0.005508\n",
            "Step 400/1584, Loss: 0.0481, LR: 0.005533\n",
            "Step 410/1584, Loss: 0.0416, LR: 0.005557\n",
            "Step 420/1584, Loss: 0.0200, LR: 0.005583\n",
            "Step 430/1584, Loss: 0.0139, LR: 0.005608\n",
            "Step 440/1584, Loss: 0.0744, LR: 0.005633\n",
            "Step 450/1584, Loss: 0.0177, LR: 0.005658\n",
            "Step 460/1584, Loss: 0.0232, LR: 0.005683\n",
            "Step 470/1584, Loss: 0.0182, LR: 0.005708\n",
            "Step 480/1584, Loss: 0.0119, LR: 0.005733\n",
            "Step 490/1584, Loss: 0.0195, LR: 0.005758\n",
            "Step 500/1584, Loss: 0.0083, LR: 0.005783\n",
            "Step 510/1584, Loss: 0.0083, LR: 0.005808\n",
            "Step 520/1584, Loss: 0.0135, LR: 0.005833\n",
            "Step 530/1584, Loss: 0.0137, LR: 0.005858\n",
            "Step 540/1584, Loss: 0.0089, LR: 0.005883\n",
            "Step 550/1584, Loss: 0.0200, LR: 0.005908\n",
            "Step 560/1584, Loss: 0.0191, LR: 0.005933\n",
            "Step 570/1584, Loss: 0.0210, LR: 0.005958\n",
            "Step 580/1584, Loss: 0.0155, LR: 0.005983\n",
            "Step 590/1584, Loss: 0.0189, LR: 0.005992\n",
            "Step 600/1584, Loss: 0.0192, LR: 0.005967\n",
            "Step 610/1584, Loss: 0.0520, LR: 0.005942\n",
            "Step 620/1584, Loss: 0.0638, LR: 0.005917\n",
            "Step 630/1584, Loss: 0.0433, LR: 0.005892\n",
            "Step 640/1584, Loss: 0.0291, LR: 0.005867\n",
            "Step 650/1584, Loss: 0.0367, LR: 0.005842\n",
            "Step 660/1584, Loss: 0.0114, LR: 0.005817\n",
            "Step 670/1584, Loss: 0.0393, LR: 0.005792\n",
            "Step 680/1584, Loss: 0.0344, LR: 0.005767\n",
            "Step 690/1584, Loss: 0.0283, LR: 0.005742\n",
            "Step 700/1584, Loss: 0.0242, LR: 0.005717\n",
            "Step 710/1584, Loss: 0.0181, LR: 0.005692\n",
            "Step 720/1584, Loss: 0.0331, LR: 0.005667\n",
            "Step 730/1584, Loss: 0.0297, LR: 0.005642\n",
            "Step 740/1584, Loss: 0.0217, LR: 0.005617\n",
            "Step 750/1584, Loss: 0.0529, LR: 0.005592\n",
            "Step 760/1584, Loss: 0.0572, LR: 0.005567\n",
            "Step 770/1584, Loss: 0.0335, LR: 0.005542\n",
            "Step 780/1584, Loss: 0.0260, LR: 0.005517\n",
            "Step 790/1584, Loss: 0.0275, LR: 0.005492\n",
            "Step 800/1584, Loss: 0.0206, LR: 0.005467\n",
            "Step 810/1584, Loss: 0.0457, LR: 0.005442\n",
            "Step 820/1584, Loss: 0.0332, LR: 0.005417\n",
            "Step 830/1584, Loss: 0.0192, LR: 0.005393\n",
            "Step 840/1584, Loss: 0.0125, LR: 0.005367\n",
            "Step 850/1584, Loss: 0.0239, LR: 0.005342\n",
            "Step 860/1584, Loss: 0.0336, LR: 0.005317\n",
            "Step 870/1584, Loss: 0.0102, LR: 0.005292\n",
            "Step 880/1584, Loss: 0.0257, LR: 0.005267\n",
            "Step 890/1584, Loss: 0.0323, LR: 0.005242\n",
            "Step 900/1584, Loss: 0.0415, LR: 0.005217\n",
            "Step 910/1584, Loss: 0.0213, LR: 0.005193\n",
            "Step 920/1584, Loss: 0.0099, LR: 0.005167\n",
            "Step 930/1584, Loss: 0.0500, LR: 0.005142\n",
            "Step 940/1584, Loss: 0.0748, LR: 0.005117\n",
            "Step 950/1584, Loss: 0.0326, LR: 0.005092\n",
            "Step 960/1584, Loss: 0.0153, LR: 0.005067\n",
            "Step 970/1584, Loss: 0.0225, LR: 0.005042\n",
            "Step 980/1584, Loss: 0.0105, LR: 0.005017\n",
            "Step 990/1584, Loss: 0.0175, LR: 0.004992\n",
            "Step 1000/1584, Loss: 0.0188, LR: 0.004967\n",
            "Step 1010/1584, Loss: 0.0123, LR: 0.004942\n",
            "Step 1020/1584, Loss: 0.0079, LR: 0.004917\n",
            "Step 1030/1584, Loss: 0.0427, LR: 0.004892\n",
            "Step 1040/1584, Loss: 0.0431, LR: 0.004867\n",
            "Step 1050/1584, Loss: 0.0338, LR: 0.004842\n",
            "Step 1060/1584, Loss: 0.0235, LR: 0.004817\n",
            "Step 1070/1584, Loss: 0.0193, LR: 0.004792\n",
            "Step 1080/1584, Loss: 0.0115, LR: 0.004767\n",
            "Step 1090/1584, Loss: 0.0067, LR: 0.004742\n",
            "Step 1100/1584, Loss: 0.0324, LR: 0.004717\n",
            "Step 1110/1584, Loss: 0.0086, LR: 0.004692\n",
            "Step 1120/1584, Loss: 0.0098, LR: 0.004667\n",
            "Step 1130/1584, Loss: 0.0444, LR: 0.004642\n",
            "Step 1140/1584, Loss: 0.1108, LR: 0.004617\n",
            "Step 1150/1584, Loss: 0.0624, LR: 0.004592\n",
            "Step 1160/1584, Loss: 0.0289, LR: 0.004568\n",
            "Step 1170/1584, Loss: 0.0166, LR: 0.004542\n",
            "Step 1180/1584, Loss: 0.0362, LR: 0.004517\n",
            "Step 1190/1584, Loss: 0.0321, LR: 0.004492\n",
            "Step 1200/1584, Loss: 0.0096, LR: 0.004467\n",
            "Step 1210/1584, Loss: 0.0190, LR: 0.004442\n",
            "Step 1220/1584, Loss: 0.0163, LR: 0.004417\n",
            "Step 1230/1584, Loss: 0.0193, LR: 0.004392\n",
            "Step 1240/1584, Loss: 0.0107, LR: 0.004367\n",
            "Step 1250/1584, Loss: 0.0086, LR: 0.004342\n",
            "Step 1260/1584, Loss: 0.0165, LR: 0.004317\n",
            "Step 1270/1584, Loss: 0.0226, LR: 0.004292\n",
            "Step 1280/1584, Loss: 0.0166, LR: 0.004267\n",
            "Step 1290/1584, Loss: 0.0270, LR: 0.004242\n",
            "Step 1300/1584, Loss: 0.0188, LR: 0.004217\n",
            "Step 1310/1584, Loss: 0.0153, LR: 0.004192\n",
            "Step 1320/1584, Loss: 0.0225, LR: 0.004167\n",
            "Step 1330/1584, Loss: 0.0229, LR: 0.004142\n",
            "Step 1340/1584, Loss: 0.0199, LR: 0.004117\n",
            "Step 1350/1584, Loss: 0.0202, LR: 0.004092\n",
            "Step 1360/1584, Loss: 0.0120, LR: 0.004067\n",
            "Step 1370/1584, Loss: 0.0135, LR: 0.004042\n",
            "Step 1380/1584, Loss: 0.0350, LR: 0.004017\n",
            "Step 1390/1584, Loss: 0.0358, LR: 0.003992\n",
            "Step 1400/1584, Loss: 0.0200, LR: 0.003967\n",
            "Step 1410/1584, Loss: 0.0072, LR: 0.003942\n",
            "Step 1420/1584, Loss: 0.0203, LR: 0.003917\n",
            "Step 1430/1584, Loss: 0.0300, LR: 0.003892\n",
            "Step 1440/1584, Loss: 0.0218, LR: 0.003867\n",
            "Step 1450/1584, Loss: 0.0157, LR: 0.003842\n",
            "Step 1460/1584, Loss: 0.0106, LR: 0.003817\n",
            "Step 1470/1584, Loss: 0.0092, LR: 0.003792\n",
            "Step 1480/1584, Loss: 0.0146, LR: 0.003767\n",
            "Step 1490/1584, Loss: 0.0103, LR: 0.003742\n",
            "Step 1500/1584, Loss: 0.0079, LR: 0.003717\n",
            "Step 1510/1584, Loss: 0.0117, LR: 0.003692\n",
            "Step 1520/1584, Loss: 0.0095, LR: 0.003667\n",
            "Step 1530/1584, Loss: 0.0073, LR: 0.003642\n",
            "Step 1540/1584, Loss: 0.0210, LR: 0.003617\n",
            "Step 1550/1584, Loss: 0.0159, LR: 0.003592\n",
            "Step 1560/1584, Loss: 0.0225, LR: 0.003567\n",
            "Step 1570/1584, Loss: 0.0274, LR: 0.003542\n",
            "Step 1580/1584, Loss: 0.0167, LR: 0.003518\n",
            "\n",
            "Epoch 12 Results:\n",
            "Training Loss: 0.0241\n",
            "Validation Loss: 0.0789\n",
            "Learning Rate: 0.004944\n",
            "\n",
            "Epoch 13/50\n",
            "Step 0/1584, Loss: 0.0259, LR: 0.003507\n",
            "Step 10/1584, Loss: 0.0220, LR: 0.003483\n",
            "Step 20/1584, Loss: 0.0232, LR: 0.003460\n",
            "Step 30/1584, Loss: 0.0174, LR: 0.003435\n",
            "Step 40/1584, Loss: 0.0107, LR: 0.003410\n",
            "Step 50/1584, Loss: 0.0108, LR: 0.003385\n",
            "Step 60/1584, Loss: 0.0175, LR: 0.003360\n",
            "Step 70/1584, Loss: 0.0155, LR: 0.003335\n",
            "Step 80/1584, Loss: 0.0170, LR: 0.003310\n",
            "Step 90/1584, Loss: 0.0167, LR: 0.003285\n",
            "Step 100/1584, Loss: 0.0097, LR: 0.003260\n",
            "Step 110/1584, Loss: 0.0064, LR: 0.003235\n",
            "Step 120/1584, Loss: 0.0195, LR: 0.003210\n",
            "Step 130/1584, Loss: 0.0195, LR: 0.003185\n",
            "Step 140/1584, Loss: 0.0102, LR: 0.003160\n",
            "Step 150/1584, Loss: 0.0125, LR: 0.003135\n",
            "Step 160/1584, Loss: 0.0257, LR: 0.003110\n",
            "Step 170/1584, Loss: 0.0104, LR: 0.003085\n",
            "Step 180/1584, Loss: 0.0135, LR: 0.003060\n",
            "Step 190/1584, Loss: 0.0162, LR: 0.003035\n",
            "Step 200/1584, Loss: 0.0191, LR: 0.003010\n",
            "Step 210/1584, Loss: 0.0145, LR: 0.002985\n",
            "Step 220/1584, Loss: 0.0182, LR: 0.002960\n",
            "Step 230/1584, Loss: 0.0095, LR: 0.002935\n",
            "Step 240/1584, Loss: 0.0365, LR: 0.002910\n",
            "Step 250/1584, Loss: 0.0395, LR: 0.002885\n",
            "Step 260/1584, Loss: 0.0323, LR: 0.002860\n",
            "Step 270/1584, Loss: 0.0450, LR: 0.002835\n",
            "Step 280/1584, Loss: 0.0655, LR: 0.002810\n",
            "Step 290/1584, Loss: 0.0641, LR: 0.002785\n",
            "Step 300/1584, Loss: 0.0183, LR: 0.002760\n",
            "Step 310/1584, Loss: 0.0186, LR: 0.002735\n",
            "Step 320/1584, Loss: 0.0178, LR: 0.002710\n",
            "Step 330/1584, Loss: 0.0264, LR: 0.002685\n",
            "Step 340/1584, Loss: 0.0266, LR: 0.002660\n",
            "Step 350/1584, Loss: 0.0238, LR: 0.002635\n",
            "Step 360/1584, Loss: 0.0168, LR: 0.002610\n",
            "Step 370/1584, Loss: 0.0183, LR: 0.002585\n",
            "Step 380/1584, Loss: 0.0171, LR: 0.002560\n",
            "Step 390/1584, Loss: 0.0229, LR: 0.002535\n",
            "Step 400/1584, Loss: 0.0407, LR: 0.002510\n",
            "Step 410/1584, Loss: 0.0465, LR: 0.002485\n",
            "Step 420/1584, Loss: 0.0152, LR: 0.002460\n",
            "Step 430/1584, Loss: 0.0124, LR: 0.002435\n",
            "Step 440/1584, Loss: 0.0618, LR: 0.002410\n",
            "Step 450/1584, Loss: 0.0240, LR: 0.002385\n",
            "Step 460/1584, Loss: 0.0226, LR: 0.002360\n",
            "Step 470/1584, Loss: 0.0147, LR: 0.002335\n",
            "Step 480/1584, Loss: 0.0124, LR: 0.002310\n",
            "Step 490/1584, Loss: 0.0163, LR: 0.002285\n",
            "Step 500/1584, Loss: 0.0109, LR: 0.002260\n",
            "Step 510/1584, Loss: 0.0094, LR: 0.002235\n",
            "Step 520/1584, Loss: 0.0116, LR: 0.002210\n",
            "Step 530/1584, Loss: 0.0133, LR: 0.002185\n",
            "Step 540/1584, Loss: 0.0105, LR: 0.002160\n",
            "Step 550/1584, Loss: 0.0192, LR: 0.002135\n",
            "Step 560/1584, Loss: 0.0173, LR: 0.002110\n",
            "Step 570/1584, Loss: 0.0226, LR: 0.002085\n",
            "Step 580/1584, Loss: 0.0167, LR: 0.002060\n",
            "Step 590/1584, Loss: 0.0199, LR: 0.002035\n",
            "Step 600/1584, Loss: 0.0192, LR: 0.002010\n",
            "Step 610/1584, Loss: 0.0526, LR: 0.001985\n",
            "Step 620/1584, Loss: 0.0769, LR: 0.001960\n",
            "Step 630/1584, Loss: 0.0343, LR: 0.001935\n",
            "Step 640/1584, Loss: 0.0241, LR: 0.001910\n",
            "Step 650/1584, Loss: 0.0361, LR: 0.001885\n",
            "Step 660/1584, Loss: 0.0146, LR: 0.001860\n",
            "Step 670/1584, Loss: 0.0332, LR: 0.001835\n",
            "Step 680/1584, Loss: 0.0346, LR: 0.001810\n",
            "Step 690/1584, Loss: 0.0284, LR: 0.001785\n",
            "Step 700/1584, Loss: 0.0258, LR: 0.001760\n",
            "Step 710/1584, Loss: 0.0211, LR: 0.001735\n",
            "Step 720/1584, Loss: 0.0269, LR: 0.001710\n",
            "Step 730/1584, Loss: 0.0251, LR: 0.001685\n",
            "Step 740/1584, Loss: 0.0264, LR: 0.001660\n",
            "Step 750/1584, Loss: 0.0490, LR: 0.001635\n",
            "Step 760/1584, Loss: 0.0620, LR: 0.001610\n",
            "Step 770/1584, Loss: 0.0389, LR: 0.001585\n",
            "Step 780/1584, Loss: 0.0153, LR: 0.001560\n",
            "Step 790/1584, Loss: 0.0274, LR: 0.001535\n",
            "Step 800/1584, Loss: 0.0179, LR: 0.001510\n",
            "Step 810/1584, Loss: 0.0360, LR: 0.001485\n",
            "Step 820/1584, Loss: 0.0341, LR: 0.001460\n",
            "Step 830/1584, Loss: 0.0168, LR: 0.001435\n",
            "Step 840/1584, Loss: 0.0124, LR: 0.001410\n",
            "Step 850/1584, Loss: 0.0209, LR: 0.001385\n",
            "Step 860/1584, Loss: 0.0358, LR: 0.001360\n",
            "Step 870/1584, Loss: 0.0142, LR: 0.001335\n",
            "Step 880/1584, Loss: 0.0232, LR: 0.001310\n",
            "Step 890/1584, Loss: 0.0316, LR: 0.001285\n",
            "Step 900/1584, Loss: 0.0384, LR: 0.001260\n",
            "Step 910/1584, Loss: 0.0197, LR: 0.001235\n",
            "Step 920/1584, Loss: 0.0104, LR: 0.001210\n",
            "Step 930/1584, Loss: 0.0430, LR: 0.001185\n",
            "Step 940/1584, Loss: 0.0795, LR: 0.001160\n",
            "Step 950/1584, Loss: 0.0306, LR: 0.001135\n",
            "Step 960/1584, Loss: 0.0160, LR: 0.001110\n",
            "Step 970/1584, Loss: 0.0226, LR: 0.001085\n",
            "Step 980/1584, Loss: 0.0106, LR: 0.001060\n",
            "Step 990/1584, Loss: 0.0162, LR: 0.001035\n",
            "Step 1000/1584, Loss: 0.0190, LR: 0.001010\n",
            "Step 1010/1584, Loss: 0.0131, LR: 0.001015\n",
            "Step 1020/1584, Loss: 0.0096, LR: 0.001040\n",
            "Step 1030/1584, Loss: 0.0333, LR: 0.001065\n",
            "Step 1040/1584, Loss: 0.0453, LR: 0.001090\n",
            "Step 1050/1584, Loss: 0.0261, LR: 0.001115\n",
            "Step 1060/1584, Loss: 0.0230, LR: 0.001140\n",
            "Step 1070/1584, Loss: 0.0189, LR: 0.001165\n",
            "Step 1080/1584, Loss: 0.0142, LR: 0.001190\n",
            "Step 1090/1584, Loss: 0.0073, LR: 0.001215\n",
            "Step 1100/1584, Loss: 0.0281, LR: 0.001240\n",
            "Step 1110/1584, Loss: 0.0108, LR: 0.001265\n",
            "Step 1120/1584, Loss: 0.0094, LR: 0.001290\n",
            "Step 1130/1584, Loss: 0.0384, LR: 0.001315\n",
            "Step 1140/1584, Loss: 0.1067, LR: 0.001340\n",
            "Step 1150/1584, Loss: 0.0726, LR: 0.001365\n",
            "Step 1160/1584, Loss: 0.0238, LR: 0.001390\n",
            "Step 1170/1584, Loss: 0.0115, LR: 0.001415\n",
            "Step 1180/1584, Loss: 0.0322, LR: 0.001440\n",
            "Step 1190/1584, Loss: 0.0350, LR: 0.001465\n",
            "Step 1200/1584, Loss: 0.0133, LR: 0.001490\n",
            "Step 1210/1584, Loss: 0.0150, LR: 0.001515\n",
            "Step 1220/1584, Loss: 0.0143, LR: 0.001540\n",
            "Step 1230/1584, Loss: 0.0196, LR: 0.001565\n",
            "Step 1240/1584, Loss: 0.0112, LR: 0.001590\n",
            "Step 1250/1584, Loss: 0.0073, LR: 0.001615\n",
            "Step 1260/1584, Loss: 0.0139, LR: 0.001640\n",
            "Step 1270/1584, Loss: 0.0230, LR: 0.001665\n",
            "Step 1280/1584, Loss: 0.0189, LR: 0.001690\n",
            "Step 1290/1584, Loss: 0.0214, LR: 0.001715\n",
            "Step 1300/1584, Loss: 0.0205, LR: 0.001740\n",
            "Step 1310/1584, Loss: 0.0148, LR: 0.001765\n",
            "Step 1320/1584, Loss: 0.0225, LR: 0.001790\n",
            "Step 1330/1584, Loss: 0.0203, LR: 0.001815\n",
            "Step 1340/1584, Loss: 0.0230, LR: 0.001840\n",
            "Step 1350/1584, Loss: 0.0220, LR: 0.001865\n",
            "Step 1360/1584, Loss: 0.0129, LR: 0.001890\n",
            "Step 1370/1584, Loss: 0.0126, LR: 0.001915\n",
            "Step 1380/1584, Loss: 0.0279, LR: 0.001940\n",
            "Step 1390/1584, Loss: 0.0387, LR: 0.001965\n",
            "Step 1400/1584, Loss: 0.0191, LR: 0.001990\n",
            "Step 1410/1584, Loss: 0.0075, LR: 0.002015\n",
            "Step 1420/1584, Loss: 0.0199, LR: 0.002040\n",
            "Step 1430/1584, Loss: 0.0250, LR: 0.002065\n",
            "Step 1440/1584, Loss: 0.0269, LR: 0.002090\n",
            "Step 1450/1584, Loss: 0.0151, LR: 0.002115\n",
            "Step 1460/1584, Loss: 0.0125, LR: 0.002140\n",
            "Step 1470/1584, Loss: 0.0088, LR: 0.002165\n",
            "Step 1480/1584, Loss: 0.0113, LR: 0.002190\n",
            "Step 1490/1584, Loss: 0.0124, LR: 0.002215\n",
            "Step 1500/1584, Loss: 0.0079, LR: 0.002240\n",
            "Step 1510/1584, Loss: 0.0113, LR: 0.002265\n",
            "Step 1520/1584, Loss: 0.0101, LR: 0.002290\n",
            "Step 1530/1584, Loss: 0.0053, LR: 0.002315\n",
            "Step 1540/1584, Loss: 0.0211, LR: 0.002340\n",
            "Step 1550/1584, Loss: 0.0172, LR: 0.002365\n",
            "Step 1560/1584, Loss: 0.0211, LR: 0.002390\n",
            "Step 1570/1584, Loss: 0.0300, LR: 0.002415\n",
            "Step 1580/1584, Loss: 0.0166, LR: 0.002440\n",
            "\n",
            "Epoch 13 Results:\n",
            "Training Loss: 0.0235\n",
            "Validation Loss: 0.0729\n",
            "Learning Rate: 0.002060\n",
            "\n",
            "Epoch 14/50\n",
            "Step 0/1584, Loss: 0.0186, LR: 0.002450\n",
            "Step 10/1584, Loss: 0.0220, LR: 0.002475\n",
            "Step 20/1584, Loss: 0.0245, LR: 0.002498\n",
            "Step 30/1584, Loss: 0.0164, LR: 0.002523\n",
            "Step 40/1584, Loss: 0.0103, LR: 0.002548\n",
            "Step 50/1584, Loss: 0.0086, LR: 0.002573\n",
            "Step 60/1584, Loss: 0.0199, LR: 0.002598\n",
            "Step 70/1584, Loss: 0.0144, LR: 0.002623\n",
            "Step 80/1584, Loss: 0.0188, LR: 0.002648\n",
            "Step 90/1584, Loss: 0.0157, LR: 0.002673\n",
            "Step 100/1584, Loss: 0.0111, LR: 0.002698\n",
            "Step 110/1584, Loss: 0.0061, LR: 0.002723\n",
            "Step 120/1584, Loss: 0.0185, LR: 0.002748\n",
            "Step 130/1584, Loss: 0.0197, LR: 0.002773\n",
            "Step 140/1584, Loss: 0.0111, LR: 0.002798\n",
            "Step 150/1584, Loss: 0.0115, LR: 0.002823\n",
            "Step 160/1584, Loss: 0.0266, LR: 0.002848\n",
            "Step 170/1584, Loss: 0.0096, LR: 0.002873\n",
            "Step 180/1584, Loss: 0.0135, LR: 0.002898\n",
            "Step 190/1584, Loss: 0.0153, LR: 0.002923\n",
            "Step 200/1584, Loss: 0.0199, LR: 0.002948\n",
            "Step 210/1584, Loss: 0.0148, LR: 0.002973\n",
            "Step 220/1584, Loss: 0.0180, LR: 0.002998\n",
            "Step 230/1584, Loss: 0.0098, LR: 0.003023\n",
            "Step 240/1584, Loss: 0.0312, LR: 0.003048\n",
            "Step 250/1584, Loss: 0.0445, LR: 0.003073\n",
            "Step 260/1584, Loss: 0.0302, LR: 0.003098\n",
            "Step 270/1584, Loss: 0.0418, LR: 0.003123\n",
            "Step 280/1584, Loss: 0.0654, LR: 0.003148\n",
            "Step 290/1584, Loss: 0.0657, LR: 0.003173\n",
            "Step 300/1584, Loss: 0.0220, LR: 0.003198\n",
            "Step 310/1584, Loss: 0.0184, LR: 0.003223\n",
            "Step 320/1584, Loss: 0.0169, LR: 0.003248\n",
            "Step 330/1584, Loss: 0.0255, LR: 0.003273\n",
            "Step 340/1584, Loss: 0.0268, LR: 0.003298\n",
            "Step 350/1584, Loss: 0.0258, LR: 0.003323\n",
            "Step 360/1584, Loss: 0.0164, LR: 0.003348\n",
            "Step 370/1584, Loss: 0.0191, LR: 0.003373\n",
            "Step 380/1584, Loss: 0.0157, LR: 0.003398\n",
            "Step 390/1584, Loss: 0.0236, LR: 0.003423\n",
            "Step 400/1584, Loss: 0.0375, LR: 0.003448\n",
            "Step 410/1584, Loss: 0.0512, LR: 0.003473\n",
            "Step 420/1584, Loss: 0.0153, LR: 0.003498\n",
            "Step 430/1584, Loss: 0.0143, LR: 0.003523\n",
            "Step 440/1584, Loss: 0.0560, LR: 0.003548\n",
            "Step 450/1584, Loss: 0.0286, LR: 0.003573\n",
            "Step 460/1584, Loss: 0.0247, LR: 0.003598\n",
            "Step 470/1584, Loss: 0.0166, LR: 0.003623\n",
            "Step 480/1584, Loss: 0.0129, LR: 0.003648\n",
            "Step 490/1584, Loss: 0.0124, LR: 0.003673\n",
            "Step 500/1584, Loss: 0.0145, LR: 0.003698\n",
            "Step 510/1584, Loss: 0.0094, LR: 0.003723\n",
            "Step 520/1584, Loss: 0.0109, LR: 0.003748\n",
            "Step 530/1584, Loss: 0.0134, LR: 0.003773\n",
            "Step 540/1584, Loss: 0.0113, LR: 0.003798\n",
            "Step 550/1584, Loss: 0.0191, LR: 0.003823\n",
            "Step 560/1584, Loss: 0.0159, LR: 0.003848\n",
            "Step 570/1584, Loss: 0.0235, LR: 0.003873\n",
            "Step 580/1584, Loss: 0.0156, LR: 0.003898\n",
            "Step 590/1584, Loss: 0.0184, LR: 0.003923\n",
            "Step 600/1584, Loss: 0.0179, LR: 0.003948\n",
            "Step 610/1584, Loss: 0.0435, LR: 0.003973\n",
            "Step 620/1584, Loss: 0.0734, LR: 0.003998\n",
            "Step 630/1584, Loss: 0.0415, LR: 0.004023\n",
            "Step 640/1584, Loss: 0.0294, LR: 0.004048\n",
            "Step 650/1584, Loss: 0.0379, LR: 0.004073\n",
            "Step 660/1584, Loss: 0.0136, LR: 0.004098\n",
            "Step 670/1584, Loss: 0.0310, LR: 0.004123\n",
            "Step 680/1584, Loss: 0.0382, LR: 0.004148\n",
            "Step 690/1584, Loss: 0.0272, LR: 0.004173\n",
            "Step 700/1584, Loss: 0.0255, LR: 0.004198\n",
            "Step 710/1584, Loss: 0.0209, LR: 0.004223\n",
            "Step 720/1584, Loss: 0.0231, LR: 0.004248\n",
            "Step 730/1584, Loss: 0.0302, LR: 0.004273\n",
            "Step 740/1584, Loss: 0.0282, LR: 0.004298\n",
            "Step 750/1584, Loss: 0.0447, LR: 0.004323\n",
            "Step 760/1584, Loss: 0.0591, LR: 0.004348\n",
            "Step 770/1584, Loss: 0.0411, LR: 0.004373\n",
            "Step 780/1584, Loss: 0.0184, LR: 0.004398\n",
            "Step 790/1584, Loss: 0.0350, LR: 0.004423\n",
            "Step 800/1584, Loss: 0.0169, LR: 0.004448\n",
            "Step 810/1584, Loss: 0.0399, LR: 0.004473\n",
            "Step 820/1584, Loss: 0.0390, LR: 0.004498\n",
            "Step 830/1584, Loss: 0.0189, LR: 0.004523\n",
            "Step 840/1584, Loss: 0.0116, LR: 0.004548\n",
            "Step 850/1584, Loss: 0.0167, LR: 0.004573\n",
            "Step 860/1584, Loss: 0.0404, LR: 0.004598\n",
            "Step 870/1584, Loss: 0.0115, LR: 0.004623\n",
            "Step 880/1584, Loss: 0.0195, LR: 0.004648\n",
            "Step 890/1584, Loss: 0.0340, LR: 0.004673\n",
            "Step 900/1584, Loss: 0.0396, LR: 0.004698\n",
            "Step 910/1584, Loss: 0.0241, LR: 0.004723\n",
            "Step 920/1584, Loss: 0.0122, LR: 0.004748\n",
            "Step 930/1584, Loss: 0.0364, LR: 0.004773\n",
            "Step 940/1584, Loss: 0.0783, LR: 0.004798\n",
            "Step 950/1584, Loss: 0.0404, LR: 0.004823\n",
            "Step 960/1584, Loss: 0.0165, LR: 0.004848\n",
            "Step 970/1584, Loss: 0.0228, LR: 0.004873\n",
            "Step 980/1584, Loss: 0.0116, LR: 0.004898\n",
            "Step 990/1584, Loss: 0.0146, LR: 0.004923\n",
            "Step 1000/1584, Loss: 0.0200, LR: 0.004948\n",
            "Step 1010/1584, Loss: 0.0135, LR: 0.004973\n",
            "Step 1020/1584, Loss: 0.0083, LR: 0.004998\n",
            "Step 1030/1584, Loss: 0.0341, LR: 0.005023\n",
            "Step 1040/1584, Loss: 0.0458, LR: 0.005048\n",
            "Step 1050/1584, Loss: 0.0366, LR: 0.005073\n",
            "Step 1060/1584, Loss: 0.0241, LR: 0.005098\n",
            "Step 1070/1584, Loss: 0.0208, LR: 0.005123\n",
            "Step 1080/1584, Loss: 0.0129, LR: 0.005148\n",
            "Step 1090/1584, Loss: 0.0071, LR: 0.005173\n",
            "Step 1100/1584, Loss: 0.0320, LR: 0.005198\n",
            "Step 1110/1584, Loss: 0.0087, LR: 0.005223\n",
            "Step 1120/1584, Loss: 0.0081, LR: 0.005248\n",
            "Step 1130/1584, Loss: 0.0365, LR: 0.005273\n",
            "Step 1140/1584, Loss: 0.1008, LR: 0.005298\n",
            "Step 1150/1584, Loss: 0.0779, LR: 0.005323\n",
            "Step 1160/1584, Loss: 0.0262, LR: 0.005348\n",
            "Step 1170/1584, Loss: 0.0217, LR: 0.005373\n",
            "Step 1180/1584, Loss: 0.0307, LR: 0.005398\n",
            "Step 1190/1584, Loss: 0.0388, LR: 0.005423\n",
            "Step 1200/1584, Loss: 0.0095, LR: 0.005448\n",
            "Step 1210/1584, Loss: 0.0192, LR: 0.005473\n",
            "Step 1220/1584, Loss: 0.0147, LR: 0.005498\n",
            "Step 1230/1584, Loss: 0.0201, LR: 0.005523\n",
            "Step 1240/1584, Loss: 0.0123, LR: 0.005548\n",
            "Step 1250/1584, Loss: 0.0091, LR: 0.005573\n",
            "Step 1260/1584, Loss: 0.0111, LR: 0.005598\n",
            "Step 1270/1584, Loss: 0.0258, LR: 0.005623\n",
            "Step 1280/1584, Loss: 0.0185, LR: 0.005648\n",
            "Step 1290/1584, Loss: 0.0226, LR: 0.005673\n",
            "Step 1300/1584, Loss: 0.0238, LR: 0.005698\n",
            "Step 1310/1584, Loss: 0.0149, LR: 0.005723\n",
            "Step 1320/1584, Loss: 0.0222, LR: 0.005748\n",
            "Step 1330/1584, Loss: 0.0177, LR: 0.005773\n",
            "Step 1340/1584, Loss: 0.0267, LR: 0.005798\n",
            "Step 1350/1584, Loss: 0.0191, LR: 0.005823\n",
            "Step 1360/1584, Loss: 0.0125, LR: 0.005848\n",
            "Step 1370/1584, Loss: 0.0148, LR: 0.005873\n",
            "Step 1380/1584, Loss: 0.0271, LR: 0.005898\n",
            "Step 1390/1584, Loss: 0.0420, LR: 0.005923\n",
            "Step 1400/1584, Loss: 0.0224, LR: 0.005948\n",
            "Step 1410/1584, Loss: 0.0102, LR: 0.005973\n",
            "Step 1420/1584, Loss: 0.0190, LR: 0.005998\n",
            "Step 1430/1584, Loss: 0.0204, LR: 0.005977\n",
            "Step 1440/1584, Loss: 0.0331, LR: 0.005952\n",
            "Step 1450/1584, Loss: 0.0145, LR: 0.005927\n",
            "Step 1460/1584, Loss: 0.0103, LR: 0.005902\n",
            "Step 1470/1584, Loss: 0.0088, LR: 0.005877\n",
            "Step 1480/1584, Loss: 0.0124, LR: 0.005852\n",
            "Step 1490/1584, Loss: 0.0148, LR: 0.005827\n",
            "Step 1500/1584, Loss: 0.0077, LR: 0.005802\n",
            "Step 1510/1584, Loss: 0.0123, LR: 0.005777\n",
            "Step 1520/1584, Loss: 0.0106, LR: 0.005752\n",
            "Step 1530/1584, Loss: 0.0046, LR: 0.005727\n",
            "Step 1540/1584, Loss: 0.0202, LR: 0.005702\n",
            "Step 1550/1584, Loss: 0.0169, LR: 0.005677\n",
            "Step 1560/1584, Loss: 0.0188, LR: 0.005652\n",
            "Step 1570/1584, Loss: 0.0281, LR: 0.005627\n",
            "Step 1580/1584, Loss: 0.0173, LR: 0.005602\n",
            "\n",
            "Epoch 14 Results:\n",
            "Training Loss: 0.0239\n",
            "Validation Loss: 0.0845\n",
            "Learning Rate: 0.004386\n",
            "\n",
            "Epoch 15/50\n",
            "Step 0/1584, Loss: 0.0214, LR: 0.005592\n",
            "Step 10/1584, Loss: 0.0217, LR: 0.005567\n",
            "Step 20/1584, Loss: 0.0249, LR: 0.005545\n",
            "Step 30/1584, Loss: 0.0168, LR: 0.005520\n",
            "Step 40/1584, Loss: 0.0094, LR: 0.005495\n",
            "Step 50/1584, Loss: 0.0082, LR: 0.005470\n",
            "Step 60/1584, Loss: 0.0213, LR: 0.005445\n",
            "Step 70/1584, Loss: 0.0122, LR: 0.005420\n",
            "Step 80/1584, Loss: 0.0199, LR: 0.005395\n",
            "Step 90/1584, Loss: 0.0161, LR: 0.005370\n",
            "Step 100/1584, Loss: 0.0124, LR: 0.005345\n",
            "Step 110/1584, Loss: 0.0061, LR: 0.005320\n",
            "Step 120/1584, Loss: 0.0167, LR: 0.005295\n",
            "Step 130/1584, Loss: 0.0203, LR: 0.005270\n",
            "Step 140/1584, Loss: 0.0120, LR: 0.005245\n",
            "Step 150/1584, Loss: 0.0119, LR: 0.005220\n",
            "Step 160/1584, Loss: 0.0271, LR: 0.005195\n",
            "Step 170/1584, Loss: 0.0107, LR: 0.005170\n",
            "Step 180/1584, Loss: 0.0146, LR: 0.005145\n",
            "Step 190/1584, Loss: 0.0142, LR: 0.005120\n",
            "Step 200/1584, Loss: 0.0209, LR: 0.005095\n",
            "Step 210/1584, Loss: 0.0150, LR: 0.005070\n",
            "Step 220/1584, Loss: 0.0171, LR: 0.005045\n",
            "Step 230/1584, Loss: 0.0112, LR: 0.005020\n",
            "Step 240/1584, Loss: 0.0251, LR: 0.004995\n",
            "Step 250/1584, Loss: 0.0505, LR: 0.004970\n",
            "Step 260/1584, Loss: 0.0266, LR: 0.004945\n",
            "Step 270/1584, Loss: 0.0421, LR: 0.004920\n",
            "Step 280/1584, Loss: 0.0634, LR: 0.004895\n",
            "Step 290/1584, Loss: 0.0655, LR: 0.004870\n",
            "Step 300/1584, Loss: 0.0278, LR: 0.004845\n",
            "Step 310/1584, Loss: 0.0183, LR: 0.004820\n",
            "Step 320/1584, Loss: 0.0152, LR: 0.004795\n",
            "Step 330/1584, Loss: 0.0249, LR: 0.004770\n",
            "Step 340/1584, Loss: 0.0278, LR: 0.004745\n",
            "Step 350/1584, Loss: 0.0277, LR: 0.004720\n",
            "Step 360/1584, Loss: 0.0159, LR: 0.004695\n",
            "Step 370/1584, Loss: 0.0204, LR: 0.004670\n",
            "Step 380/1584, Loss: 0.0150, LR: 0.004645\n",
            "Step 390/1584, Loss: 0.0234, LR: 0.004620\n",
            "Step 400/1584, Loss: 0.0324, LR: 0.004595\n",
            "Step 410/1584, Loss: 0.0576, LR: 0.004570\n",
            "Step 420/1584, Loss: 0.0153, LR: 0.004545\n",
            "Step 430/1584, Loss: 0.0167, LR: 0.004520\n",
            "Step 440/1584, Loss: 0.0485, LR: 0.004495\n",
            "Step 450/1584, Loss: 0.0357, LR: 0.004470\n",
            "Step 460/1584, Loss: 0.0258, LR: 0.004445\n",
            "Step 470/1584, Loss: 0.0182, LR: 0.004420\n",
            "Step 480/1584, Loss: 0.0133, LR: 0.004395\n",
            "Step 490/1584, Loss: 0.0095, LR: 0.004370\n",
            "Step 500/1584, Loss: 0.0174, LR: 0.004345\n",
            "Step 510/1584, Loss: 0.0097, LR: 0.004320\n",
            "Step 520/1584, Loss: 0.0099, LR: 0.004295\n",
            "Step 530/1584, Loss: 0.0140, LR: 0.004270\n",
            "Step 540/1584, Loss: 0.0117, LR: 0.004245\n",
            "Step 550/1584, Loss: 0.0188, LR: 0.004220\n",
            "Step 560/1584, Loss: 0.0142, LR: 0.004195\n",
            "Step 570/1584, Loss: 0.0247, LR: 0.004170\n",
            "Step 580/1584, Loss: 0.0154, LR: 0.004145\n",
            "Step 590/1584, Loss: 0.0172, LR: 0.004120\n",
            "Step 600/1584, Loss: 0.0182, LR: 0.004095\n",
            "Step 610/1584, Loss: 0.0379, LR: 0.004070\n",
            "Step 620/1584, Loss: 0.0745, LR: 0.004045\n",
            "Step 630/1584, Loss: 0.0439, LR: 0.004020\n",
            "Step 640/1584, Loss: 0.0311, LR: 0.003995\n",
            "Step 650/1584, Loss: 0.0385, LR: 0.003970\n",
            "Step 660/1584, Loss: 0.0144, LR: 0.003945\n",
            "Step 670/1584, Loss: 0.0275, LR: 0.003920\n",
            "Step 680/1584, Loss: 0.0405, LR: 0.003895\n",
            "Step 690/1584, Loss: 0.0266, LR: 0.003870\n",
            "Step 700/1584, Loss: 0.0267, LR: 0.003845\n",
            "Step 710/1584, Loss: 0.0219, LR: 0.003820\n",
            "Step 720/1584, Loss: 0.0160, LR: 0.003795\n",
            "Step 730/1584, Loss: 0.0347, LR: 0.003770\n",
            "Step 740/1584, Loss: 0.0300, LR: 0.003745\n",
            "Step 750/1584, Loss: 0.0407, LR: 0.003720\n",
            "Step 760/1584, Loss: 0.0596, LR: 0.003695\n",
            "Step 770/1584, Loss: 0.0456, LR: 0.003670\n",
            "Step 780/1584, Loss: 0.0170, LR: 0.003645\n",
            "Step 790/1584, Loss: 0.0345, LR: 0.003620\n",
            "Step 800/1584, Loss: 0.0163, LR: 0.003595\n",
            "Step 810/1584, Loss: 0.0354, LR: 0.003570\n",
            "Step 820/1584, Loss: 0.0399, LR: 0.003545\n",
            "Step 830/1584, Loss: 0.0202, LR: 0.003520\n",
            "Step 840/1584, Loss: 0.0112, LR: 0.003495\n",
            "Step 850/1584, Loss: 0.0131, LR: 0.003470\n",
            "Step 860/1584, Loss: 0.0437, LR: 0.003445\n",
            "Step 870/1584, Loss: 0.0124, LR: 0.003420\n",
            "Step 880/1584, Loss: 0.0172, LR: 0.003395\n",
            "Step 890/1584, Loss: 0.0339, LR: 0.003370\n",
            "Step 900/1584, Loss: 0.0379, LR: 0.003345\n",
            "Step 910/1584, Loss: 0.0262, LR: 0.003320\n",
            "Step 920/1584, Loss: 0.0119, LR: 0.003295\n",
            "Step 930/1584, Loss: 0.0291, LR: 0.003270\n",
            "Step 940/1584, Loss: 0.0796, LR: 0.003245\n",
            "Step 950/1584, Loss: 0.0436, LR: 0.003220\n",
            "Step 960/1584, Loss: 0.0173, LR: 0.003195\n",
            "Step 970/1584, Loss: 0.0229, LR: 0.003170\n",
            "Step 980/1584, Loss: 0.0118, LR: 0.003145\n",
            "Step 990/1584, Loss: 0.0130, LR: 0.003120\n",
            "Step 1000/1584, Loss: 0.0207, LR: 0.003095\n",
            "Step 1010/1584, Loss: 0.0144, LR: 0.003070\n",
            "Step 1020/1584, Loss: 0.0093, LR: 0.003045\n",
            "Step 1030/1584, Loss: 0.0274, LR: 0.003020\n",
            "Step 1040/1584, Loss: 0.0481, LR: 0.002995\n",
            "Step 1050/1584, Loss: 0.0322, LR: 0.002970\n",
            "Step 1060/1584, Loss: 0.0239, LR: 0.002945\n",
            "Step 1070/1584, Loss: 0.0213, LR: 0.002920\n",
            "Step 1080/1584, Loss: 0.0160, LR: 0.002895\n",
            "Step 1090/1584, Loss: 0.0065, LR: 0.002870\n",
            "Step 1100/1584, Loss: 0.0292, LR: 0.002845\n",
            "Step 1110/1584, Loss: 0.0091, LR: 0.002820\n",
            "Step 1120/1584, Loss: 0.0080, LR: 0.002795\n",
            "Step 1130/1584, Loss: 0.0331, LR: 0.002770\n",
            "Step 1140/1584, Loss: 0.0962, LR: 0.002745\n",
            "Step 1150/1584, Loss: 0.0902, LR: 0.002720\n",
            "Step 1160/1584, Loss: 0.0227, LR: 0.002695\n",
            "Step 1170/1584, Loss: 0.0179, LR: 0.002670\n",
            "Step 1180/1584, Loss: 0.0279, LR: 0.002645\n",
            "Step 1190/1584, Loss: 0.0395, LR: 0.002620\n",
            "Step 1200/1584, Loss: 0.0143, LR: 0.002595\n",
            "Step 1210/1584, Loss: 0.0147, LR: 0.002570\n",
            "Step 1220/1584, Loss: 0.0139, LR: 0.002545\n",
            "Step 1230/1584, Loss: 0.0207, LR: 0.002520\n",
            "Step 1240/1584, Loss: 0.0136, LR: 0.002495\n",
            "Step 1250/1584, Loss: 0.0072, LR: 0.002470\n",
            "Step 1260/1584, Loss: 0.0098, LR: 0.002445\n",
            "Step 1270/1584, Loss: 0.0241, LR: 0.002420\n",
            "Step 1280/1584, Loss: 0.0200, LR: 0.002395\n",
            "Step 1290/1584, Loss: 0.0181, LR: 0.002370\n",
            "Step 1300/1584, Loss: 0.0249, LR: 0.002345\n",
            "Step 1310/1584, Loss: 0.0134, LR: 0.002320\n",
            "Step 1320/1584, Loss: 0.0223, LR: 0.002295\n",
            "Step 1330/1584, Loss: 0.0188, LR: 0.002270\n",
            "Step 1340/1584, Loss: 0.0259, LR: 0.002245\n",
            "Step 1350/1584, Loss: 0.0208, LR: 0.002220\n",
            "Step 1360/1584, Loss: 0.0146, LR: 0.002195\n",
            "Step 1370/1584, Loss: 0.0125, LR: 0.002170\n",
            "Step 1380/1584, Loss: 0.0202, LR: 0.002145\n",
            "Step 1390/1584, Loss: 0.0469, LR: 0.002120\n",
            "Step 1400/1584, Loss: 0.0180, LR: 0.002095\n",
            "Step 1410/1584, Loss: 0.0104, LR: 0.002070\n",
            "Step 1420/1584, Loss: 0.0181, LR: 0.002045\n",
            "Step 1430/1584, Loss: 0.0152, LR: 0.002020\n",
            "Step 1440/1584, Loss: 0.0380, LR: 0.001995\n",
            "Step 1450/1584, Loss: 0.0136, LR: 0.001970\n",
            "Step 1460/1584, Loss: 0.0141, LR: 0.001945\n",
            "Step 1470/1584, Loss: 0.0086, LR: 0.001920\n",
            "Step 1480/1584, Loss: 0.0093, LR: 0.001895\n",
            "Step 1490/1584, Loss: 0.0145, LR: 0.001870\n",
            "Step 1500/1584, Loss: 0.0082, LR: 0.001845\n",
            "Step 1510/1584, Loss: 0.0103, LR: 0.001820\n",
            "Step 1520/1584, Loss: 0.0116, LR: 0.001795\n",
            "Step 1530/1584, Loss: 0.0048, LR: 0.001770\n",
            "Step 1540/1584, Loss: 0.0186, LR: 0.001745\n",
            "Step 1550/1584, Loss: 0.0180, LR: 0.001720\n",
            "Step 1560/1584, Loss: 0.0198, LR: 0.001695\n",
            "Step 1570/1584, Loss: 0.0310, LR: 0.001670\n",
            "Step 1580/1584, Loss: 0.0183, LR: 0.001645\n",
            "\n",
            "Early stopping triggered after 15 epochs\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define evaluation code that handles custom layers and provides comprehensive metrics\n",
        "def evaluate_model(model, test_dataset, num_samples=5):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the depth estimation model.\n",
        "    Calculates both numerical metrics and generates visualizations to assess performance.\n",
        "\n",
        "    Args:\n",
        "        model: The trained depth estimation model\n",
        "        test_dataset: Dataset object containing test data\n",
        "        num_samples: Number of example predictions to visualize\n",
        "\n",
        "    Returns:\n",
        "        float: Average MSE loss across the test dataset\n",
        "    \"\"\"\n",
        "    print(\"\\nStarting model evaluation...\")\n",
        "\n",
        "    # Initialize tracking metrics\n",
        "    all_losses = []          # Track MSE losses\n",
        "    all_abs_rel = []         # Track relative errors\n",
        "    all_rmse = []           # Track root mean square errors\n",
        "    # Standard thresholds used in depth estimation literature (within 25%, 56.25%, and 95.0625% of ground truth)\n",
        "    threshold_accuracies = {1.25: 0, 1.25**2: 0, 1.25**3: 0}\n",
        "    total_samples = 0\n",
        "\n",
        "    # Store example predictions for visualization\n",
        "    example_data = []\n",
        "\n",
        "    # Process all batches in test dataset\n",
        "    for step in range(len(test_dataset)):\n",
        "        batch = test_dataset.get_batch()\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        images, true_depths = batch\n",
        "        predictions = model(images, training=False)\n",
        "        predictions = predictions[:,:,:,0]  # Remove single-channel dimension\n",
        "\n",
        "        # Store first few examples for visualization\n",
        "        if step < num_samples:\n",
        "            example_data.append((images[:1], true_depths[:1], predictions[:1]))\n",
        "\n",
        "        # Calculate metrics for each image in batch\n",
        "        for i in range(len(images)):\n",
        "            true_depth = true_depths[i]\n",
        "            pred_depth = predictions[i]\n",
        "\n",
        "            # Calculate mean squared error\n",
        "            mse = tf.reduce_mean(tf.square(true_depth - pred_depth))\n",
        "            all_losses.append(float(mse))\n",
        "\n",
        "            # Calculate relative error (important for depth estimation)\n",
        "            abs_rel = tf.reduce_mean(tf.abs(true_depth - pred_depth) / true_depth)\n",
        "            all_abs_rel.append(float(abs_rel))\n",
        "\n",
        "            # Calculate RMSE (in same units as depth)\n",
        "            rmse = tf.sqrt(mse)\n",
        "            all_rmse.append(float(rmse))\n",
        "\n",
        "            # Calculate threshold accuracies (percentage of pixels within error bounds)\n",
        "            ratios = tf.maximum(true_depth / pred_depth, pred_depth / true_depth)\n",
        "            for threshold in threshold_accuracies.keys():\n",
        "                threshold_accuracies[threshold] += tf.reduce_mean(\n",
        "                    tf.cast(ratios < threshold, tf.float32)\n",
        "                )\n",
        "\n",
        "            total_samples += 1\n",
        "\n",
        "    # Print comprehensive metrics summary\n",
        "    print(\"\\nOverall Model Performance:\")\n",
        "    print(f\"Average MSE Loss: {np.mean(all_losses):.4f}\")\n",
        "    print(f\"Average Absolute Relative Error: {np.mean(all_abs_rel):.4f}\")\n",
        "    print(f\"Root Mean Square Error: {np.mean(all_rmse):.4f}\")\n",
        "\n",
        "    # Calculate and print threshold accuracies\n",
        "    for threshold, total in threshold_accuracies.items():\n",
        "        accuracy = total / total_samples\n",
        "        print(f\"δ < {threshold:.2f}: {accuracy:.4f}\")\n",
        "\n",
        "    # Generate visualization of example predictions\n",
        "    plt.figure(figsize=(15, 3*num_samples))\n",
        "    for idx, (image, true_depth, pred_depth) in enumerate(example_data):\n",
        "        # Show input image\n",
        "        plt.subplot(num_samples, 3, idx*3 + 1)\n",
        "        plt.imshow(image[0])\n",
        "        plt.title('Input Image')\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Show ground truth depth\n",
        "        plt.subplot(num_samples, 3, idx*3 + 2)\n",
        "        plt.imshow(true_depth[0], cmap='magma')\n",
        "        plt.title('Ground Truth Depth')\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Show predicted depth\n",
        "        plt.subplot(num_samples, 3, idx*3 + 3)\n",
        "        plt.imshow(pred_depth[0], cmap='magma')\n",
        "        plt.title('Predicted Depth')\n",
        "        plt.colorbar()\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('evaluation_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Generate error distribution visualizations\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(all_losses, bins=50, alpha=0.75)\n",
        "    plt.title('Distribution of MSE Losses')\n",
        "    plt.xlabel('MSE Loss')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(all_abs_rel, bins=50, alpha=0.75)\n",
        "    plt.title('Distribution of Absolute Relative Errors')\n",
        "    plt.xlabel('Absolute Relative Error')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('error_distributions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Generate detailed error analysis for a sample prediction\n",
        "    sample_image, sample_true, sample_pred = example_data[0]\n",
        "    error_map = tf.abs(sample_true[0] - sample_pred[0])\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(sample_image[0])\n",
        "    plt.title('Sample Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(sample_pred[0], cmap='magma')\n",
        "    plt.title('Predicted Depth')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(error_map, cmap='hot')\n",
        "    plt.title('Depth Error Map')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('depth_error_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return np.mean(all_losses)\n",
        "\n",
        "# Run evaluation after training\n",
        "print(\"\\nEvaluating model performance...\")\n",
        "try:\n",
        "    # Register custom layer for model loading\n",
        "    custom_objects = {\n",
        "        'UpsampleBlock': UpsampleBlock\n",
        "    }\n",
        "\n",
        "    # Load the best model if available\n",
        "    if os.path.exists('best_model.h5'):\n",
        "        with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "            model = tf.keras.models.load_model('best_model.h5')\n",
        "            print(\"Loaded best model for evaluation\")\n",
        "\n",
        "    # Create fresh test dataset\n",
        "    test_dataset = MemoryEfficientDataset(TEST_CSV, BATCH_SIZE)\n",
        "\n",
        "    # Run comprehensive evaluation\n",
        "    final_loss = evaluate_model(model, test_dataset)\n",
        "    print(f\"\\nEvaluation complete! Final average loss: {final_loss:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during evaluation: {str(e)}\")\n",
        "    raise\n",
        "finally:\n",
        "    # Clean up any remaining memory\n",
        "    cleanup()"
      ],
      "metadata": {
        "id": "nseS4cGke5eb",
        "outputId": "69584906-da3c-49a1-d9cc-504e695379b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nseS4cGke5eb",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model performance...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model for evaluation\n",
            "\n",
            "Starting model evaluation...\n",
            "\n",
            "Overall Model Performance:\n",
            "Average MSE Loss: 0.0649\n",
            "Average Absolute Relative Error: 8.4485\n",
            "Root Mean Square Error: 0.2545\n",
            "δ < 1.25: 0.0000\n",
            "δ < 1.56: 0.0000\n",
            "δ < 1.95: 0.0000\n",
            "\n",
            "Evaluation complete! Final average loss: 0.0649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyPzh_sgXykE"
      },
      "id": "UyPzh_sgXykE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}